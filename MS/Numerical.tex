\section{Performance evaluation}
\label{sec:sec4}
Next, we evaluate the performance of our proposed JMMLE algorithm and the hypothesis testing framework in a two-layer simulation setup (Sections \ref{sec:eval-jmmle} and \ref{sec:eval-testing}, respectively), and also introduce some computational techniques that significantly accelerate calculations for high data dimensions (Section~\ref{sec:tricks-jmmle}).

\subsection{Simulation 1: estimation}
\label{sec:eval-jmmle}
As a first step towards obtaining a two-layer structure with horizontal (across $k$) integration and inter-layer directed edges, we generate the precision matrices $\{ \Omega_{x0}^k \}$ and $\{ \Omega_{y0}^k \}$ using a dependency structure across $k$ that was first used in the simulation study of \cite{MaMichailidis15}. We set $K=5$, and set different shared sparsity patterns across $k$ inside the lower $p/2 \times p/2$ block of the upper layer precision matrices, and outside the block. In our notation, this gives the following elementwise group structure:
%
$$
\cG_{x,ii'} = \begin{cases}
\{ (1,2),(3,4), 5 \} \text{ if } i \leq p/2 \text{ or } j \leq p/2\\
\{ (1,3,5),(2,4) \} \text{ otherwise }
\end{cases}
$$
%
The schematic in Figure~\ref{fig:sim-structure} illustrates this structure. We set an off-diagonal element inside each of these common blocks (i.e. $A,B,C$ and $\alpha, \beta$ in the figure) to be non-zero with probability $\pi_x \in \{ 5/p, 30/p \}$, then generate the values of all non-zero elements independently from the uniform distribution in the interval $[-1, 0.5] \cup [0.5, 1]$. The precision matrices $\Omega_{x0}^k$ are generated by putting together the corresponding common blocks, their positive definiteness ensured by setting all diagonal elements to be $1 + |\Lambda_{\min} (\Omega_{x0}^k)|$. Then, we get elements in the covariance matrix as
%
$$
\sigma_{x0,ii'}^k = (\bar \Omega_{x0}^k)_{ii'} / \sqrt{(\bar \Omega_{x0}^k)_{ii} (\bar \Omega_{x0}^k)_{i'i'} },
\text{ where } \bar \Omega_{x0}^k = (\Omega_{x0}^k)^{-1},
$$
and generate rows of $\bfX^k$ independently from $\cN(0, \Sigma_{x0}^k)$. We obtain $\Sigma_{y0}^k$ and then $\bfE^k$ using the same setup but with the number of variables being $q$ and setting off-diagonal elements non-zero with probability $\pi_y \in \{ 5/q, 30/q \}$. To obtain the matrices $\bfB_0^k$, for a fixed $(i,j), i \in \cI_p, j \in \cI_q$, we set $b_{0,ij}^k$ non-zero across all $k$ with probability $\pi \in \{ 5/p, 30/p \}$, generate the non-zero groups independently from $\text{Unif} \{ [-1, 0.5] \cup [0.5, 1] \}$, and set $\bfY^k = \bfX^k \bfB_0^k + \bfE^k, k \in \cI_K$. Finally, we generate 50 such independent two-layer datasets for each of the following model settings:

\begin{itemize}
\item Set $\pi_x = \pi = 5/p, \pi_y = 5/q$, and
%
$$
(p,q,n) \in \{ (60,30,100), (30,60,100), (200,200,150), (300,300,150) \};
$$

\item Set $\pi_x = \pi = 30/p, \pi_y = 30/q$, and $(p,q,n) \in  \{ (200,200,100), (200,200,200) \}$.
\end{itemize}
%

\begin{figure}
\centering
\includegraphics{omega-structure}
\caption{Shared sparsity patterns across $k$ for the precision matrices $\{ \Omega_{x0}^k\}$ and $\{ \Omega_{y0}^k\}$}
\label{fig:sim-structure}
\end{figure}

We use the following arrays of tuning parameters to train Algorithm~\ref{algo:jmmle-algo}-
%
$$
\gamma_n \in \left\{ 0.3, 0.4, ..., 1 \right\} \sqrt{\frac{\log q}{n}}; \quad
\lambda_n \in \left\{ 0.4, 0.6, ..., 1.8 \right\} \sqrt{\frac{\log p}{n}},
$$
%
using the one-step version (Section~\ref{sec:tricks-jmmle}) instead of the full algorithm to save computation time.

We use the following performance metrics to evaluate our estimates $\widehat \cB = \{ \widehat \bfB^k \}$:

\begin{itemize}
\item True positive Rate-
%
\[
\text{TPR}(\widehat \bfB_k) = \frac{| \supp(\hat \bfB^k) \cap \supp (\bfB_0^k)|}{| \supp (\bfB_0^k)|}; \quad
\text{TPR} (\widehat \cB) = \frac{1}{K} \sum_{k=1}^K \text{TP}(\widehat \bfB_k).
\]
\item True negative Rate-
%
\[
\text{TNR}(\widehat \bfB_k) = \frac{| {\supp}^c (\hat \bfB^k) \cap {\supp}^c (\bfB_0^k)|}{| \supp^c (\bfB_0^k)|}; \quad
\text{TNR} (\widehat \cB) = \frac{1}{K} \sum_{k=1}^K \text{TNR}(\widehat \bfB_k).
\]
%
\item Matthews Correlation Coefficient-
%
$$
\text{TP}(\widehat \bfB_k) = | \supp(\hat \bfB^k) \cap \supp (\bfB_0^k)|; \quad
\text{TN}(\widehat \bfB_k) = | {\supp}^c (\hat \bfB^k) \cap {\supp}^c (\bfB_0^k)|,
$$
$$
\text{FP}(\widehat \bfB_k) = | {\supp}^c (\bfB^k_0) | - \text{TN}(\widehat \bfB_k); \quad
\text{FN}(\widehat \bfB_k) = | {\supp} (\bfB^k_0) | - \text{TP}(\widehat \bfB_k), $$
\begin{align*}
& \text{MCC} (\widehat \bfB_k) = \\
& \frac{ \text{TP}(\widehat \bfB_k) \text{TN}(\widehat \bfB_k) -
\text{FP}(\widehat \bfB_k) \text{FN}(\widehat \bfB_k)}
{\sqrt{(\text{TP}(\widehat \bfB_k) + \text{FP}(\widehat \bfB_k))
(\text{TP}(\widehat \bfB_k) + \text{FN}(\widehat \bfB_k))
(\text{TN}(\widehat \bfB_k) + \text{FP}(\widehat \bfB_k))
(\text{TN}(\widehat \bfB_k) + \text{FN}(\widehat \bfB_k)) }}, \\
& \text{MCC} (\widehat \cB) = \frac{1}{K} \sum_{k=1}^K \text{MCC}(\widehat \bfB_k).
\end{align*}
%
\item Relative error in Frobenius norm-
%
\[
\text{RF} (\widehat \cB) = \frac{1}{K} \sum_{k=1}^K \frac{\| \hat \bfB^k - \bfB_0^k \|_F}{\| \bfB_0^k \|_F}.
\]
%
\end{itemize}
%
We use the same metrics to evaluate the precision matrix estimates $\widehat \Omega_y^k$ as well.

\paragraph{}
Tables \ref{table:simtable11} and \ref{table:simtable12} summarize the results. For estimation of $\cB$, we compare our results to the method in \cite{LinEtal16} that estimates parameters in each of the $K$ two-layer structure separately, while for estimation of $\Omega_y$, we compare them with the results in \cite{LinEtal16} and using the single-layer JSEM \citep{MaMichailidis15} that estimates $\Omega_y$ assuming structured sparsity patterns and centered matrices $\bfY^k$, but not the data in the upper layer, i.e. $\cX$.

Our joint method has higher average MCC across all data settings than the separate method for the estimation of $\cB$, although TPR and TNR values are similar, except for $p=200, q=200, n=100$ where JMMLE has a much higher average TPR. For estimation of $\Omega_y$, incorporating information from the upper layer vastly improves performance, as demonstrated by the differences in performances between JMMLE and JSEM. For the 4 data settings with lower sparsity $(\pi_x = \pi = 5/p, \pi_y = 5/q)$, JMMLE is either slightly conservative or has similar TPR and TNR compared to the separate method while estimating $\Omega_y$. However, JMMLE does better in both of the higher sparsity settings. Lastly, for the estimation of both $\cB$ and $\Omega_y$, JMMLE gives more accurate estimates across the methods, as evident from the lower average RF values across all data settings.
%

%\begin{scriptsize}
%\begin{table}
%    \begin{tabular}{ccccccc}
%    \hline
%    $(\pi_x, \pi_y)$ & $(p,q,n)$   & Method   & TP            & TN             & F1 & RF            \\ \hline
%    $(5/p, 5/q)$   & (60,30,100)   & JMMLE    & 0.97 (0.045) & 0.99 (0.006)  & 0.97 (0.03)  & 0.24 (0.073) \\
%    ~              & ~             & Separate & 0.66 (0.058) & 0.99 (0.003)  & 0.78 (0.039) & 1.22 (0.025) \\
%    ~              & (30,60,100)   & JMMLE    & 0.97 (0.029) & 0.99 (0.005)  & 0.96 (0.015) & 0.27 (0.053) \\
%    ~              & ~             & Separate & 0.92 (0.029) & 0.98 (0.005)  & 0.88 (0.04)  & 1.24 (0.034) \\
%    ~              & (200,200,150) & JMMLE    & 0.98 (0.025) & 1.0 (0)       & 0.99 (0.012) & 0.16 (0.056) \\
%    ~              & ~             & Separate & 0.99 (0.003) & 0.99 (0.001)  & 0.88 (0.01)  & 1.26 (0.008) \\
%    ~              & (300,300,150) & JMMLE    & 0.99 (0.015) & 1.0 (0)      & 0.99 (0.008)   & 0.14 (0.033)\\
%    ~              & ~             & Separate & ~             & ~              & ~   & ~             \\\hline
%    $(30/p, 30/q)$ & (200,200,100) & JMMLE    & 0.97 (0.039) & 1.0 (0)      & 0.98 (0.017)   & 0.21 (0.07) \\
%    ~              & ~             & Separate & ~             & ~              & ~   & ~             \\
%    ~              & (200,200,200) & JMMLE    & 0.99 (0.01)  & 1.0 (0)      & 0.99 (0.017)   & 0.13 (0.036) \\
%    ~              & ~             & Separate & ~             & ~              & ~   & ~             \\ \hline
%    \end{tabular}
%    \caption{Table of outputs for joint and separate estimation of regression matrices, giving empirical mean and standard deviation (in brackets) of each evaluation metric over 50 replications.}
%    \label{table:simtable11}
%\end{table}
\begin{scriptsize}
\begin{table}
    \begin{tabular}{ccccccc}
    \hline
    $(\pi_x, \pi_y)$ & $(p,q,n)$   & Method   & TPR            & TNR            & MCC & RF            \\ \hline
    $(5/p, 5/q)$   & (60,30,100)   & JMMLE    & 0.97(0.02)  & 0.99(0.003)  & 0.96(0.014) & 0.24(0.033) \\
    ~              & ~             & Separate & 0.96(0.018) & 0.99(0.004)  & 0.93(0.014) & 0.22(0.029) \\\cline{2-7}
    %
    ~              & (30,60,100)   & JMMLE    & 0.97(0.013) & 0.99(0.002)  & 0.96(0.008) & 0.27(0.024) \\
    ~              & ~             & Separate & 0.99(0.009) & 0.99(0.003)  & 0.93(0.017) & 0.18(0.021) \\\cline{2-7}
    %
    ~              & (200,200,150) & JMMLE    & 0.98(0.011) & 1.0(0)       & 0.99(0.005) & 0.16(0.025) \\
    ~              & ~             & Separate & 0.99(0.001) & 0.99 (0.001) & 0.88(0.009) & 0.18(0.007) \\\cline{2-7}
    %
    ~              & (300,300,150) & JMMLE    & 1.0(0.001)  & 1.0(0)       & 0.99(0.001) & 0.14 (0.015)\\
    ~              & ~             & Separate & 1.0(0.001)  & 0.99(0.001)  & 0.84(0.01)  & 0.21(0.007)\\\hline
    %
    $(30/p, 30/q)$ & (200,200,100) & JMMLE    & 0.97(0.017) & 1.0(0)       & 0.98(0.008) & 0.21(0.032) \\
    ~              & ~             & Separate & 0.32(0.01)  & 0.99(0.001)  & 0.49(0.009) & 0.85(0.06)  \\\cline{2-7}
    %
    ~              & (200,200,200) & JMMLE    & 0.99(0.006) & 1.0(0)       & 0.99(0.007) & 0.13(0.016) \\
    ~              & ~             & Separate & 0.97(0.004) & 0.98(0.001)  & 0.93(0.002) & 0.19(0.07)  \\    \hline
    \end{tabular}
    \caption{Table of outputs for estimation of regression matrices, giving empirical mean and standard deviation (in brackets) of each evaluation metric over 50 replications.}
    \label{table:simtable11}
\end{table}

%
%\begin{table}
%    \begin{tabular}{ccccccc}
%    \hline
%    $(\pi_x, \pi_y)$ & $(p,q,n)$   & Method   & TP            & TN             & F1 & RF            \\ \hline
%    $(5/p, 5/q)$   & (60,30,100)   & JMMLE    & 0.76 (0.041) & 0.90 (0.014)  & 0.68 (0.048)   & 0.32 (0.018) \\
%    ~              & ~             & Separate & 0.17 (0.009) & 0.93 (0.004)  & 0.26 (0.012)   & 0.66 (0.038) \\
%    ~              & ~             & JSEM     & 0.24 (0.029) & 0.8 (0.008)   & 0.24 (0.028)   & 1.03 (0.003)\\
%    ~              & (30,60,100)   & JMMLE    & 0.7 (0.035)  & 0.94 (0.003)  & 0.58 (0.039)   & 0.3 (0.01) \\
%    ~              & ~             & Separate & 0.3 (0.018)  & 0.87 (0.013)  & 0.41 (0.022)   & 0.81 (0.092) \\
%    ~              & ~             & JSEM     & 0.13 (0.011) & 0.9 (0.002)   & 0.16 (0.013)   & 1.04 (0.002)\\
%    ~              & (200,200,150) & JMMLE    & 0.68 (0.037) & 0.98 (0.001)  & 0.47 (0.031)   & 0.26 (0.005) \\
%    ~              & ~             & Separate & 0.06 (0.002) & 0.98 (0.001)  & 0.11 (0.004)   & 0.59 (0.012) \\
%    ~              & ~             & JSEM & ~             & ~              & ~   & ~             \\
%    ~              & (300,300,150) & JMMLE    & 0.71 (0.03)  & 0.98 (0)      & 0.41 (0.019)   & 0.25 (0.004) \\
%    ~              & ~             & Separate & ~             & ~              & ~   & ~             \\\hline
%    ~              & ~             & JSEM & ~             & ~              & ~   & ~             \\
%    $(30/p, 30/q)$ & (200,200,100) & JMMLE    & 0.77 (0.037) & 0.98 (0.001)  & 0.42 (0.034)   & 0.31 (0.008) \\
%    ~              & ~             & Separate & ~             & ~              & ~   & ~             \\
%    ~              & ~             & JSEM & ~             & ~              & ~   & ~             \\
%    ~              & (200,200,200) & JMMLE    & 0.76 (0.04)  & 0.98 (0.001)  & 0.54 (0.035)   & 0.27 (0.008) \\
%    ~              & ~             & Separate & ~             & ~              & ~   & ~             \\
%    ~              & ~             & JSEM & ~             & ~              & ~   & ~             \\\hline
%    \end{tabular}
%    \caption{Table of outputs for joint and separate estimation of lower layer precision matrices over 50 replications.}
%    \label{table:simtable12}
%\end{table}
%\end{scriptsize}
\begin{table}
    \begin{tabular}{ccccccc}
    \hline
    $(\pi_x, \pi_y)$ & $(p,q,n)$   & Method   & TPR            & TNR             & MCC & RF            \\ \hline
    $(5/p, 5/q)$   & (60,30,100)   & JMMLE    & 0.76(0.018) & 0.90(0.006)  & 0.61(0.024)  & 0.32(0.008) \\
    ~              & ~             & Separate & 0.77(0.031) & 0.92(0.007)  & 0.56(0.03)   & 0.51(0.017) \\
    ~              & ~             & JSEM     & 0.24(0.013) & 0.8(0.003)   & 0.05(0.015)  & 1.03(0.002)\\\cline{2-7}
    %
    ~              & (30,60,100)   & JMMLE    & 0.7(0.018)  & 0.94(0.002)  & 0.55(0.018)  & 0.3(0.005) \\
    ~              & ~             & Separate & 0.76(0.041) & 0.89(0.015)  & 0.59(0.039)  & 0.49(0.014) \\
    ~              & ~             & JSEM     & 0.13(0.005) & 0.9(0.001)   & 0.03(0.007)  & 1.04(0.001) \\\cline{2-7}
    %
    ~              & (200,200,150) & JMMLE    & 0.68(0.017) & 0.98(0)      & 0.48(0.013)  & 0.26(0.002) \\
    ~              & ~             & Separate & 0.78(0.019) & 0.97(0.001)  & 0.55(0.012)  & 0.6(0.007) \\
    ~              & ~             & JSEM     & 0.05(0.002) & 0.97(0)      & 0.02(0.002)  & 1.01(0) \\\cline{2-7}
    %
    ~              & (300,300,150) & JMMLE    & 0.71(0.014) & 0.98(0)      & 0.44(0.008)  & 0.25(0.002) \\
    ~              & ~             & Separate & 0.71(0.017) & 0.98(0.001)  & 0.51(0.011)  & 0.59(0.005) \\
    ~              & ~             & JSEM     & 0.04(0.002) & 0.98(0)      & 0.02(0.002)  & 1.01(0)     \\\hline
    %
    $(30/p, 30/q)$ & (200,200,100) & JMMLE    & 0.77(0.016) & 0.98(0)      & 0.46(0.013)  & 0.31(0.003) \\
    ~              & ~             & Separate & 0.57(0.027) & 0.44(0.007)  & 0.04(0.008)  & 0.84(0.002)\\
    ~              & ~             & JSEM     & 0.05(0.002) & 0.97(0)      & 0.01(0.002)  & 1.01(0)     \\\hline
    %
    ~              & (200,200,200) & JMMLE    & 0.76(0.018)  & 0.98(0)     & 0.55(0.015)  & 0.27(0.004) \\
    ~              & ~             & Separate & 0.73(0.023) & 0.94(0.003)  & 0.39(0.017)  & 0.62(0.011)\\
    ~              & ~             & JSEM     & 0.05(0.002) & 0.97(0)      & 0.03(0.003)  & 1.01(0)     \\\hline
    \end{tabular}
    \caption{Table of outputs for estimation of lower layer precision matrices over 50 replications.}
    \label{table:simtable12}
\end{table}
\end{scriptsize}


\subsubsection{Effect of heterogeneity}
We repeat the above setups to check the performance of JMMLE in presence of within-group misspecification. For this task, we first set individual elements inside a non-zero group to be zero with probability 0.2 while generating the data, then pass the JMMLE estimates $\widehat \bfB^k$ through the FDR controlling thresholds as given in \eqref{eqn:fdr-threshold}. The results are summarized in Tables \ref{table:simtable2} and \ref{table:simtable22}. Across the simulation settings, values of all metrics are very close to the correctly specified counterparts in Table~\ref{table:simtable11}. Thus, the thresholding step 
proves largely effective. Also, in all cases the empirical FDR for estimating entries in $\cB$ is below 0.2. The performance is slightly worse than the correctly specified cases when estimating $\Omega_y$. This is expected, as the estimates $\widehat \Omega_y$ are obtained from neighborhood coefficients that are calculated based on the {\it pre-thresholding} coefficient estimates.
\begin{scriptsize}
\begin{table}
\centering
    \begin{tabular}{cccccc}
    \hline
    $(\pi_x, \pi_y)$ & $(p,q,n)$   & TPR$(\widehat \cB)$            & TNR$(\widehat \cB)$             & MCC$(\widehat \cB)$ & RF$(\widehat \cB)$    \\ \hline
    $(5/p, 5/q)$   & (60,30,100)   & 0.98 (0.01)   & 0.99 (0.002)   & 0.89 (0.017)  & 0.29 (0.014) \\
    ~              & (30,60,100)   & 0.94 (0.022)  & 0.99 (0.003)   & 0.93 (0.016)  & 0.31 (0.028) \\
    ~              & (200,200,150) & 0.99 (0.002)  & 0.99 (0)       & 0.98 (0.004)  & 0.17 (0.007) \\
    ~              & (300,300,150) & 0.99 (0.001)  & 1 (0)          & 0.99 (0.002)  & 0.15 (0.006) \\
    $(30/p, 30/q)$ & (200,200,100) & 0.99 (0.006)  & 1 (0)          & 0.98 (0.005)  & 0.2 (0.014)  \\
    ~              & (200,200,200) & 0.99 (0.009)  & 1 (0)          & 0.98 (0.005)  & 0.15 (0.017) \\\hline
    \hline
    $(\pi_x, \pi_y)$ & $(p,q,n)$   & TPR$(\widehat \Omega_y)$            & TNR$(\widehat \Omega_y)$             & MCC$(\widehat \Omega_y)$ & RF$(\widehat \Omega_y)$            \\ \hline
    $(5/p, 5/q)$   & (60,30,100)   & 0.71 (0.024)  & 0.90 (0.005)   & 0.64 (0.024)  & 0.34 (0.008)\\
    ~              & (30,60,100)   & 0.7 (0.019)   & 0.94 (0.002)   & 0.59 (0.014)  & 0.3 (0.004) \\
    ~              & (200,200,150) & 0.62 (0.012)  & 0.98 (0)       & 0.43 (0.009)  & 0.27 (0.003)\\
    ~              & (300,300,150) & 0.69 (0.013)  & 0.98 (0)       & 0.39 (0.008)  & 0.26 (0.02) \\
    $(30/p, 30/q)$ & (200,200,100) & 0.78 (0.024)  & 0.98 (0)       & 0.43 (0.012)  & 0.31 0.003) \\
    ~              & (200,200,200) & 0.69 (0.026)  & 0.98 (0.001)   & 0.5 (0.02)    & 0.29 (0.004)\\\hline
    \end{tabular}
    \caption{Table of outputs for joint estimation in presence of group misspecification}
    \label{table:simtable2}
\end{table}
\end{scriptsize}

\begin{scriptsize}
\begin{table}[t]
\centering
    \begin{tabular}{ccc}
    \hline
    $(\pi_x, \pi_y)$ & $(p,q,n)$   & FDR          \\\hline
    $(5/p, 5/q)$   & (60,30,100)   & 0.19 (0.077) \\
    ~              & (30,60,100)   & 0.08 (0.064) \\
    ~              & (200,200,150) & 0.04 (0.016) \\
    ~              & (300,300,150) & 0.02 (0.007) \\\hline
    $(30/p, 30/q)$ & (200,200,100) & 0.03 (0.019) \\
    ~              & (200,200,200) & 0.03 (0.016) \\\hline
    \end{tabular}
    \caption{Table of outputs giving empirical FDR for estimating $\cB$ using JMMLE in presence of group misspecification}
    \label{table:simtable22}
\end{table}
\end{scriptsize}

\subsection{Simulation 2: testing}
\label{sec:eval-testing}
We slightly change the data generating model to evaluate our proposed global testing and FDR control procedure. We set $K=2$, then generate the $\bfB_0^1$ by first randomly assigning each of its element to be non-zero with probability $\pi$, then drawing values of those elements from $\text{Unif}\{ [ -1, -0.5] \cup [0.5,1]\}$ independently. After this we generate a matrix of differences $\bfD$, where $(\bfD)_{ij}, i \in \cI_p, j \in \cI_q$ takes values --1, 1, 0 with probabilities 0.1, 0.1 and 0.8, respectively. Finally we set $\bfB_0^2 = \bfB_0^1 + \bfD$. We set identical sparsity structures for the pairs of precision matrices $\{ \Omega_{x0}^1, \Omega_{x0}^2 \}$ and $\{ \Omega_{y0}^1, \Omega_{y0}^2 \}$. We use 50 replications of the above setup to calculate empirical power of global tests, as well as empirical power and FDR of simultaneous tests. To get the empirical sizes of global tests we use estimators obtained from applying JMMLE on a separate set of data generated setting all elements of $\bfD$ to 0. The type-I error of global tests is controlled at level 0.05, while FDR is set at 0.2 obtained by calculating the respective thresholds.

Table~\ref{table:simtable3} reports the empirical mean and standard deviations (in brackets) of all relevant quantities. We report outputs for all combinations of data dimensions and sparsity used in Section~\ref{sec:eval-jmmle}, and also for increased sample sizes in each setting until a satisfactory FDR is reached. As expected from the theoretical analysis, higher sample sizes than those used in Section~\ref{sec:eval-jmmle} result in increased power for both global and simultaneous tests, and decreased size and FDR for all but one ($p=30, q=60$) of the settings.

%
\begin{scriptsize}
\begin{table}
    \begin{tabular}{ccccccc}
    \hline
$(\pi_x, \pi_y)$ & $(p,q)$   & $n$ & \multicolumn{2}{c}{Global test} & \multicolumn{2}{c}{Simultaneous tests}\\\cline{4-7}
 & & & Power     & Size			   & Power         & FDR           \\ \hline
    $(5/p, 5/q)$ & (60,30)   & 100 & 0.977 (0.018) & 0.058 (0.035) & 0.937 (0.021) & 0.237 (0.028) \\
    ~            & ~         & 200 & 0.987 (0.016) & 0.046 (0.032) & 0.968 (0.013) & 0.218 (0.032) \\
    ~            & (30,60)   & 100 & 0.985 (0.018) & 0.097 (0.069) & 0.925 (0.022) & 0.24 (0.034)  \\
    ~            & ~         & 200 & 0.990 (0.02)  & 0.119 (0.059) & 0.958 (0.024) & 0.245 (0.041) \\
    ~            & (200,200) & 150 & 0.987 (0.005) & 0.004 (0.004) & 0.841 (0.13)  & 0.213 (0.007) \\
    ~            & (300,300) & 150 & 0.988 (0.002) & 0.002 (0.003) & 0.546 (0.035) & 0.347 (0.017) \\
    ~            & ~         & 300 & 0.998 (0.003) & 0.000 (0.001) & 0.989 (0.003) & 0.117 (0.006) \\ \hline
  $(30/p, 30/q)$ & (200,200) & 100 & 0.994 (0.005) & 0.262 (0.06)  & 0.479 (0.01)  & 0.557 (0.006) \\
    ~            & ~         & 200 & 0.998 (0.004) & 0.020 (0.01)  & 0.962 (0.003) & 0.266 (0.007) \\
    ~            & ~         & 300 & 0.999 (0.002) & 0.011 (0.008) & 0.990 (0.004) & 0.185 (0.009) \\ \hline
    \end{tabular}
    \caption{Table of outputs for hypothesis testing.}
    \label{table:simtable3}
\end{table}
\end{scriptsize}

\subsection{Computation}
\label{sec:tricks-jmmle}
We now discuss some observations and strategies that speed up the JMMLE algorithm and reduce computation time significantly, especially for higher number of features in either layer.

\paragraph{Block update and refit $\bfB^k$ in each iteration.} Similar to the case of $K=1$ \citep{LinEtal16}, we use block coordinate descent {\it within} each $\bfB^k$. This means instead of the full update step \eqref{eqn:update-B} we perform the following steps in each iteration to speed up convergence:
%
$$
\left\{\widehat \bfB^{k (t+1)}_j \right\}_{k=1}^K =
\argmin_{\substack{\bfb_j^k \in \BR^p\\k \in \cI_K}} \left\{ \frac{1}{n} \sum_{j=1}^q \sum_{k=1}^K \| \bfY^k_j + \bfr_j^{k (t)} - \bfX^k \bfB_j^{k } \|^2
+ \lambda \sum_{h \in \cH} \| \bfB_j^{[h]} \| \right\}
$$
%
where $\bfr_1^{k (t)} = \widehat \bfE_{-1}^{k (t)} \widehat \bftheta_1^{k (t)}$, and
%
$$
\bfr_j^{k (t)} = \sum_{j'=1}^{j-1} \hat \bfe_j^{k (t+1)} \hat \theta_{jj'}^{k (t)} +
\sum_{j'=j+1}^{q} \hat \bfe_j^{k (t)} \hat \theta_{jj'}^{k (t)}
$$
%
for $j \geq 2$. Further, when starting from the initializer of the coefficient matrix given in \eqref{eqn:init-B}, the support set of coefficient estimates becomes constant after only a few ($< 10$) iterations of our algorithm, after which it refines the values inside the same support until overall convergence. This process speeds up significantly if a refitting step is added {\it inside each iteration} after the matrices $\widehat \bfB^k$ are updated:
%
\begin{align*}
\left\{\widetilde \bfB^{k (t+1)}_j \right\}_{k=1}^K &=
\argmin_{\substack{\bfb^k \in \BR^p\\k \in \cI_K}} \left\{ \frac{1}{n} \sum_{j=1}^q \sum_{k=1}^K \| \bfY^k_j + \bfr_j^{k (t)} - \bfX^k \bfB_j^{k } \|^2
+ \lambda \sum_{h \in \cH} \| \bfB^{[h]}_{-j} \| \right\}; \\
\widehat \bfB^{k (t+1)}_j &= \left[ (\bfX_{\cS_{jk}}^k)^T (\bfX_{\cS_{jk}}^k) \right]^- (\bfX_{\cS_{jk}}^k)^T \bfY_j^k
\end{align*}
%
where $\cS_{jk} = \supp(\widetilde \bfB^{k (t+1)}_j)$.

\paragraph{One-step estimator.} Algorithm~\ref{algo:jmmle-algo}, even after the above modifications, is computation-intensive. The reason behind this is the full tuning and updating of the lower layer neighborhood estimates $\{ \widehat \Theta_j \}$ in each iteration. In practice, the algorithm speeds up significantly without compromising on estimation accuracy if we dispense of the $\Theta$ update step in all, but the last iteration. More precisely, we consider the following one-step version of the original algorithm.

\begin{Algorithm}
(The one-step JMMLE Algorithm)
\label{algo:jmmle-algo-1step}

\noindent 1. Initialize $\widehat \cB$ using \eqref{eqn:init-B}.

\noindent 2. Initialize $\widehat \Theta$ using \eqref{eqn:init-Theta}.

\noindent 3. Update $\widehat \cB$ as:
%
\begin{align*}
\widehat \cB^{(t+1)} &= \argmin_{\substack{\bfB^k \in \BM(p,q)\\k \in \cI_K}} \left\{ \frac{1}{n} \sum_{j=1}^q \sum_{k=1}^K \| \bfY^k_j - (\bfY_{-j}^k - \bfX^k \bfB_{-j}^k) \widehat \bftheta_j^{k (0)} - \bfX^k \bfB_j^{k } \|^2
+ \lambda_n \sum_{h \in \cH} \| \bfB^{[h]} \| \right\}
\end{align*}

\noindent 4. Continue till convergence to obtain $\widehat \cB = \{ \widehat \bfB^k \}$.

\noindent 5. Obtain $\widehat \bfE^k := \bfY^k - \bfX^k \widehat \bfB^k, k \in \cI_K$. Update $\widehat \Theta$ as:
%
\begin{align*}
\widehat \Theta_j = \argmin_{\Theta_j \in \BM(q-1, K)}
\left\{ \frac{1}{n} \sum_{k=1}^K
\| \widehat \bfE_j^k - \widehat \bfE_{-j}^k \bftheta_j^k \|^2
+ \gamma \sum_{j \neq j'} \sum_{g \in \cG_y^{jj'}} \| \bftheta_{jj'}^{[g]} \| \right\}
\end{align*}

\noindent 6. Calculate $\widehat \Omega_y^k, k \in \cI_K$ using \eqref{eqn:omega-y-calc}.
\end{Algorithm}

Compared to one-step algorithms based on first order approximation of the objective function \citep{ZouLi08,Taddy17}, we let $\cB$ converge completely, then use these solutions to recover the support set of the precision matrices. The estimation accuracy of $\Omega_y$ depends on the solution $\widehat \cB$ used to solve the sub-problem \eqref{eqn:EstEqn2} (Theorem~\ref{thm:thm-Theta} and Lemmas \ref{thm:ThetaThm} and \ref{prop:ErrorRE}). Thus, letting $\cB$ converge first ensures that the solutions $\widehat \Theta$ and $\widehat \Omega_y$ obtained subsequently are of a better quality compared to a simple early stopping of the JMMLE algorithm.

We compared the performance of both versions of our algorithm for the two data settings with smaller feature dimensions. Computations were performed on the HiperGator supercomputer\footnote{\url{https://www.rc.ufl.edu/services/hipergator}}, in parallel across 8 cores of an Intel E5-2698v3 2.3GHz processor with 2GB RAM per core, the parallelization being done across the range of values for $\lambda_n$ within each replication. As seen in Table~\ref{table:simtable41}, performance is  indistinguishable across all the metrics, but the one-step algorithm saves a significant amount of computation time compared to the full version (Table~\ref{table:simtable42}).

% p30, q=60: full- 118552, 1-step- 18772 secs for 100 replications
%\begin{table}
%\centering
%    \begin{tabular}{cccccc}
%    \hline
%    $(p,q,n)$     & Method         & TP$(\widehat \cB)$            & TN$(\widehat \cB)$            & F1$(\widehat \cB)$ & RF$(\widehat \cB)$             \\\hline
%    (60,30,100) & Full         & 0.999 (0.002) & 0.992 (0.009) & ~   & 0.195 (0.021)                \\
%    ~           & One step     & 0.999 (0.002) &  0.993 (0.01) & ~   & 0.190 (0.019)                 \\
%    (30,60,100) & Full          & 0.997 (0.004) & 0.986 (0.007) & ~   & 0.205 (0.014)               \\
%    ~           & One step & 0.996 (0.004) & 0.988 (0.006) & ~   & 0.206 (0.014)             \\ \hline
%    \hline
%    $(p,q,n)$     & Method         & TP$(\widehat \Omega)$            & TN$(\widehat \Omega)$            & F1$(\widehat \Omega)$ & RF$(\widehat \Omega)$            \\\hline
%    (60,30,100) & Full         & 0.671 (0.052) & 0.949 (0.01) & ~   & 0.327 (0.015)                \\
%    ~           & One step     & 0.663 (0.058) & 0.95 (0.01)   & ~   & 0.328 (0.018)               \\
%    (30,60,100) & Full          & 0.58 (0.039) & 0.982 (0.003) & ~   & 0.32 (0.009)             \\
%    ~           & One step & 0.577 (0.035) & 0.981 (0.003)  & ~   & 0.321 (0.008)            \\ \hline
%    \end{tabular}
%    \caption{Comaprison of evaluation metrics for full and one-step versions of the JMMLE algorithm.}
%    \label{table:simtable41}
%\end{table}
\begin{table}
\centering
    \begin{tabular}{cccccc}
    \hline
    $(p,q,n)$     & Method         & TPR$(\widehat \cB)$            & TNR$(\widehat \cB)$            & MCC$(\widehat \cB)$ & RF$(\widehat \cB)$             \\\hline
    (60,30,100) & Full         & 0.982 (0.013) & 0.994 (0.003) & 0.959 (0.014)   & 0.23 (0.021)  \\
    ~           & One step     & 0.971 (0.02)  & 0.996 (0.003) & 0.965 (0.014)   & 0.242 (0.033) \\
    (30,60,100) & Full         & 0.966 (0.015) & 0.991 (0.003) & 0.954 (0.008)   & 0.269 (0.026) \\
    ~           & One step     & 0.968 (0.013) & 0.992 (0.002) & 0.957 (0.008)   & 0.265 (0.024) \\ \hline
    \hline
    $(p,q,n)$     & Method         & TPR$(\widehat \Omega_y)$            & TNR$(\widehat \Omega_y)$            & MCC$(\widehat \Omega_y)$ & RF$(\widehat \Omega_y)$            \\\hline
    (60,30,100) & Full         & 0.756 (0.019) & 0.907 (0.005)  & 0.616 (0.021)   & 0.318 (0.007) \\
    ~           & One step     & 0.764 (0.018) & 0.904 (0.006)  & 0.678 (0.024)   & 0.321 (0.008) \\
    (30,60,100) & Full         & 0.695 (0.016) & 0.943 (0.002)  & 0.552 (0.015)   & 0.304 (0.005) \\
    ~           & One step     & 0.696 (0.018) & 0.943 (0.002)  & 0.552 (0.018)   & 0.304 (0.005) \\\hline
    \end{tabular}
    \caption{Comaprison of evaluation metrics for full and one-step versions of the JMMLE algorithm.}
    \label{table:simtable41}
\end{table}

\begin{table}[t]
\centering
  \begin{tabular}{ccc}
    \hline
    $(p,q,n)$     & Method   & Comp. time (min) \\ \hline
    (60,30,100) & Full     & 6.1              \\ 
    ~           & One-step & 0.7              \\ \hline
    (30,60,100) & Full     & 22.4             \\ 
    ~           & One-step & 2.7              \\ \hline
    \end{tabular}
    \caption{Comaprison of computation times (averaged over 50 replications) for full and one-step versions of the JMMLE algorithm.}
    \label{table:simtable42}
\end{table}

%\paragraph{Parallelization.} While training JMMLE models for multiple values for $\lambda$, it is possible to speed up 

