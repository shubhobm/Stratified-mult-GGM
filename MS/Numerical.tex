\section{Numerical performance}
\label{sec:sec4}
In this section, we evaluate the performance of our proposed JMMLE algorithm and the hypothesis testing framework in a two-layer simulation setup (Sections \ref{sec:eval-jmmle} and \ref{sec:eval-testing}), and also introduce some computational techniques that significantly accelerates computation for high data dimensions (Section~\ref{sec:tricks-jmmle}).

\subsection{Simulation 1: estimation}
\label{sec:eval-jmmle}
As a first step towards obtaining a two-layer structure with horizontal (across $k$) complexity and inter-layer directed edges, we generate the precision matrices $\{ \Omega_{x0}^k \}$ and $\{ \Omega_{y0}^k \}$ using a dependency structure across $k$ that was first used in the simulation study of \cite{MaMichailidis15}. We set $K=5$, and set different shared sparsity patterns across $k$ inside the lower $p/2 \times p/2$ block of the upper layer precision matrices, and outside the block. In our notation, this means the following elementwise group structure:
%
$$
\cG_{x,ii'} = \begin{cases}
\{ (1,2),(3,4), 5 \} \text{ if } i \leq p/2 \text{ or } j \leq p/2\\
\{ (1,3,5),(2,4) \} \text{ otherwise }
\end{cases}
$$
%
The schematic in Figure~\ref{fig:sim-structure} illustrates this structure. We set an off-diagonal element inside each of these common blocks (i.e. $A,B,C$ and $\alpha, \beta$ in the figure) to be non-zero with probability $\pi_x \in \{ 5/p, 30/p \}$, then generate the values of all non-zero elements independently from the uniform distribution in the interval $[-1, 0.5] \cup [0.5, 1]$. The precision matrices $\Omega_{x0}^k$ are generated by putting together the corresponding common blocks, their positive definiteness ensured by setting all diagonal elements to be $1 + |\Lambda_{\min} (\Omega_{x0}^k)|$. We get elements in the covariance matrix as
%
$$
\sigma_{x0,ii'}^k = (\omega_{x0,ii'}^k)^{-1} / \sqrt{(\omega_{x0,ii}^k)^{-1} (\omega_{x0,i'i'}^k)^{-1} },
$$
and generate rows of $\bfX^k$ independently from $\cN(0, \Sigma_{x0}^k)$. We obtain $\Sigma_{y0}^k$ and then $\bfE^k$ using the same setup but with the number of variables being $q$ and setting off-diagonal elements non-zero with probability $\pi_y \in \{ 5/q, 30/q \}$. To obtain the matrices $\bfB_0^k$, for a fixed $(i,j), i \in \cI_p, j \in \cI_q$, we set $\bfb_{ij}^k$ non-zero across all $k$ with probability $\pi \in \{ 5/p, 5/q \}$, generate the non-zero groups independently from $\text{Unif} \{ [-1, 0.5] \cup [0.5, 1] \}$, and set $\bfY^k = \bfX^k \bfB_0^k + \bfE^k, k \in \cI_K$. Finally, we generate 50 such independent two-layer datasets for each of the following model settings:

\begin{itemize}
\item Set $\pi_x = \pi = 5/p, \pi_y = 5/q$, and
%
$$
(p,q,n) \in \{ (60,30,100), (30,60,100), (200,200,150), (300,300,150) \};
$$

\item Set $\pi_x = \pi = 30/p, \pi_y = 30/q$, and $(p,q,n) \in  \{ (200,200,100), (200,200,200) \}$.
\end{itemize}
%

\begin{figure}
\centering
\includegraphics{omega-structure}
\caption{Shared sparsity patterns across $k$ for the precision matrices $\{ \Omega_{x0}^k\}$ and $\{ \Omega_{y0}^k\}$}
\label{fig:sim-structure}
\end{figure}

We use the following array of tuning parameters to train Algorithm~\ref{algo:jmmle-algo}:
%
$$
\gamma \in \left\{ 0.3, 0.4, ..., 1 \right\} \sqrt{\frac{\log q}{n}}; \quad
\lambda \in \left\{ 0.4, 0.6, ..., 1.8 \right\} \sqrt{\frac{\log p}{n}}
$$
%
using the one-step version (Section~\ref{sec:tricks-jmmle}) instead of the full algorithm to save computation time. We compare the performance of our joint estimation method with separate estimates of the parameters using the method of \cite{LinEtal16}, using the following performance metrics to evaluate estimates $\tilde \cB = \{ \tilde \bfB^k \}$:

\begin{itemize}
\item True positive Rate-
%
\[
\text{TP}(\tilde \bfB_k) = | \supp(\hat \bfB^k) \cap \supp (\bfB_0^k)|; \quad
\text{TPR} (\tilde \cB) = \frac{\sum_k \text{TP}(\tilde \bfB_k) }{\sum_k | \supp (\bfB_0^k)| }
\]
\item True negative Rate-
%
\[
\text{TN}(\tilde \bfB_k) = | {\supp}^c (\hat \bfB^k) \cap {\supp}^c (\bfB_0^k)|; \quad
\text{TNR} (\tilde \cB) = \frac{\sum_k \text{TN}(\tilde \bfB_k) }{\sum_k | \supp^c (\bfB_0^k)| }
\]
%
\item F1 measure-
%
$$
\text{FP}(\tilde \bfB_k) = | {\supp}^c (\bfB^k_0) | - \text{TN}(\tilde \bfB_k); \quad
\text{FN}(\tilde \bfB_k) = | {\supp} (\bfB^k_0) | - \text{TP}(\tilde \bfB_k) $$
$$ \text{F1} (\tilde \cB) = \frac{ \sum_k \text{TP}(\tilde \bfB_k)}
{2 \sum_k \text{TP}(\tilde \bfB_k) + \text{FP}(\tilde \bfB_k) + \text{FN}(\tilde \bfB_k)}
$$
%
\item Relative error in Frobenius norm-
%
\[
\text{RF} (\tilde \cB) = \sum_{k=1}^K \frac{\| \hat \bfB^k - \bfB_0^k \|_F}{\| \bfB_0^k \|_F}
\]
%
\end{itemize}
%
We use the same metrics to evaluate the precision matrix estimates $\tilde \Omega_y^k$ as well.

Tables \ref{table:simtable11} and \ref{table:simtable12} summarize the results. Joint estimation vastly outperforms separate estimation for $\cB_0$ across all metrics. JMMLE tends to be conservative for the estimation of $\Omega_{y0}^k$, producing sparse estimates and very high true negative proportions, although they are still far more accurate than those obtained from separate estimation, as evident by the low average RF scores.
%

\begin{scriptsize}
\begin{table}
    \begin{tabular}{ccccccc}
    \hline
    $(\pi_x, \pi_y)$ & $(p,q,n)$   & Method   & TP            & TN             & F1 & RF            \\ \hline
    $(5/p, 5/q)$   & (60,30,100)   & JMMLE    & 0.97 (0.045) & 0.99 (0.006)   & 0.97 (0.03)  & 0.24 (0.073) \\
    ~              & ~             & Separate & 0.95 (0.018) & 0.99 (0.002)  & ~   & 0.27 (0.031) \\
    ~              & (30,60,100)   & JMMLE    & 0.97 (0.029) & 0.99 (0.005)  & 0.96 (0.015)   & 0.27 (0.053) \\
    ~              & ~             & Separate & 0.66 (0.038) & 0.99 (0.001) & ~   & 0.59 (0.033) \\
    ~              & (200,200,150) & JMMLE    & 0.98 (0.025) & 1.0 (0)      & 0.99 (0.012)   & 0.16 (0.056) \\
    ~              & ~             & Separate & ~             & ~              & ~   & ~             \\
    ~              & (300,300,150) & JMMLE    & 0.99 (0.015) & 1.0 (0)      & 0.99 (0.008)   & 0.14 (0.033)\\
    ~              & ~             & Separate & ~             & ~              & ~   & ~             \\\hline
    $(30/p, 30/q)$ & (200,200,100) & JMMLE    & 0.97 (0.039) & 1.0 (0)      & 0.98 (0.017)   & 0.21 (0.07) \\
    ~              & ~             & Separate & ~             & ~              & ~   & ~             \\
    ~              & (200,200,200) & JMMLE    & 0.99 (0.01)  & 1.0 (0)      & 0.99 (0.017)   & 0.13 (0.036) \\
    ~              & ~             & Separate & ~             & ~              & ~   & ~             \\ \hline
    \end{tabular}
    \caption{Table of outputs for joint and separate estimation of regression matrices, giving empirical mean and standard deviation (in brackets) of each evaluation metric over 50 replications.}
    \label{table:simtable11}
\end{table}
%
\begin{table}
    \begin{tabular}{ccccccc}
    \hline
    $(\pi_x, \pi_y)$ & $(p,q,n)$   & Method   & TP            & TN             & F1 & RF            \\ \hline
    $(5/p, 5/q)$   & (60,30,100)   & JMMLE    & 0.76 (0.041) & 0.90 (0.014)   & 0.68 (0.048)   & 0.32 (0.018) \\
    ~              & ~             & Separate & 0.89 (0.018) & 0.63 (0.014)  & ~   & 0.77 (0.044) \\
    ~              & (30,60,100)   & JMMLE    & 0.7 (0.035) & 0.94 (0.003)  & 0.58 (0.039)   & 0.3 (0.01) \\
    ~              & ~             & Separate & 0.62 (0.027) & 0.81 (0.007)  & ~   & 0.43 (0.011) \\
    ~              & (200,200,150) & JMMLE    & 0.68 (0.037) & 0.98 (0.001)  & 0.47 (0.031)   & 0.26 (0.005) \\
    ~              & ~             & Separate & ~             & ~              & ~   & ~             \\
    ~              & (300,300,150) & JMMLE    & 0.71 (0.03)  & 0.98 (0)      & 0.41 (0.019)   & 0.25 (0.004) \\
    ~              & ~             & Separate & ~             & ~              & ~   & ~             \\\hline
    $(30/p, 30/q)$ & (200,200,100) & JMMLE    & 0.77 (0.037) & 0.98 (0.001)  & 0.42 (0.034)   & 0.31 (0.008) \\
    ~              & ~             & Separate & ~             & ~              & ~   & ~             \\
    ~              & (200,200,200) & JMMLE    & 0.76 (0.04)  & 0.98 (0.001)  & 0.54 (0.035)   & 0.27 (0.008) \\
    ~              & ~             & Separate & ~             & ~              & ~   & ~             \\ \hline
    \end{tabular}
    \caption{Table of outputs for joint and separate estimation of lower layer precision matrices over 50 replications.}
    \label{table:simtable12}
\end{table}
\end{scriptsize}

\subsubsection{Effect of heterogeneity}
We repeat the above setups to check the performance of JMMLE in presence of within-group misspecification. For this we set individual elements in a non-zero group $\{ b_{ij}^k, k \in \cI_K \}$ to be non-zero with probability 0.2, then pass JMMLE estimates of $\bfB_0^k$ through the FDR controlling thresholds as given in \eqref{eqn:fdr-threshold}. The results are summarized in Tables \ref{table:simtable2} and \ref{table:simtable22}. For each setting, TP and TN rates are almost identical to the fully specified rates in Table~\ref{table:simtable11}, thus the thresholding step is effective. In all cases the empirical FDR for estimating $\cB$ is below 0.2. The estimation quality of $\Theta$, however, suffers more. This is expected, as the estimates $\widehat \Omega_y^k$ are obtained from neighborhood coefficients that are calculated based on the pre-thresholding coefficient estimates.
\begin{scriptsize}
\begin{table}
\centering
    \begin{tabular}{cccccc}
    \hline
    $(\pi_x, \pi_y)$ & $(p,q,n)$   & TP$(\widehat \cB)$            & TN$(\widehat \cB)$             & F1$(\widehat \cB)$ & RF$(\widehat \cB)$    \\ \hline
    $(5/p, 5/q)$   & (60,30,100)   & 0.98 (0.022)  & 0.99 (0.005)   & 0.89 (0.04)   & 0.29 (0.031) \\
    ~              & (30,60,100)   & 0.94 (0.048)  & 0.99 (0.008)   & 0.93 (0.033)  & 0.31 (0.062) \\
    ~              & (200,200,150) & 0.99 (0.004)  & 0.99 (0)       & 0.98 (0.008)  & 0.17 (0.015) \\
    ~              & (300,300,150) & 0.99 (0.002)  & 1 (0)          & 0.99 (0.004)  & 0.15 (0.013) \\
    $(30/p, 30/q)$ & (200,200,100) & 0.99 (0.021)  & 1 (0)          & 0.98 (0.011)  & 0.2 (0.031)  \\
    ~              & (200,200,200) & 0.99 (0.02)   & 1 (0)          & 0.98 (0.011)  & 0.15 (0.037) \\\hline
    \hline
    $(\pi_x, \pi_y)$ & $(p,q,n)$   & TP$(\widehat \Omega)$            & TN$(\widehat \Omega)$             & F1$(\widehat \Omega)$ & RF$(\widehat \Omega)$            \\ \hline
    $(5/p, 5/q)$   & (60,30,100)   & 0.71 (0.054)  & 0.90 (0.012)   & 0.64 (0.044)  & 0.34 (0.018)\\
    ~              & (30,60,100)   & 0.7 (0.043)   & 0.94 (0.004)   & 0.59 (0.03)   & 0.3 (0.01)  \\
    ~              & (200,200,150) & 0.62 (0.027)  & 0.98 (0.001)   & 0.43 (0.021)  & 0.27 (0.006)\\
    ~              & (300,300,150) & 0.69 (0.03)   & 0.98 (0)       & 0.39 (0.02)   & 0.26 (0.05) \\
    $(30/p, 30/q)$ & (200,200,100) & 0.78 (0.054)  & 0.98 (0.001)   & 0.43 (0.037)  & 0.31 0.008) \\
    ~              & (200,200,200) & 0.69 (0.059)  & 0.98 (0.001)   & 0.5 (0.044)   & 0.29 (0.009)\\\hline
    \end{tabular}
    \caption{Table of outputs for joint estimation in presence of group misspecification}
    \label{table:simtable2}
\end{table}
\end{scriptsize}

\begin{scriptsize}
\begin{table}[t]
\centering
    \begin{tabular}{ccc}
    \hline
    $(\pi_x, \pi_y)$ & $(p,q,n)$   & FDR          \\\hline
    $(5/p, 5/q)$   & (60,30,100)   & 0.19 (0.077) \\
    ~              & (30,60,100)   & 0.08 (0.064) \\
    ~              & (200,200,150) & 0.04 (0.016) \\
    ~              & (300,300,150) & 0.02 (0.007) \\\hline
    $(30/p, 30/q)$ & (200,200,100) & 0.03 (0.019) \\
    ~              & (200,200,200) & 0.03 (0.016) \\\hline
    \end{tabular}
    \caption{Table of outputs giving empirical FDR for estimating $\cB$ using JMMLE in presence of group misspecification}
    \label{table:simtable22}
\end{table}
\end{scriptsize}

\subsection{Simulation 2: testing}
\label{sec:eval-testing}
We slightly change our data generating model to evaluate our proposed global testing and FDR control procedure. We set $K=2$, then generate the $\bfB_0^1$ by first randomly assigning each of its element to be non-zero with probability $\pi$, then drawing values of those elements from $\text{Unif}\{ [ -1, -0.5] \cup [0.5,1]\}$ independently. After this we generate a matrix of differences $\bfD$, with $(\bfD)_{ij}, i \in \cI_p, j \in \cI_q$ taking values -1, 1, 0 with probabilities 0.1, 0.1 and 0.8, respectively. Finally we set $\bfB_0^2 = \bfB_0^1 + \bfD$. We set the same sparsity structures for the pairs of precision matrices $\{ \Omega_{x0}^1, \Omega_{x0}^2 \}$ and $\{ \Omega_{y0}^1, \Omega_{y0}^2 \}$. We use 50 replications of the above setup to calculate empirical power of global tests, as well as empirical power and FDR of simultaneous tests, while to get size of global tests we use JMMLE estimators from a separate set of data generated setting all elements of $\bfD$ to 0. The type-I error of global tests is taken as 0.05, while FDR is set at 0.2 while calculating the respective thresholds.

Table~\ref{table:simtable3} reports the empirical mean and standard deviations (in brackets) of all relevant quantities. We report outputs for all combinations of data dimensions and sparsity used in Section~\ref{sec:eval-jmmle}, and also for increased sample sizes in each setting until a satisfactory FDR is reached. As expected, higher sample sizes result in increased power for both global and simultaneous tests, and decreased size and FDR for all but one ($p=30, q=60$) of the settings.

%
\begin{scriptsize}
\begin{table}
    \begin{tabular}{ccccccc}
    \hline
$(\pi_x, \pi_y)$ & $(p,q)$   & $n$ & \multicolumn{2}{c}{Global test} & \multicolumn{2}{c}{Simultaneous tests}\\\cline{4-7}
 & & & Power     & Size			   & Power         & FDR           \\ \hline
    $(5/p, 5/q)$ & (60,30)   & 100 & 0.977 (0.018) & 0.058 (0.035) & 0.937 (0.021) & 0.237 (0.028) \\
    ~            & ~         & 200 & 0.987 (0.016) & 0.046 (0.032) & 0.968 (0.013) & 0.218 (0.032) \\
    ~            & (30,60)   & 100 & 0.985 (0.018) & 0.097 (0.069) & 0.925 (0.022) & 0.24 (0.034)  \\
    ~            & ~         & 200 & 0.990 (0.02)  & 0.119 (0.059) & 0.958 (0.024) & 0.245 (0.041) \\
    ~            & (200,200) & 150 & 0.987 (0.005) & 0.004 (0.004) & 0.841 (0.13)  & 0.213 (0.007) \\
    ~            & (300,300) & 150 & 0.988 (0.002) & 0.002 (0.003) & 0.546 (0.035) & 0.347 (0.017) \\
    ~            & ~         & 300 & 0.998 (0.003) & 0.000 (0.001) & 0.989 (0.003) & 0.117 (0.006) \\ \hline
  $(30/p, 30/q)$ & (200,200) & 100 & 0.994 (0.005) & 0.262 (0.06)  & 0.479 (0.01)  & 0.557 (0.006) \\
    ~            & ~         & 200 & 0.998 (0.004) & 0.020 (0.01)  & 0.962 (0.003) & 0.266 (0.007) \\
    ~            & ~         & 300 & 0.999 (0.002) & 0.011 (0.008) & 0.990 (0.004) & 0.185 (0.009) \\ \hline
    \end{tabular}
    \caption{Table of outputs for hypothesis testing.}
    \label{table:simtable3}
\end{table}
\end{scriptsize}

\subsection{Computation}
\label{sec:tricks-jmmle}
We now discuss some observations and strategies that speed up the JMMLE algorithm and reduces computations time significantly, especially for higher number of features in either layer.

\paragraph{Block update and refit $\bfB^k$ in each iteration.} Similar to the case of $K=1$ \citep{LinEtal16}, we use block coordinate descent {\it within} each $\bfB^k$. This means instead of the full update step \eqref{eqn:update-B} we perform the following steps in each iteration to speed up convergence:
%
$$
\left\{\widehat \bfB^{k (t+1)}_j \right\}_{k=1}^K =
\argmin_{\substack{\bfb_j^k \in \BR^p\\k \in \cI_K}} \left\{ \frac{1}{n} \sum_{j=1}^q \sum_{k=1}^K \| \bfY^k_j + \bfr_j^{k (t)} - \bfX^k \bfB_j^{k } \|^2
+ \lambda \sum_{h \in \cH} \| \bfB_j^{[h]} \| \right\}
$$
%
where $\bfr_1^{k (t)} = \widehat \bfE_{-1}^{k (t)} \widehat \bftheta_1^{k (t)}$, and
%
$$
\bfr_j^{k (t)} = \sum_{j'=1}^{j-1} \hat \bfe_j^{k (t+1)} \hat \theta_{jj'}^{k (t)} +
\sum_{j'=j+1}^{q} \hat \bfe_j^{k (t)} \hat \theta_{jj'}^{k (t)}
$$
%
for $j \geq 2$. Further, when starting from the initializer of the coefficient matrix given in \eqref{eqn:init-B}, the support set of coefficient estimates becomes constant after only a few ($\sim 10$) iterations of our algorithm, after which it refines the values inside the same support until overall convergence. This process speeds up significantly if a refitting step is added {\it in each iteration} after the matrices $\widehat \bfB^k$ are updated:
%
\begin{align*}
\left\{\widetilde \bfB^{k (t+1)}_j \right\}_{k=1}^K &=
\argmin_{\substack{\bfb^k \in \BR^p\\k \in \cI_K}} \left\{ \frac{1}{n} \sum_{j=1}^q \sum_{k=1}^K \| \bfY^k_j + \bfr_j^{k (t)} - \bfX^k \bfB_j^{k } \|^2
+ \lambda \sum_{h \in \cH} \| \bfB^{[h]}_{-j} \| \right\}; \\
\widehat \bfB^{k (t+1)}_j &= \left[ (\bfX_{\cS_{jk}}^k)^T (\bfX_{\cS_{jk}}^k) \right]^- (\bfX_{\cS_{jk}}^k)^T \bfY_j^k
\end{align*}
%
where $\cS_{jk} = \supp(\widetilde \bfB^{k (t+1)}_j)$.

\paragraph{One-step estimator.} Algorithm~\ref{algo:jmmle-algo}, even after the above modifications, is conputation-intensive. The reason behind this is the full tuning and updating of the lower layer neighborhood estimates $\{ \widehat \Theta_j \}$ in each iteration. In practice, the algorithm speeds up significantly without compromising on estimation accuracy if we dispense of the $\Theta_j$ updation step in all but the last iteration. More precisely, we consider the following one-step version of the original algorithm.

\begin{Algorithm}
(The one-step JMMLE Algorithm)
\label{algo:jmmle-algo-1step}

\noindent 1. Initialize $\widehat \cB$ using \eqref{eqn:init-B}.

\noindent 2. Initialize $\widehat \Theta$ using \eqref{eqn:init-Theta}.

\noindent 3. Update $\widehat \cB$ as:
%
\begin{align*}
\widehat \cB^{(t+1)} &= \argmin_{\substack{\bfB^k \in \BM(p,q)\\k \in \cI_K}} \left\{ \frac{1}{n} \sum_{j=1}^q \sum_{k=1}^K \| \bfY^k_j - (\bfY_{-j}^k - \bfX^k \bfB_{-j}^k) \widehat \bftheta_j^{k (0)} - \bfX^k \bfB_j^{k } \|^2
+ \lambda_n \sum_{h \in \cH} \| \bfB^{[h]} \| \right\}
\end{align*}

\noindent 4. Continue till convergence to obtain $\widehat \cB = \{ \widehat \bfB^k \}$.

\noindent 5. Obtain $\widehat \bfE^k := \bfY^k - \bfX^k \widehat \bfB^k, k \in \cI_K$. Update $\widehat \Theta$ as:
%
\begin{align*}
\widehat \Theta_j = \argmin_{\Theta_j \in \BM(q-1, K)}
\left\{ \frac{1}{n} \sum_{k=1}^K
\| \widehat \bfE_j^k - \widehat \bfE_{-j}^k \bftheta_j^k \|^2
+ \gamma \sum_{j \neq j'} \sum_{g \in \cG_y^{jj'}} \| \bftheta_{jj'}^{[g]} \| \right\}
\end{align*}

\noindent 6. Calculate $\widehat \Omega_y^k, k \in \cI_K$ using \eqref{eqn:omega-y-calc}.
\end{Algorithm}

Note that compared to early stopping rules of iterative algorithms \citep{} {\colrbf refs?}, we let $\cB$ converge completely, then use these solutions to recover the support set of the precision matrices. The estimation accuracy of $\Omega_y$ depends on the solution $\widehat \cB$ used to solve the sub-problem \eqref{eqn:EstEqn2} (Theorem~\ref{thm:thm-Theta} and Lemmas \ref{thm:ThetaThm} and \ref{prop:ErrorRE}). Letting $\cB$ converge first ensures that the solutions $\widehat \Theta$ and $\widehat \Omega_y$ obtained subsequently are of a better quality compared to a simple early stopping of the JMMLE algorithm.

Table~\ref{table:simtable41} compares performances the full algorithm and the one-step version for the two data settings with smaller feature dimensions. The performances are indistinguishable across metrics, but the one-step algorithm saves computation time in orders of magnitude (Table~\ref{table:simtable42}).

% p3=, q=60: full- 118552, 1-step- 18772 secs for 100 replications
\begin{table}
\centering
    \begin{tabular}{cccccc}
    \hline
    $(p,q,n)$     & Method         & TP$(\widehat \cB)$            & TN$(\widehat \cB)$            & F1$(\widehat \cB)$ & RF$(\widehat \cB)$             \\\hline
    (60,30,100) & Full         & 0.999 (0.002) & 0.992 (0.009) & ~   & 0.195 (0.021)                \\
    ~           & One step     & 0.999 (0.002) &  0.993 (0.01) & ~   & 0.190 (0.019)                 \\
    (30,60,100) & Full          & 0.997 (0.004) & 0.986 (0.007) & ~   & 0.205 (0.014)               \\
    ~           & One step & 0.996 (0.004) & 0.988 (0.006) & ~   & 0.206 (0.014)             \\ \hline
    \hline
    $(p,q,n)$     & Method         & TP$(\widehat \Omega)$            & TN$(\widehat \Omega)$            & F1$(\widehat \Omega)$ & RF$(\widehat \Omega)$            \\\hline
    (60,30,100) & Full         & 0.671 (0.052) & 0.949 (0.01) & ~   & 0.327 (0.015)                \\
    ~           & One step     & 0.663 (0.058) & 0.95 (0.01)   & ~   & 0.328 (0.018)               \\
    (30,60,100) & Full          & 0.58 (0.039) & 0.982 (0.003) & ~   & 0.32 (0.009)             \\
    ~           & One step & 0.577 (0.035) & 0.981 (0.003)  & ~   & 0.321 (0.008)            \\ \hline
    \end{tabular}
    \caption{Comaprison of evaluation metrics for full and one-step versions of the JMMLE algorithm.}
    \label{table:simtable41}
\end{table}

\begin{table}[t]
\centering
  \begin{tabular}{ccc}
    \hline
    $(p,q,n)$     & Method   & Comp. time (min) \\ \hline
    (60,30,100) & Full     & 5.8              \\ 
    ~           & One-step & 1.0              \\ \hline
    (30,60,100) & Full     & ~             \\ 
    ~           & One-step & 3.2              \\ \hline
    \end{tabular}
    \caption{Comaprison of computation times (averaged over 50 replications) for full and one-step versions of the JMMLE algorithm.}
    \label{table:simtable42}
\end{table}

%\paragraph{Parallelization.} While training JMMLE models for multiple values for $\lambda$, it is possible to speed up 

