\section{Hypothesis testing in multilayer models}
\label{sec:sec3}
In this section, we lay out a framework for hypothesis testing in our proposed joint multilayer structure. Present literature in high-dimensional hypothesis testing either focuses on testing for simularities in the within-layer connections of single-layer networks \citep{CaiLiu16,Liu17}, or coefficients of single response penalized regression \citep{vanDeGeerEtal14,ZhangZhang14,MitraZhang16}. However, to our knowledge no method is available in the literature to perform testing for {\it between-layer} connections in a two-layer (or multilayer) setup.

There are two main challenges in doing the above: firstly the need to mitigate estimation the bias of estimators that are obtained from lasso and group lasso-based procedures and assumptions on the design matrix required for the same, and secondly the dependency among response nodes translating into the need for controlling False Discovery Rate (FDR) while simultaneously testing for such hypotheses. In Section~\ref{sec:testing-subsec-1} we propose a debiased estimator for rows of the coefficient matrix estimates $\bfB^k$ that makes use of already computed (using JSEM) nodewise regression coefficients in the upper layer, and establish asymptotic properties of scaled version of them. Section~\ref{sec:testing-subsec-2} is devoted to pairwise testing, where we asssume $K=2$, and propose asymptotic global tests for detecting differential effects of a variable in the upper layer, as well as pairwise simultaneous tests for detecting elementwise difference in the coefficient matrices across $k$.

%Suppose there are two disease subtypes: $k = 1,2$, and we are interested in testing whether the downstream effect of a predictor in X-data is same across both subtypes, i.e. if $\bfb_{0i}^1 = \bfb_{0i}^2$ for some fixed $i \in \cI_p$. For this we consider the modified optimization problem:
%%
%\begin{align}
%& \min_{\cB, \Theta} \frac{1}{n} \left\{ \sum_{j=1}^q \sum_{k=1}^2 \| \bfY_j^k - \bfY_{-j}^k \bftheta_j^k - \bfX^k \bfB_{j}^k \|^2 + \sum_{j \neq j'} \lambda_{jj'} \| \bftheta_{jj'}^* \| + \sum_{i=1}^p \eta_i \| \bfB_{i*}^* \| \right\} \notag\\
%&= \min \left\{ f ( \cY, \cX, \cB, \Theta) + P (\Theta) + Q (\cB) \right\} 
%\end{align}
%%
%with $n_1 = n_2 = n$ for simplicity; and $\bfB^k = (\bfb_1^k, \ldots, \bfb_q^k), (\bfB_{i*}^*) \in \BR^{ q \times K}$.

\subsection{Debiased estimators and asymptotic normality}
\label{sec:testing-subsec-1}
\cite{ZhangZhang14} proposed a debiasing procedure for lasso estimates and subsequently calculate confidence intervals for individual coefficients $\beta_j$ in high-dimensional linear regression: $\bfy = \bfX \bfbeta + \bfepsilon, \bfy \in \BR^n, \bfX \in \BM(n,p)$ and $\epsilon_r \sim N(0,\sigma^2), r \in \cI_n$ for some $\sigma>0$. Given an initial lasso estimate $\widehat \bfbeta^{(init)} \in \BR^p$ their debiased estimator was defined as:
%
$$
\hat \beta_j^{(\text{deb})} = \hat \beta_j^{(\text{init})} + \frac{\bfz_j^T ( \bfy - \bfX \bfbeta^{(\text{init})})}{\bfz^T \bfx_j}
$$
%
where $\bfz_j$ is the vector of residuals from $\ell_1$-penalized regression of $\bfx_j$ on $\bfX_{-j}$. With centering around the true parameter value, say $\beta_j^0$, and proper scaling this has an asymptotic normal distribution:
%
$$
\frac{\hat \beta_j^{(\text{deb})} - \beta_j^0}{\| \bfz_j \|/| \bfz_j^T \bfx_j |} \sim N(0, \sigma^2)
$$
%
Essentially, they obtain the debiasing factor for the $j^{\Th}$ coefficient by taking residuals from the regularized regression and scale them using the projection of $\bfx_j$ onto a space approximately orthogonal to it. \cite{MitraZhang16} later generalized this idea to group lasso estimates. Further, \cite{vanDeGeerEtal14} and \cite{JavanmardMontanari14} performed debiasing on the entire coefficient vectors.

We start off by defining debiased estimates for individual rows of the coefficient matrices $\bfB^k$ in our two-layer model:
%
\begin{align}\label{eqn:DebiasedBeta}
\widehat \bfc_i^k = \widehat \bfb_i^k + \frac{1}{n t_i^k} \left( \bfX_i^k - \bfX_{-i}^k \widehat \bfzeta_i^k \right)^T (\bfY^k - \bfX^k \widehat \bfB^k )
; \quad i \in \cI_p, k \in \cI_K
\end{align}
%
where $t_i^k = ( \bfX_i^k - \bfX_{-i}^k \widehat \bfzeta_i^k )^T \bfX_{i}^k/n$, and $\widehat \bfzeta_i^k, \widehat \bfB^k$ are {\it generic estimators} of the neighborhood coefficient matrices in the upper layer and within-layer coefficient matrices, respectively. By structure this is similar to the proposal of \cite{ZhangZhang14}. However, as we see shortly, minimal conditions need to be imposed on the parameter estimates used in \eqref{eqn:DebiasedBeta} for the asymptotic results based on a scaled version of the debiased estimator to go thorugh, and they continue to hold for arbitrary sparsity patterns over $k$ in all the parameters.

Present methods of debiasing coefficients from regularized regression require specific assumptions on the regularization structure of the main regression, as well as on how to calculate the debiasing factor. While \cite{ZhangZhang14}, \cite{JavanmardMontanari14} and \cite{vanDeGeerEtal14} work on coefficients from lasso regressions, \cite{MitraZhang16} debias the coefficients of pre-specified groups in the coefficient vector from a group lasso. The current proposals for the debiasing factor available in the literature include nodewise lasso \citep{ZhangZhang14} and a variance minimization scheme with $\ell_\infty$-constraints \citep{JavanmardMontanari14}. In comparison, we only assume the following generic constraints on the parameter estimates used in our procedure.

\vspace{1em}
\noindent{\bf (T1)} For the upper layer neighborhood coefficients, the following holds for all $k \in \cI_K$:
%
$$
\| \widehat \bfzeta^k - \bfzeta_0^k \|_1 \leq D_\zeta  = O \left( \sqrt { \frac{\log p}{n}} \right)
$$
%
where $D_\zeta$ depends only on the true values, i.e. $\{ \zeta^k_0 \}$.

\noindent{\bf (T2)} The lower layer precision matrix estimators satisfy for all $k \in \cI_K$
%
$$
\| \widehat \Omega_y^k - \Omega_{y0}^k \|_\infty \leq D_\Omega
= O \left( \sqrt { \frac{\log (pq)}{n}} \right)$$
%
where $D_\Omega$ depends only on $\Omega_{y 0}$.

\noindent{\bf (T3)} For the regression coefficient matrices, the following holds for all $k \in \cI_K$:
%
$$
\| \widehat \bfB^k - \bfB^k_0 \|_1 \leq D_\beta = O \left( \sqrt { \frac{\log (p q)}{n}} \right)
$$
%
where $D_\beta$ depends on $\cB_0$ only.
\vspace{1em}

Given these conditions, the following theorem provides the asymptotic joint distribution of a scaled version of the debiased coefficients. A similar result for fixed design in the context of single-response linear regression can be found in the preprint by \cite{StuckyVandeGeer17}. However they use nuclear norm as the loss function while obtaining the debiasing factors and use the resulting Karush-Kuhn-Tucker (KKT) conditions to derive their results, whereas we leverage bounds on generic parameter estimates combined with the sub-gaussianity of our design matrices.

\begin{Theorem}\label{Thm:ThmTesting}
Define $\widehat s_i^k = \sqrt{\| \bfX_i^k - \bfX_{-i}^k \widehat \bfzeta_i^k \|^2/n}$, and $m_i^k = \sqrt n t_i^k / \widehat s_i^k$. Consider parameter estimates that satisfy conditions (T1)-(T3). Define the following:
%
\begin{align*}
\widehat \Omega_y &= \diag(\widehat \Omega_y^1, \ldots, \widehat \Omega_y^K)\\
\bfM_i &= \diag(m_i^1, \ldots, m_i^K)\\
\widehat \bfC_i &= \ve(\widehat \bfc_i^1, \ldots, \widehat \bfc_i^K)^T\\
\bfD_i &= \ve(\bfb_{0i}^1, \ldots, \bfb_{0i}^K)^T\\
\end{align*}
%
Also assume that the precision matrices $\Omega_{x0}^k, k \in \cI_K$ are diagonally dominant. Then for sample size satisfying $\log p = o(n^{1/2}), \log q = o(n^{1/2})$ we have
%
\begin{align}\label{eqn:ThmTestingEqn}
\widehat \Omega_y^{1/2} \bfM_i (\widehat \bfC_i - \bfD_i) \sim
\cN_{Kq} ({\bf 0}, \bfI) + \bfR_n
\end{align}
%
where $\| \bfR_n \|_\infty = o_P(1)$.
\end{Theorem}
%

\subsection{Test formulation}
\label{sec:testing-subsec-2}
Now we simply plug in estimators from the JMMLE algorithm in Theorem~\ref{Thm:ThmTesting}. This is fairly straightforward. Condition (T1) is ensured by the JSEM penalized neighborhood estimators (immediate from Proposition A.1 in \cite{MaMichailidis15}), and a bound on total sparsity of the true coefficient matrices: $B = o(\sqrt{n} / \log(pq))$, in conjunction with Corollary~\ref{corollary:jmmle-final}, ensures conditions (T2) and (T3), all with probability approaching 1 as $(n,p,q) \rightarrow \infty$.

An asymptotic joint distribution of debiased versions of the JMMLE regression estimates is now immediate.
%
\begin{Corollary}\label{corollary:CorTesting}
Consider the estimates $\widehat \cB$ and $\widehat \Omega_y $ obtained from Algorithm~\ref{algo:jmmle-algo}, and upper layer neighborhood coefficients from solving the nodewise regression in \eqref{eqn:jsem-model}. Suppose that $\log (pq) /\sqrt n \rightarrow 0$, and the sparsity condition $B = o(\sqrt{n} / \log(pq))$ is satisfied. Then, with the same notations in Theorem~\ref{Thm:ThmTesting} we have
%
\begin{align}\label{eqn:CorTestingEqn}
\widehat \Omega_y^{1/2} \bfM_i (\widehat \bfC_i - \bfD_i) \sim
\cN_{Kq} ({\bf 0}, \bfI) + \bfR_{1n}
\end{align}
%
where $\| \bfR_{1n} \|_\infty = o_P(1)$.
\end{Corollary}

We are now ready to formulate asymptotic global and simultaneous testing procedures based on Corollary~\ref{corollary:CorTesting}. In this paper, we restrict our attention to testing for pairwise differences only. Specifically, we set $K=2$, and are interested in testing whether there are overall and elementwise differences between the coefficient vectors $\bfb_{0i}^1$ and $\bfb_{0i}^2$.

When $\bfb_{0 i}^1 = \bfb_{0 i}^2$, it is immediate from Corollary~\ref{corollary:CorTesting} that a scaled version of the vector of estimated differences $\widehat \bfc_i^1 - \widehat \bfc_i^2$ follows a $q$-variate multinormal distribution. Consequently we formulate a global test for detecting differential overall downstream effect of the $i^{\Th}$ covariate in the upper layer.

\begin{Algorithm}\label{algo:AlgoGlobalTest}
(Global test for $H_0^i: \bfb_{0 i}^1 = \bfb_{0 i}^2$ at level $\alpha, 0< \alpha< 1$)

\noindent 1. Obtain the debiased estimators $\widehat \bfc_i^1, \widehat \bfc_i^2$ using \eqref{eqn:DebiasedBeta}.

\noindent 2. Calculate the test statistic
%
$$
D_i = (\widehat \bfc_i^1 - \widehat \bfc_i^2)^T
\left( \frac{ \widehat \Sigma_y^1}{(m_i^1)^2} +
\frac{\widehat \Sigma_y^2}{(m_i^2)^2} \right)^{-1} (\widehat \bfc_i^1 - \widehat \bfc_i^2)
$$
%
where $\widehat \Sigma_y^k = (\widehat \Omega_y^k)^{-1}, k = 1,2$.

\noindent 3. Reject $H_0^i$ if $D_i \geq \chi^2_{q, 1-\alpha}$.
\end{Algorithm}

Besides controlling the type-I error at a specified level $\alpha$, the above testing procedure maintains rate optimal power.

\begin{Theorem}\label{thm:PowerThm}
Consider the global test given in Algorithm~\ref{algo:AlgoGlobalTest}, performed using parameter estimates satisfying conditions (T1)-(T3). Say $\bfdelta := \bfb_{0 i}^1 - \bfb_{0 i}^2$. Assume that either of the following sufficient conditions are satisfied.

\begin{itemize}
\item[(I)] The following bound holds: $D_\Omega \leq \Delta_0 (\Omega_{y0}^k), k \in \cI_K$;

\item[(II)] For every $j \in \cI_q, k \in \cI_K$, we have
$
\sum_{j' = 1}^q | \sigma_{y0,jj'}^k |^q \leq c_0 (p)
$ for some $q \in [0,1)$ and positive-valued function $c_0(\cdot)$.
\end{itemize}

Then the power of the global test is given by
%
$$
K_q \left( \chi^2_{q,1-\alpha} + n \bfdelta^T 
\left( \frac{ \Sigma_{y 0}^1}{\sigma_{x0, i,-i}^1} + \frac{\Sigma_{y 0}^2}{\sigma_{x0, i,-i}^2} \right)^{-1} \bfdelta \right) + o(1)
$$
%
where $K_q$ is the cumulative distribution function of the $\chi^2_q$ distribution. Consequently, for the minimum difference $ \delta_0 := \min_j | \delta_j| \geq O(\sqrt{ \log (pq)/n} )$, $P( H_0^i \text{ is rejected }) \rightarrow 1$ as $(n,p,q) \rightarrow \infty$.
\end{Theorem}

The conditions (I) or (II) above are needed to derive upper bounds for $\| \widehat \Sigma_y^k - \Sigma_{y0}^k \|_\infty$ using those for $\| \widehat \Omega_y^k - \Omega_{y0}^k \|_\infty$. While (I) imposes a potentially more stringent bound on the estimation error of $\Omega_y$, (II) restricts the power calculations to a uniformity class of covariance matrices \citep{BickelLevina08,CaiLiuLuo12}.
%

\subsection{Control of False Discovery Rate}
Given the null hypothesis is rejected, we consider the multiple testing problem of simultaneously testing for all entrywise differences, i.e. testing
%
$$
H_0^{ij}: b_{0 ij}^1 = b_{0ij}^2 \quad \text{vs.} \quad H_1^{ij}: b_{0 ij}^1 \neq b_{0 ij}^2 
$$
%
for all $j \in \cI_q$. Here we use the test statistic
%
\begin{align}\label{eqn:PairwiseStatistic}
d_{ij} &= \frac{\widehat c_{ij}^1 - \widehat c_{ij}^2}{\sqrt{\hat \sigma_{jj}^1/ (m_i^1)^2 + \hat \sigma_{jj}^2/ (m_i^2)^2}}
\end{align}
%
with $\hat \sigma_{jj}^k$ being the $j^{\Th}$ diagonal element of $\widehat \Sigma_y^k, k = 1,2$.

For the purpose of simultaneous testing, we consider tests with a common rejection threshold $\tau$, i.e. for $j \in \cI_q$, $H_0^{ij}$ is rejected if $| d_{ij} | > \tau$. We denote $\cH_0^i = \{ j: b_{0,ij}^1 = b_{0,ij}^2 \}$ and define the false discovery proportion (FDP) and false discovery rate (FDR) for these tests as follows:
%
$$
FDP (\tau) = \frac{\sum_{j \in \cH_0^i} \BI( |d_{ij}| \geq \tau)}{\max\left\{
\sum_{j \in \cI_q} \BI( |d_{ij}| \geq \tau), 1\right\} }\quad
FDR (\tau) = \BE [ FDP (\tau) ]
$$
%
For a pre-specified level $\alpha$, we choose a threshold that ensures both FDP and FDR $\leq \alpha$ using the Benjamini-Hochberg (BH) procedure. % To do this we define the following:
%%
%\begin{align*}
%P_0 = 2 \Phi (1) - 1; \quad
%\hat P_0 = \frac{1}{q} \sum_{j \in \cH_0^i} \BI (|d_{ij}| \leq 1); \quad
%Q_0 = \sqrt 2 \phi (1);\\
%A = \frac{P_0 - \hat P_0}{Q_0}; \quad A(t) = \left[ 1 + |A| \frac{|t| \phi(t)}{\sqrt 2 (1 - \Phi(t))} \right]^{-1}
%\end{align*}
%%
%where $\Phi(\cdot)$ and $\phi(\cdot)$ are the standard normal distribution and density functions, respectively.   
The procedure for FDR control is now given by Algorithm \ref{algo:AlgoFDR}.

\begin{Algorithm}\label{algo:AlgoFDR}
(Simultaneous tests for $H_0^{ij}: b_{0 ij}^1 = b_{0 ij}^2$ at level $\alpha, 0< \alpha< 1$)

\noindent 1. Calculate the pairwise test statistics $d_{ij}$ using \eqref{algo:AlgoFDR} for $j \in \cI_q$.

\noindent 2. Obtain the threshold
%
$$
\hat \tau = \inf \left\{\tau \in \BR: 1 - \Phi(\tau) \leq \frac{\alpha}{2 q}
\max \left( \sum_{j \in \cI_q} \BI( |d_{ij}| \geq \tau), 1 \right) \right\}
$$
%

\noindent 3. For $j \in \cI_q$, reject $H_0^{ij}$ if $|d_{ij}| \geq \hat \tau$.
\end{Algorithm}

%Denote $\hat \cH_0^i = \{ j\ in \cI_q: |d_{ij}| \geq \hat \tau \}$.
For this procedure to maintain FDR and FDP asymptotically at a pre-specified level $\alpha \in (0,1)$, we need some dependence conditions on true correlation matrices in the lower layer. Following \cite{LiuShao14}, we introduce there are two types of dependency we consider:

\noindent {\bf (D1)} Define $r_{jj'}^k = \sigma_{y0,jj'}^k /\sqrt{\sigma_{y0,jj}^k \sigma_{y0,j'j'}^k}$ for $j,j' \in \cI_q, k = 1,2$. Suppose there exists $0 < r < 1$ such that $\max_{1 \leq j < j' \leq q} | r_{jj'}^k | \leq r$, and for every $j \in \cI_q$,
%
$$
\sum_{j'=1}^q \BI \left\{ |r_{jj'}^k| \geq \frac{1}{(\log q)^{2 + \theta}} \right\} \leq O(q^\rho)
$$
%
for some $\theta > 0$ and $0 < \rho < (1-r)/(1+r)$.

\noindent {\bf (D1*)} Suppose there exists $0 < r < 1$ such that $\max_{1 \leq j < j' \leq q} | r_{jj'}^k | \leq r$, and for every $j \in \cI_q$,
%
$$
\sum_{j'=1}^q \BI \left\{ |r_{jj'}^k| > 0 \right\} \leq O(q^\rho)
$$
%
for some $0 < \rho < (1-r)/(1+r)$.

Originally proposed by \cite{LiuShao14}, the above dependency conditions are meant to control the amount of correlation among the test statistics. Condition (D1) allows each variable to be highly correlated with at most $O(q^\rho)$ other variables and weakly correlated with others, while (D1*) limits the number of variables to have {\it any} correlation with it to $O(q^\rho)$. Note that (D1*) is a stronger condition, and can be seen as the limiting condition of (D1) as $q \rightarrow \infty$.

\begin{Theorem}\label{thm:FDRthm}
Suppose $\mu_j = b_{0,ij}^1 - b_{0,ij}^2, \sigma_j^2 = n \sigma_{y0,jj}^1/ (m_{0i}^1)^2 + \sigma_{y0,jj}^2/ (m_{0i}^2)^2)$. Assume the following holds as $(n,q) \rightarrow \infty$
%
\begin{align}\label{eqn:FDRthmEqn1}
\left| \left\{ j \in \cI_q: |\mu_j / \sigma_j | \geq
4 \sqrt{ \log q/n} \right\} \right| \rightarrow \infty
\end{align}
%
Now Consider the conditions (D1) and (D1*). If (D1) is satisfied, then the following holds when $\log q = O(n^{\xi}), 0 < \xi < 3/23$:
%
\begin{align}\label{eqn:FDReqn}
\frac{FDP( \hat \tau)}{(| \cH_0^i|/q) \alpha} \stackrel{P}{\rightarrow} 1; \quad
\lim_{n, q \rightarrow \infty} \frac{FDR( \hat \tau)}{(| \cH_0^i|/q) \alpha} = 1
\end{align}
%
Further, if (D1*) is satisfied, then \eqref{eqn:FDReqn} holds for $\log q = o(n^{1/3})$.
\end{Theorem}
%
%Theorem~\ref{thm:FDRthm} is essentially a restatement of Theorem 4.1 in \cite{LiuShao14}.
The condition \eqref{eqn:FDRthmEqn1} is essential for FDR control in a diverging parameter space \citep{LiuShao14, Liu17}.

\begin{Remark}
Following \eqref{eqn:FDRthmEqn1}, a sufficient condition on the sparsity of $\cB_0$ for FDR to be asymptotically controlled at some specified level is $B = o(n^\zeta/ \log q)$ if (D1) is satisfied, and $B = o(n^{1/3}/ \log q)$ if (D1*) is satisfied. In comparison, our results for the global testing procedure require $B = o(\sqrt n/ \log (pq))$, and point estimation requires $B = o(n/ \log (pq))$. In finite samples settings, the stricter sparsity requirements translate to higher sample sizes being needed (given the same $(p,q)$) for our testing procedures to have satisfactory performances compared to estimation only (See Sections \ref{sec:eval-jmmle} and \ref{sec:eval-testing}).

In recent work, \cite{JavanmardMontanari18} showed that the $o(\sqrt n/ \log p)$ bound on the sparsity of the true coefficient vector required to construct confidence intervals from debiased lasso coefficient estimates \citep{vanDeGeerEtal14, ZhangZhang14,JavanmardMontanari14} can be weakened to $o(n/ (\log p)^2)$ when the random design precision matrix is known, or is unknown but satisfies certain sparsity assumptions. Similar relaxations may be possible in our case. For example, the machineries in \cite{Liu17}, which performs simultaneous testing in multiple (single layer) GGMs using slightly modified FDR thresholds, can be useful in obtaining \eqref{eqn:FDReqn} for $ \log q = o(n^{1/2})$ under (D1), (D1*) or other suitable dependency assumptions.% However, this requires a significant amount of theoretical analysis, and we leave it for future research.
\end{Remark}

\begin{Remark}
Based on the FDR control procedure in Algorithm~\ref{algo:AlgoFDR}, we can perform {\it within-row thresholding} in the matrices $\widehat \bfB^k$ to tackle group misspecification.
%
\begin{align}
& \hat \tau_i^k := \inf \left\{\tau \in \BR: 1 - \Phi(\tau) \leq \frac{\alpha}{2 q}
\max \left( \sum_{j \in \cI_q} \BI( | \sqrt{\hat \omega_{jj}^k} m_i^k \hat c_{ij}^k | \geq \tau), 1 \right) \right\} \notag\\
& \hat b_{ij}^{k, \text{thr}} =  \hat b_{ij}^k \BI \left(
|\sqrt{\hat \omega_{jj}^k} m_i^k \hat c_{ij}^k | \geq \hat \tau_i^k \right)
\label{eqn:fdr-threshold}
\end{align}
%
Even without group misspecification, this helps identify directed edges between layers that have high nonzero values. Similar post-estimation thresholdings have been proposed in the context of multitask regression \citep{ObozinskiEtal11,MajumdarChatterjeeStat} and neighborhood selection \citep{MaMichailidis15}. However, our procedure is the first one to provide explicit guarantees on the amount of false discoveries while doing so.
\end{Remark}