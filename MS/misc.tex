\section{Proofs of main results}
%{\colrbf need modification}

\begin{proof}[Proof of Theorem~\ref{thm:algo-convergence}]
The theorem is a generalization of Theorem 1 in \cite{LinEtal16}. The proof follows directly from the proof of that theorem, substituting $( \widehat B^{(k)}, \widehat \Theta_\epsilon^{(k)}), (B^*, \Theta_\epsilon^*)$ and $ (B^\infty, \Theta_\epsilon^\infty)$ therein with $(\widehat \cB^{(t)}, \widehat \Theta_y^{(t)}), ( \cB_0, \Theta_{y 0})$ and $ (\cB^\infty, \Theta_y^\infty)$, and corresponding variations as required.

\end{proof}

We use the following condition extensively while deriving the results that follow.

\vspace{1em}
\noindent {\bf Condition 3} (Restricted eigenvalues). A symmetric matrix $\bfM \in \BM(b,b)$ is said to satisfy the restricted eigenvalue or RE condition with parameters $\psi, \phi >0$, denoted as curvature and tolerance, respectively, if
%
$$
\bftheta^T \bfM \bftheta \geq \psi \| \bftheta \|^2 - \phi \| \bftheta \|_1^2
$$
%
for all $\bftheta \in \BR^b$. In short, this is denoted by $\bfM \sim RE(\psi, \phi)$.

Starting from \cite{BickelRitovTsybakov09}, different versions of the RE conditions have been proposed and used in high-dimensional analysis \citep{LohWainwright12,BasuMichailidis15,MaMichailidis15,vandeGeerBuhlmann09} to ensure that a covariance matrix satisfies a somewhat relaxed positive-definitess condition.

\begin{proof}[Proof of Theorem~\ref{thm:thm-Theta}]
The proof strategy is as follows. We first show that given fixed $(\cX,\cE)$, and some conditions on $\widehat \bfE^k := \bfY^k - \bfX^k \widehat \bfB^k, k \in \cI_K$, the bounds in in Theorem~\ref{thm:thm-Theta} hold. We then show that for random $(\cX,\cE)$, those conditions hold with probability approaching 1.

\begin{Lemma}\label{thm:ThetaThm}
Assume fixed $\cX, \cE$ and deterministic $\widehat \cB = \{ \widehat \bfB^k \}$, and the following conditions.

\noindent{\bf(A1)} For $k \in \cI_K$,
%
$$
\| \widehat \bfB^k - \bfB^k_0 \|_1 \leq C_\beta \sqrt{\frac{\log (pq)}{n}}
$$
%
with $C_\beta = O(1)$ is non-negative and depends on $\cB_0$ only.

%\noindent{\bf(T2)} $\| \bfX^k (\widehat \bfB^k - \bfB^k_0 ) \|_\infty \leq c(v_\beta)$, where $c(v_\beta)$ is $O(1)$ and depends on $v_\beta$.

\noindent{\bf(A2)} For all $j \in \cI_q$,
%
$$
\frac{1}{n} \left\| (\widehat \bfE_{-j}^k)^T \widehat \bfE^k \bfT_{0,j}^k \right\|_\infty \leq
\BQ \left(C_\beta, \Sigma_{x0}^k, \Sigma_{y0}^k \right) \sqrt {\frac{ \log (p q)}{n}},
$$
%
where $\BQ \left(C_\beta, \Sigma_x^k, \Sigma_y^k \right) = O(1)$ is non-negative and depends on $\cB_0, \Sigma_{x0}^k$ and $\Sigma_{y0}^k$ only.

\noindent{\bf(A3)} Denote $\widehat \bfS^k = (\widehat \bfE^k)^T \widehat \bfE^k/n$. Then $\widehat \bfS^k \sim RE(\psi^k, \phi^k)$ with $Kq \phi \leq \psi/2$ where $ \psi = \min_k \psi^k, \phi = \max_k \phi^k $.

%\noindent{\bf(T4)} Assumption (A2) holds for $\Sigma_y^k$.

Then the following hold

\noindent (I) Given the choice of tuning parameter
%
$$
\gamma_n \geq 4 \sqrt{| g_{\max}|} \BQ_0 \sqrt {\frac{ \log (p q)}{n}}; \quad
\BQ_0 := \max_{k \in \cI_K} \BQ \left(C_\beta, \Sigma_{x0}^k, \Sigma_{y0}^k  \right)
$$
%
\begin{align}
\| \widehat \Theta_j - \Theta_{0,j} \|_F & \leq 12 \sqrt{s_j} \gamma_n / \psi, \label{eqn:theta-norm-bound-1a}\\
\sum_{j \neq j', g \in \cG_y^{jj'}} \| \hat \bftheta_{jj'}^{[g]} - \bftheta_{0,jj'}^{[g]} \| & \leq 48 s_j \gamma_n / \psi. \label{eqn:theta-norm-bound-2a}\\
| \supp (\widehat \Theta_j) | & \leq 128 s_j / \psi
\end{align}
%

\noindent (II) For the choice of tuning parameter $\gamma_n = 4 \sqrt{| g_{\max}|} \BQ_0 \sqrt{\log (p q)/n}$,
%
\begin{align}\label{eqn:OmegaBounds0a}
\frac{1}{K} \sum_{k=1}^K \| \widehat \Omega_y^k - \Omega_{y0}^k \|_F \leq
O \left( \BQ_0 \sqrt{\frac{| g_{\max}| S}{K}}
\sqrt {\frac{ \log (p q)}{n}} \right)
\end{align}
%
%Further if A1 holds with $s = s_0$, and A3 is satisfied then
%
%(II) Direction consistency.
\end{Lemma}

Condition (A1) holds by assumption. When $\cX$ and $\cE$ are random, the following proposition ensures that (A2) and (A3) hold with probabilities approaching to 1.

\begin{Lemma}\label{prop:ErrorRE}
Consider deterministic $\widehat \cB$ satisfying assumption (A1), and conditions (E1), (E2) from the main paper. Then for sample size $n \succsim \log (pq)$ and $k \in \cI_K$,

\begin{enumerate}
\item $\widehat \bfS^k$ satisfies the RE condition: $ \widehat \bfS^k \sim RE (\psi^k, \phi^k)$, where 
%
$$
\psi^k = \frac{ \Lambda_{\min} (\Sigma_{x0}^k)}{2}; \quad \phi^k = \frac{ \psi^k \log p}{n} + 2 C_\beta c_2 [ \Lambda_{\max} (\Sigma_{x0}^k) \Lambda_{\max} (\Sigma_{y0}^k) ]^{1/2} \frac{ \log(pq)}{n}
$$
%
with probability $\geq 1 - 6c_1 \exp [-(c_2^2-1) \log(pq)] - 2 \exp (- c_3 n), c_1, c_3 > 0, c_2 > 1$.
%
\item The following deviation bound is satisfied for any $j \in \cI_q$
%
$$
\left\|\frac{1}{n} (\widehat \bfE_{-j}^k)^T \widehat \bfE^k \bfT_{0,j}^k \right\|_\infty \leq \BQ \left(C_\beta, \Sigma_{x0}^k, \Sigma_{y0}^k \right) \sqrt {\frac{ \log (p q)}{n}}
$$
%

with probability $\geq 1 - 1/p^{\tau_1-2} - 12 c_1 \exp [-(c_2^2-1) \log(pq)] - 6c_4 \exp [-(c_5^2-1) \log(pq)], c_4 > 0, c_5 > 1, \tau_1 > 2$, where
%
\begin{align*}
\BQ \left(C_\beta, \Sigma_{x0}^k, \Sigma_{y0}^k \right) &=
\left[ 2 C_\beta^2 V_x^k + 4 C_\beta c_2 [ \Lambda_{\max} (\Sigma_{x0}^k) \Lambda_{\max} (\Sigma_{y0}^k) ]^{1/2} \right] \sqrt{\frac{ \log(pq)}{n}} +\\
& c_5 \left[ \Lambda_{\max} ( \Sigma_{y0}^k) \sigma_{y0,j,-j}^k \right]^{1/2} \sqrt{\frac{\log q}{\log (pq)}}
\end{align*}
%
with $\sigma_{y0,j,-j}^k = \BV( E_j - \BE_{-j} \bftheta_{0,j})$, and 
%
$$
V_x^k = \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x^k n}} + \max_i \sigma_{x,ii}^k; \quad
c_x^k = \left[ 128 ( 1 + 4 \Lambda_{\max} (\Sigma_{x0}^k)  )^2 \max_i (\sigma_{x0,ii}^k)^2 \right]^{-1}
$$
\end{enumerate} 
\end{Lemma}

\noindent We prove the main theorem by putting together Lemma~\ref{thm:ThetaThm} and Lemma~\ref{prop:ErrorRE}.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:thm-B}]
The strategy is same as Theorem~\ref{thm:thm-Theta}. We first show the theorem statements hold for fixed $\cX, \cE$ in presence of some conditions, then show that those conditions are satisfied with probability approaching 1 when $\cX$ and $\cE$ are random.

%The proof strategy is as follows. We first show that given fixed $(\cX,\cE)$, and some conditions on the quantities $\widehat \bfgamma, \widehat \bfGamma$, the bounds in in \ref{thm:thm-B} hold. We then show that for random $(\cX,\cE)$, these conditions hold with probability approaching 1.

\begin{Lemma}\label{thm:BetaThm}
Assume fixed $(\cX, \cE)$, and deterministic $\widehat \Theta = \{ \widehat \Theta_j \}$, so that

\noindent{\bf(B1)} For $j \in \cI_q$,
%
$$
\| \widehat \Theta_j - \Theta_{0,j} \|_F \leq C_\Theta \sqrt{\frac{\log q}{n}},
$$
%
for some $C_\Theta = O(1)$ dependent on $\Theta_0$ only.

\noindent{\bf(B2)} Denote $\widehat \bfGamma^k = (\widehat \bfT^k)^2 \otimes (\bfX^k)^T \bfX^k/n, \widehat \bfgamma^k = (\widehat \bfT^k)^2 \otimes (\bfX^k)^T \bfY^k/n$. Then the deviation bound holds:
%
$$
\left\| \widehat \bfgamma^k - \widehat \bfGamma^k \bfbeta_0 \right\|_\infty \leq \BR( C_\Theta, \Sigma_{x0}^k, \Sigma_{y0}^k) \sqrt{ \frac{ \log(pq)}{n}}.
$$
%
where $\BR (C_\Theta, \Sigma_{x0}^k, \Sigma_{y0}^k ) = O(1)$ depends on $\Theta_0, \Sigma_{x0}^k$ and $\Sigma_{y0}^k$ only.

\noindent{\bf(B3)} $\widehat \bfGamma \sim RE(\psi_*, \phi_*)$ with $Kpq \phi_* \leq \psi_*/2$.

Then, given the choice of tuning parameter
%
$$
\lambda_n \geq 4 \sqrt{| h_{\max} |} \BR_0 \sqrt{ \frac{ \log(pq)}{n}}; \quad 
\BR_0 := \max_{k \in \cI_K} \BR \left(C_\Theta, \Sigma_{x0}^k, \Sigma_{y0}^k \right)
$$
%
the following holds
%
\begin{align}
\| \widehat \bfbeta - \bfbeta_0 \|_1 & \leq 48 \sqrt{ | h_{\max} |} s_\beta \lambda_n / \psi^* \label{eqn:BetaThmEqn1}\\
\| \widehat \bfbeta - \bfbeta_0 \| & \leq 12 \sqrt s_\beta \lambda_n / \psi^* \label{eqn:BetaThmEqn2}\\
\sum_{h \in \cH} \| \bfbeta^{[h]} - \bfbeta_0^{[h]} \| & \leq 48 s_\beta \lambda_n / \psi^* \label{eqn:BetaThmEqn3}\\
(\widehat \bfbeta - \bfbeta_0 )^T \widehat \bfGamma (\widehat \bfbeta - \bfbeta_0 ) & \leq
72 s_\beta \lambda_n^2 / \psi^* \label{eqn:BetaThmEqn4}
\end{align}
\end{Lemma}

Condition (B1) holds by assumption. Next we verify that conditions (B2) and (B3) hold with high probability given fixed $\widehat \Theta$.

\begin{Lemma}\label{prop:ThmBetaRE}
Consider deterministic $\widehat \Theta$ satisfying assumption (B1). Also assume conditions (E3), (E4) from the main paper. Then for sample size $n \succsim \log (pq)$,

\begin{enumerate}
\item $\widehat \bfGamma$ satisfies the RE condition: $ \widehat \bfGamma \sim RE (\psi_*, \phi_*)$, where 
%
$$
\psi_* = \min_k \psi^k \left( \min_i \psi_k^j - d_k C_\Theta \sqrt{ \frac{ \log (pq)}{n}}\right), 
\phi_* = \max_k \phi^k \left( \min_i \phi_k^j + d_k C_\Theta \sqrt{ \frac{ \log (pq)}{n}}\right)
$$
%
with probability $\geq 1 - 2 \exp(c_3 n), c_3>0$.
%
\item The deviation bound in (A2) is satisfied with probability $ \geq 1 - 12 c_1 \exp[ (c_2^2-1) \log (pq)]$, where
%
$$
\BR \left(C_\Theta, \Sigma_{x0}^k, \Sigma_{y0}^k \right) =
c_2 \left\{ d_k C_\Theta \sqrt{ \frac{ \log (pq)}{n}}
[ \Lambda_{\max} (\Sigma_x^k) \Lambda_{\max} (\Sigma_y^k)]^{1/2} +
\left[ \frac{\Lambda_{\max} (\Sigma_x^k)}{\Lambda_{\min} (\Sigma_y^k) } \right]^{1/2} \right\}
$$
%
\end{enumerate}
\end{Lemma}

The theorem is immediate by putting together Lemma~\ref{thm:BetaThm} and Lemma~\ref{prop:ThmBetaRE}.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:starting-values}]
The first part is immediate from the proof of part I of Theorem 4 in \cite{LinEtal16}. By choice of $\lambda_n$, we now have
%
$$
\| \widehat \bfB^{k (0)} - \bfB^k_0 \|_1 = O\left( \sqrt{ \frac{ \log (pq)}{n}} \right),
$$
%
so we can apply Theorem~\ref{thm:thm-Theta} to prove the bounds on $\{\widehat \Theta_j^{(0)} \}$.
\end{proof}

%{\colrbf Discuss tighter bound compared to vanilla JSEM}
%
%After providing the error bounds for solutions to the subproblem \eqref{eqn:EstEqn2}, we concentrate on the subproblem \eqref{eqn:EstEqn1}. Following a similar strategy, we first get error bounds for $\widehat \bfbeta$ assuming everything else fixed.

%with
%$$
%\bfbeta = \begin{bmatrix}
%\ve (\bfB^1)\\
%\vdots\\
%\ve (\bfB^K)\\
%\end{bmatrix}; \quad
%\bfGamma = \begin{bmatrix}
%I_q \otimes (\bfX^1) TX^1 / n) & &\\
%& \ddots &\\
%& & I_q \otimes (\bfX^K)^T X^K / n)
%\end{bmatrix} 
%$$


%%
%\begin{align}
%\| \widehat \bfS^k \bfT_{0,j}^k \|_\infty & \leq
%\| \widehat \bfS^k \|_\infty \| \bfT_{0,j}^k  \|_1 \notag\\
%& \leq \left[ \| \widehat \bfS^k - \bfS^k \|_\infty + \| \bfS^k \|_\infty \right] \| \bfT_{0,j}^k  \|_1 \label{eqn:ErrorRElemma2eqn1}
%\end{align}
%%
%Since $\Sigma_y^k$ is diagonally dominant, (see proof of Proposition~\ref{lemma:LemmaE2}) we have
%%
%\begin{align}\label{eqn:ErrorRElemma2eqn2}
% \| \bfT_{0,j}^k \|_1 \leq 2
%\end{align}
%%
%Moving on to $\| \widehat \bfS^k - \bfS^k \|_\infty$:
%%
%\begin{align}
%\| \widehat \bfS^k - \bfS^k \|_\infty & \leq \left\| \frac{2}{n} (\bfE^k)^T \bfX^k (\bfB^k_0 - \widehat \bfB^k) \right\|_\infty + 
%\left\| (\widehat \bfB^k - \bfB_0^k)^T \left( \frac{(\bfX^k)^T (\bfX^k)}{n} \right) (\widehat \bfB^k) - \bfB_0^k)) \right\|_\infty \notag\\
%& \leq 2 \left\| \frac{(\bfX^k)^T \bfE^k}{n} \right\|_\infty \left\| \bfB^k_0 - \widehat \bfB^k) \right\|_1 + 
%\left\| (\widehat \bfB^k - \bfB_0^k)^T \left( \frac{(\bfX^k)^T (\bfX^k)}{n} \right) (\widehat \bfB^k) - \bfB_0^k)) \right\|_\infty \label{eqn:ErrorRElemma2eqn3}
%\end{align}
%%
%By applying Lemma~\ref{lemma:ErrorRElemma2} on $\bfX^k, \bfE^k$, and assumption (T1) we have for $n \succsim \log (pq)$ and with probability $\geq 1 - 6c_1 \exp [-(c_2^2-1) \log(pq)]$,
%%
%$$
%2 \left\| \frac{(\bfX^k)^T \bfE^k}{n} \right\|_\infty \left\| \bfB^k_0 - \widehat \bfB^k) \right\|_1 \leq
%2 v_\beta c_2 [ \Lambda_{\max} (\Sigma_x^k) \Lambda_{\max} (\Sigma_y^k) ]^{1/2} \sqrt{\frac{ \log(pq)}{n}}
%$$
%%
%The second term is bounded by substituting $\bfX, \hat \bfB, \bfB_0$ with $\bfX^k, \hat \bfB^k, \hat \bfB_0^k$ respectively in (\ref{eqn:ErrorREeqn2}).
%
%For a bound on the $\ell_\infty$-norm of $\bfS^k = (\bfX^k)^T \bfX^k/n$, we apply Lemma 8 of \cite{RavikumarEtal11} on the (sub-gaussian) design matrix $\bfX^k$ to obtain that for sample size
%$$
%n \geq 512 ( 1 + 4 \Lambda_{\max} (\Sigma_x^k))^4 \max_j (\sigma_{x,jj}^k )^4 \log (4p^{\tau_1})
%$$
%%
%we get that with probability $ \geq 1 - 1/p^{\tau_1-2}$,
%%
%\begin{align}\label{eqn:ErrorRElemma2eqn4}
%\| \bfS^k \|_\infty & \leq \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x^k n}} + \max_j \sigma_{x,jj}^k; \quad
%c_x^k = \left[ 128 ( 1 + 4 \Lambda_{\max} (\Sigma_x^k)  )^2 \max_j (\sigma_{x,jj}^k)^2 \right]^{-1}
%\end{align}
%%
%We now bound the right hand side of (\ref{eqn:ErrorRElemma2eqn1}) using (\ref{eqn:ErrorRElemma2eqn2}), (\ref{eqn:ErrorRElemma2eqn3}) and (\ref{eqn:ErrorRElemma2eqn4}), which proves part 3.

%, with an extra step to prove that $(\bfT^k)^2$ is diagonally dominant. For this, denote the elements of $(\bfT^k)^2$ by $\tau_{jj'}, j; j' \in \cI_q$. Then
%%
%\begin{align*}
%\tau_{jj} - \sum_{j \neq j'} | \tau_{jj'}| & = \sum_{l=1}^q \left[
%\frac{\omega_{jl}^2}{\omega_{jj}^2 } - \sum_{j \neq j'} \frac{| \omega_{jl} \omega_{j'l} |}{\omega_{jj} \omega_{j'j'} } \right]
%\end{align*}
%%
%For each summand,
%%
%\begin{align*}
%\frac{\omega_{jl}^2}{\omega_{jj}^2 } - \sum_{j \neq j'} \frac{| \omega_{jl} \omega_{j'l} |}{\omega_{jj} \omega_{j'j'}} & = \frac{| \omega_{jl} |}{\omega_{jj} } \left[ \frac{| \omega_{jl}| }{\omega_{jj} } - \sum_{j' \neq j} \frac{| \omega_{j'l} |}{\omega_{j'j'}} \right] \\
%\end{align*}
%

\begin{proof}[Proof of Theorem~\ref{Thm:ThmTesting}]
Define the following:
%
$$
\widehat \bfD_i = \ve(\widehat \bfb^1_i, \ldots, \widehat \bfb^K_i); \quad
\bfR_i^k = \bfX_i^k - \bfX_{-i}^k \widehat \bfzeta_i^k; k \in \cI_K
$$
%
Then from \eqref{eqn:DebiasedBeta} we have
%
\begin{align}\label{eqn:ThmTestingProofeq1}
\bfM_i ( \widehat \bfC_i - \widehat \bfD_i )^T &= \frac{1}{\sqrt n}
\begin{bmatrix}
\frac{1}{\widehat s^1_i} (\bfR^1_i)^T \widehat \bfE^1 \\
\vdots\\
\frac{1}{\widehat s^K_i} (\bfR^K_i)^T \widehat \bfE^K
\end{bmatrix}
\end{align}
%
We now decompose $\widehat \bfE^k:$
%
\begin{align*}
\widehat \bfE^k &= \bfY^k - \bfX^k \widehat \bfB^k \\
&= \bfE^k + \bfX^k (\bfB_0^k - \widehat \bfB^k)\\
&= \bfE^k + \bfX_i^k (\bfb_{0i}^k - \widehat \bfb_i^k) + \bfX_{-i}^k (\bfB_{0,-i}^k - \widehat \bfB_{-i}^k)
\end{align*}
%
Putting them back in \eqref{eqn:ThmTestingProofeq1} and using $t_i^k = (\bfR_i^k)^T \bfX_i^k/n$,
%
\begin{align}
\bfM_i ( \widehat \bfC_i - \widehat \bfD_i)^T &= \frac{1}{\sqrt n}
\begin{bmatrix}
\frac{1}{\widehat s^1_i} (\bfR^1_i)^T \bfE^1 \\
\vdots\\
\frac{1}{\widehat s^K_i} (\bfR^K_i)^T \bfE^K
\end{bmatrix} +
\bfM_i (\bfD_i - \widehat \bfD_i )^T \notag\\
& + \frac{1}{\sqrt n}
\begin{bmatrix}
\frac{1}{\widehat s^1_i} (\bfR^1_i)^T \bfX_{-i}^1 (\bfB_{0,-i}^1 - \widehat \bfB_{-i}^1) \\
\vdots\\
\frac{1}{\widehat s^K_i} (\bfR^K_i)^T \bfX_{-i}^K (\bfB_{0,-i}^K - \widehat \bfB_{-i}^K)
\end{bmatrix} \notag\\
\Rightarrow
\widehat \Omega_y^{1/2} \bfM_i ( \widehat \bfC_i - \bfD_i)^T &=
\frac{\widehat \Omega_y^{1/2}}{\sqrt n}
\begin{bmatrix}
\frac{1}{\widehat s^1} (\bfR^1_i)^T \bfE^1 \\
\vdots\\
\frac{1}{\widehat s^K} (\bfR^K_i)^T \bfE^K
\end{bmatrix} +
\frac{\widehat \Omega_y^{1/2}}{\sqrt n}
\begin{bmatrix}
\frac{1}{\widehat s^1_i} (\bfR^1_i)^T \bfX_{-i}^1 (\bfB_{0,-i}^1 - \widehat \bfB_{-i}^1) \\
\vdots\\
\frac{1}{\widehat s^K_i} (\bfR^K_i)^T \bfX_{-i}^K (\bfB_{0,-i}^K - \widehat \bfB_{-i}^K)
\end{bmatrix}\label{eqn:ThmTestingProofeq2}
\end{align}

At this point, we drop $k$ and 0 in the subscripts, and prove the following:

\begin{Lemma}\label{Lemma:ThmTestingLemma}
Given conditions (T1) and (T2), the following holds for sample size $n$ such that $n \succsim \log (pq)$ and $\sigma_{x,i,-i} - n^{-1/4} - v_\zeta \sqrt{V_x} > 0$:
%
$$
\frac{1}{\sqrt n \widehat s_i}  \widehat \Omega_y^{1/2} \bfE^T \bfR_i \sim
\cN_q ({\bf 0}, \bfI) + \bfS_{1n};
$$
%
\begin{align}\label{eqn:ThmTestingProofeq3}
\| \bfS_{1n} \|_\infty & \leq 
\frac{D_\Omega^{1/2} (2 + D_\zeta) c_2 [ \Lambda_{\max} (\Sigma_x) \Lambda_{\max} (\Sigma_e) ]^{1/2} \sqrt{ \log(pq)}}{\sigma_{x,i,-i} - n^{-1/4} - D_\zeta \sqrt{V_x}} =
O \left( \frac{ \log(pq)}{\sqrt n} \right)
\end{align}
%
with probability $\geq 1 - 6c_1 \exp[-(c_2^2-1) \log (p q)] - 1/p^{\tau_1-2} - \kappa_i/\sqrt n$, where $\kappa_i := \BV [(X_i - \BX_{-i} \bfzeta_{0,-i})^2]$.

Additionally, given condition (T3)
%
\begin{align}\label{eqn:ThmTestingProofeq4}
& \left\| \frac{1}{\sqrt n \widehat s_i} \bfR_i^T \bfX_{-i} (\bfB_{-i} - \widehat \bfB_{-i})
\widehat \Omega_y^{1/2} \right\|_\infty \notag\\
& \leq
\frac{D_\beta ( \Lambda_{\min} (\Sigma_y)^{1/2} + D_\Omega^{1/2})}{\sigma_{x,i,-i} - n^{-1/2} - D_\zeta \sqrt{V_x}} 
\left[ c_{7} \sqrt{ (\sigma_{x,i,-i} \Lambda_{\max} (\Sigma_{x, -i}) ) \log p} + \sqrt n D_\zeta V_x \right] =
O \left( \frac{ \log(pq)}{\sqrt n} \right)
\end{align}
%
holds with probability $\geq 1 - 6c_6 \exp[-(c_7^2-1) \log (p q)] - 1/p^{\tau_1-2} - \kappa_i/\sqrt n$ for some $c_6 > 0, c_7 > 1$.
\end{Lemma}

Given Lemma~\ref{Lemma:ThmTestingLemma}, the first and second summands on the right hand side of \eqref{eqn:ThmTestingProofeq2} are bounded above by applying each of \eqref{eqn:ThmTestingProofeq3} and \eqref{eqn:ThmTestingProofeq4} $K$ times. This completes our proof.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:PowerThm}]
From the test statistic formulation in Algorithm~\ref{algo:AlgoGlobalTest} and Lemma~\ref{Lemma:ThmTestingLemma} it is immediate that when the alternative is true,
%
\begin{align*}
D_i \sim \chi^2_q + \bfdelta^T
\left( \frac{ \widehat \Sigma_y^1}{(m_i^1)^2} + \frac{\widehat \Sigma_y^2}{(m_i^2)^2} \right)^{-1} \bfdelta +
o_P(1)
\end{align*}
%
Dropping $k$ in the superscripts and 0 in subscripts for the true values, we now provide bounds for the quantities $\widehat \Sigma_y$ and $(m_i)^2$. First we denote $\Delta_{1n} = \widehat \Sigma_y - \Sigma_y$. Also, from the definition of $m_i$,
%
$$
\frac{1}{m_i} =
\frac{\| \bfX_i - \bfX_{-i} \widehat \bfzeta_i \|}{(\bfX_i - \bfX_{-i} \widehat \bfzeta_i)^T \bfX_i} =
\frac{\| \bfX_i - \bfX_{-i} \bfzeta_i + \bfX_{-i} (\widehat \bfzeta_i - \bfzeta_i)\|}
{(\bfX_i - \bfX_{-i} \bfzeta_i)^T \bfX_i + \bfX_{-i} (\widehat \bfzeta_i -\bfzeta_i)^T \bfX_i}
$$

\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:FDRthm}]
The proof follows the general structure of Theorem 4.1 in \cite{LiuShao14}, with two modifications. Firstly we replace the bound in equation (12) of \cite{LiuShao14} by a new deviation bound
%
$$
P \left( \left| d_{ij} - \frac{\mu_j}{\sigma_j} \right| \geq t \right) = (1 - \Phi(t))(1 + o(1))
$$
%
for any $t$, since $(d_{ij} - \mu_j)/\sigma_j \sim N(0,1) + o_P(1)$ from Corollary~\ref{corollary:CorTesting}. We replace $G_\kappa(t)$ in all following calculations in \cite{LiuShao14} with $1 - \Phi(t)$. Secondly, we need to ensure that given both $\Sigma_{y0}^1$ and $\Sigma_{y0}^2$ satisfy the condition (D1) or (D1*), the pooled covariance matrix $\Sigma_{y0}^1/ \sigma_{x0,i,-i}^1 + \Sigma_{y0}^2/  \sigma_{x0,i,-i}^2$ also does so.

For this, denote $c_k =  \sigma_{x0,i,-i}^k, k = 1,2$. Notice that for any $C_1, C_2 > 0$,
%
\begin{align*}
r_{0,jj'}^k \geq C_k & \Rightarrow \sigma_{y0,jj'}^k \geq (\sigma_{y0,jj}^k \sigma_{y0,j'j'}^k)^{1/2} C_k\\
& \Rightarrow \frac{\sigma_{y0,jj'}^1}{c_1} + \frac{\sigma_{y0,jj'}^2}{c_2} \geq
\frac{ (\sigma_{y0,jj}^1 \sigma_{y0,j'j'}^1)^{1/2} C_1}{c_1} +
\frac{ (\sigma_{y0,jj}^2 \sigma_{y0,j'j'}^2)^{1/2} C_2}{c_2}\\
& \Rightarrow \frac{ \sigma_{y0,jj'}^1/c_1 + \sigma_{y0,jj'}^2/c_2}
{ (\sigma_{y0,jj}^1 \sigma_{y0,j'j'}^1)^{1/2}/ c_1 + (\sigma_{y0,jj}^2 \sigma_{y0,j'j'}^2)^{1/2}/ c_2}
\geq \min \{ C_1, C_2 \}
\end{align*}
%
It is now immediate that (D1) or (D1*) holds for the pooled covariance matrices.
\end{proof}


