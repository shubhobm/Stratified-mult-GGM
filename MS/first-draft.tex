\documentclass[fleqn,11pt]{article}
%\documentclass[aoas,preprint]{imsart}

\usepackage{mycommands1,amssymb,amsmath,amsthm,color,pagesize,outlines,cite,subfigure}
\usepackage[small]{caption}
\usepackage[pdftex]{epsfig}
\usepackage{hyperref} % for linking references
\hypersetup{colorlinks = true, citecolor = blue, urlcolor = blue}

\usepackage{stackrel}

\usepackage[round]{natbib}

% for algorithm
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}

%\addtolength{\evensidemargin}{-.5in}
%\addtolength{\oddsidemargin}{-.5in}
%\addtolength{\textwidth}{0.9in}
%\addtolength{\textheight}{0.9in}
%\addtolength{\topmargin}{-.4in}

%% measurements for 1 inch margin
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}
\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\usepackage{setspace}
%\doublespacing

%\pagestyle{myheadings}
%\markboth{}{\underline{{\bf Notes: (do not circulate)} \hspace{4.5cm} {\sc  Ansu Chatterjee} \hspace{0.25cm}}}

\DeclareMathOperator*{\ve}{vec}
\DeclareMathOperator*{\diag}{diag }
\DeclareMathOperator*{\Tr}{Tr}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\makeatletter
\newcommand{\opnorm}{\@ifstar\@opnorms\@opnorm}
\newcommand{\@opnorms}[1]{%
  \left|\mkern-1.5mu\left|\mkern-1.5mu\left|
   #1
  \right|\mkern-1.5mu\right|\mkern-1.5mu\right|
}
\newcommand{\@opnorm}[2][]{%
  \mathopen{#1|\mkern-1.5mu#1|\mkern-1.5mu#1|}
  #2
  \mathclose{#1|\mkern-1.5mu#1|\mkern-1.5mu#1|}
}
\makeatother

%% Appendix theorem counter
\usepackage{chngcntr}
\usepackage{apptools}
\AtAppendix{\counterwithin{Theorem}{section}}
\numberwithin{equation}{section}

\begin{document}

\newtheorem{Theorem}{Theorem}[section]
\newtheorem{Lemma}[Theorem]{Lemma}
\newtheorem{Corollary}[Theorem]{Corollary}
\newtheorem{Proposition}[Theorem]{Proposition}
\newtheorem{Conjecture}[Theorem]{Conjecture}
\theoremstyle{definition} \newtheorem{Definition}[Theorem]{Definition}

\title{Joint Estimation and Inference for Multiple Multi-layered Gaussian Graphical Models}
\date{}
\author{Subhabrata Majumdar}
\maketitle

\noindent\textbf{Abstract}: 
The rapid development of high-throughput technologies has enabled generation of data from biological processes that span multiple layers, like genomic, proteomic or metabolomic data; and pertain to multiple sources, like disease subtypes or experimental conditions. In this work we propose a general statistical framework based on graphical models for horizontal (i.e. across conditions or subtypes) and vertical (i.e. across different layers containing data on molecular compartments) integration of information in such datasets. We start with decomposing the multi-layer problem into a series of two-layer problems. For each two-layer problem, we model the outcomes at a node in the lower layer as dependent on those of other nodes in that layer, as well as all nodes in the upper layer. Following the biconvexity of our objective function, this estimation problem decomposes into two parts, where we use neighborhood selection and subsequent refitting of the precision matrix to quantify the dependency of two nodes in a single layer, and use group-penalized least square estimation to quantify the directional dependency of two nodes in different layers. Finally, to test for differences in these directional dependencies across multiple sources, we devise a hypothesis testing procedure that utilizes already computed neighborhood selection coefficients for nodes in the upper layer. We establish theoretical results for the validity of this testing procedure and the consistency of our estimates, and also evaluate their performance through simulations and a real data application.

\vspace{.5cm}
\noindent\textbf{Keywords}: Data integration; Gaussian Graphical Models; Neighborhood selection; Group lasso

\newpage

\section{Notations}
We shall denote scalars by small letters, vectors by bold small letters and matrices by bold capital letters. For any matrix $\bfA$, $(\bfA)_{ij}$ denote its element in the $(i,j)^\text{th}$ position. For $a,b \in \BN$, we denote the set of all $a \times b$ real matrices by $\BM(a,b)$. For any positive integer $c$, define $\cI_c = \{ 1, \ldots, c\}$.

\section{Model}
Consider the two -layered setup:

\begin{eqnarray}
\BX^k = (X^k_1, \ldots, X^k_p)^T \sim \cN (0, \Sigma^k_x)\\
\BY^k = \BX^k \bfB^k + \bfE^k; \quad \bfE^k = (E^k_1, \ldots, E^k_p)^T \sim \cN (0, \Sigma^k_y)\\
\bfB^k \in \BM(p,q); \quad \Omega^k_x = (\Sigma^k_x)^{-1}; \quad \Omega^k_y = (\Sigma^k_y)^{-1}
\end{eqnarray}

Want to estimate $\{ (\Omega^k_x, \Omega^k_y, \bfB^k); k \in \cI_K$ from data $\cZ^k = \{ (\bfY^k, \bfX^k); \bfY^k \in \BM(n,q), \bfX^k \in \BM(n,p), k \in \cI_K\}$. in presence of known grouping structures $\cG_x, \cG_y, \cH$ respectively. 

%\paragraph{Notation:} Denote 3-dimensional array objects as elements of $\BT(a,b,c)$, the set of all $a \times b \times c$ tensors.
%Define $\cS^x = (\Omega^k_x), \cS^y = (\Omega^k_y), \cB = (\bfB^k)$

Estimation of $\{ \Omega_x^k \}$ done using JSEM. For the other part, we use the following two-step procedure:

\begin{enumerate}
\item Run neighborhood selection on $y$-network incorporating effects of $x$-data and an additional blockwise group penalty:
%
\begin{align}
& \min_{\cB, \Theta} \left\{ \sum_{j=1}^q  \frac{1}{n_k} \left[ \sum_{k=1}^K \| \bfY^k_j - (\bfY_{-j}^k - \bfX^k \bfB_{-j}^k) \bftheta_j^k - \bfX^k \bfB_j^k \|^2 + \sum_{j \neq i} \sum_{g \in \cG_y^{ij}} \lambda_{ij}^g \| \bftheta_{ij}^{[g]} \| \right] + \sum_{b \in \cG_x \times \cG_y \times \cH} \eta^b \| \bfB^{[b]} \| \right\}\\
&= \min \left\{ f ( \cY, \cX, \cB, \Theta) + P (\Theta) + Q (\cB) \right\} 
\end{align}
%
where $\Theta = \{ \Theta_i \}, \cB = \{ \bfB^k \}, \cY = \{ \bfY^k \}, \cX = \{ \bfX^k \}, \cE = \{ \bfE^k \}$.

This estimates $\cB$ { \colrbf (possibly refit and/or within-group threshold) }.

\item Step I part 2 and step II of JSEM (see 15-656 pg 6) follows to estimate $\{ \Omega_y^k \}$.
\end{enumerate}

The objective function is bi-convex, so we are going to do the following in step 1-

\begin{itemize}
\item Start with initial estimates of $\cB$ and $\Theta$, say $\cB^{(0)}, \Theta^{(0)}$.
\item Iterate:
%
\begin{align}
\Theta^{(t+1)} &= \argmin \left\{ f ( \cY, \cX, \cB^{(t)}, \Theta^{(t)}) + P (\Theta^{(t)}) \right\}\\
\cB^{(t+1)} &= \argmin \left\{ f ( \cY, \cX, \cB^{(t)}, \Theta^{(t+1)}) + Q (\cB^{(t)}) \right\}
\end{align}
\item Continue till convergence.
\end{itemize}
%

\section{Two-sample testing}
Suppose there are two disease subtypes: $k = 1,2$, and we are interested in testing whether the downstream effect of a predictor is X-data is same across both subtypes, i.e. if $\bfb_i^1 = \bfb_i^2$ for some $i \in \{ 1, \ldots, p \}$. For this we consider the modified optimization problem:
%
\begin{align}
& \min_{\cB, \Theta} \frac{1}{n} \left\{ \sum_{j=1}^q \sum_{k=1}^2 \| \bfY_j^k - \bfY_{-j}^k \bftheta_j^k - \bfX^k \bfB_{j}^k \|^2 + \sum_{j \neq j'} \lambda_{jj'} \| \bftheta_{jj'}^* \| + \sum_{i=1}^p \eta_i \| \bfB_{i*}^* \| \right\}\\
&= \min \left\{ f ( \cY, \cX, \cB, \Theta) + P (\Theta) + Q (\cB) \right\} 
\end{align}
%
with $n_1 = n_2 = n$ for simplicity; and $\bfB^k = (\bfb_1^k, \ldots, \bfb_q^k), (\bfB_{i*}^*) \in \BR^{ q \times K}$

\section{Conditions}
Conditions A1 from JSEM paper holds for $\cX$ and $\cE$. Also A2, A3 from JSEM paper.

\section{Results}
To prove the results in this section, we shall use a reparametrization of the neighborhood coefficients at the lower level. Specifically, notice that for $j \in \cI_q, k \in \cI_K$, the corresponding summand in $f(\cY, \cX, \cB, \Theta)$ can be rearranged as
%
\begin{align*}
\| \bfY^k_j - \bfX^k \bfB_j^k - (\bfY_{-j}^k - \bfX^k \bfB_{-j}^k) \bftheta_j^k \|^2 &=
\| \bfY^k_j - \bfY_{-j}^k \bftheta_j^k - (\bfX^k \bfB_j^k -\bfX^k \bfB_{-j}^k \bftheta_j^k) \|^2 \\
&= \| ( \bfY - \bfX \bfB ) \bfT_j^k \|^2
\end{align*}
%
where
%
$$
T_{jj'}^k = \begin{cases}
1 \text{ if } j = j'\\
- \theta_{jj'}^k \text{ if } j \neq j'
\end{cases}
$$
%
Thus, with $\bfT^k := (\bfT_j^k)_{j \in \cI_q}$, we have
$$
f( \cY, \cX, \cB, \Theta) = \frac{1}{n} \sum_{j=1}^p \sum_{k=1}^K \| ( \bfY^k - \bfX^k \bfB^k ) \bfT_j^k \|^2
= \frac{1}{n} \sum_{k=1}^K \| \bfY^k - \bfX^k \bfB^k ) \bfT^k \|_F^2
= \sum_{k=1}^K \Tr (\bfS^k (\bfT^k)^2 )
$$
%
where $\bfS^k = (1/n) (\bfY^k - \bfX^k \bfB^k) (\bfY^k - \bfX^k \bfB^k)^T$ is the sample covariance matrix.

Now suppose $\bfbeta = \ve (\bfB)$, and any subscript or superscript on $\bfB$ will be passed on to $\bfbeta$. Denote by $\widehat \bfbeta$ and $\widehat \bfTheta$ the generic estimators given by
%
\begin{align}
\widehat \bfbeta &= \argmin_{\bfbeta \in \BR^{pq}} \left\{-2 \bfbeta^T \widehat \bfgamma + \bfbeta^T \widehat \bfGamma \bfbeta + \lambda_n \sum_{g \in \cG} \| \bfbeta^{[g]}  \| \right\} \\
\widehat \Theta_j &= \argmin_{\Theta_j \in \BM(q-1, K)} \left\{ \frac{1}{n} \sum_{k=1}^K \| \bfY^k_j - \bfX^k \widehat \bfB_j^k - (\bfY_{-j}^k - \bfX^k \widehat \bfB^k_{-j} ) \bftheta_j^k \|^2 + \sum_{j \neq j'} \sum_{g \in \cG_y^{jj'}} \lambda_{jj'}^g \| \bftheta_{jj'}^{[g]} \| \right\}
\end{align}
%
where
%
$$
\widehat \bfGamma = \begin{bmatrix}
(\widehat \bfT^1)^2 \otimes \frac{(\bfX^1)^T \bfX^1}{n} & &\\
& \ddots &\\
& & (\widehat \bfT^K)^2 \otimes \frac{(\bfX^K)^T \bfX^K}{n}
\end{bmatrix}; \quad
\widehat \bfgamma = \begin{bmatrix}
(\widehat \bfT^1)^2 \otimes \frac{(\bfX^1)^T}{n}\\
\vdots\\
(\widehat \bfT^K)^2 \otimes \frac{(\bfX^K)^T}{n}
\end{bmatrix}
\begin{bmatrix}
\ve (\bfY^1)\\
\vdots\\
\ve (\bfY^K)
\end{bmatrix}
$$
with $\widehat \bfT^k$ defined the same way using $\widehat \bftheta_j^k$ as we defined $\bfT^k$ using $\bftheta_j^k$.
\begin{Theorem}\label{thm:ThetaThm}
Assume fixed $\cX, \cE$ and deterministic $\widehat \cB = \{ \widehat \bfB^k \}$. Also for $k = 1, \ldots, K$,

\noindent{\bf(T1)} $\| \widehat \bfB^k - \bfB^k_0 \|_F \leq v_\beta$, where $v_\beta = {\colrbf tbd}$;

\noindent{\bf(T2)} $\| \bfX^k (\widehat \bfB^k - \bfB^k_0 ) \|_\infty \leq c(v_\beta)$ for some non-negative function $c(.)$;

\noindent{\bf(T3)} Assumption (A1) holds for $\widehat \bfS^k := (\bfY^k - \bfX^k \widehat \bfB^k)(\bfY^k - \bfX^k \widehat \bfB^k)^T/n, k \in \cI_K$.

\noindent{\bf(T4)} Assumption (A2) holds for $\widehat \bfE^k$. Then

(I) Estimation consistency.

Further if A1 holds with $s = s_0$, and A3 is satisfied then

(II) Direction consistency.
\end{Theorem}

\begin{proof}[Proof of Theorem~\ref{thm:ThetaThm}]


\textit{Part I.} Follows proof of thm 1 in 15-656. The proof has 3 parts: consistency of neighborhood regression, selection of edge sets, and finally the refitting step.

Define $\bfT_{0,j}^k$ the same way as $\bfT_j^k$. For any $g \in \cG^{jj'}, k \in g$, and $j \neq j'$, let
%
$$
\widehat \bfepsilon_j^k = (\bfY^k - \bfX^k \widehat \bfB^k ) \bfT_{0,j}^k; \quad
\widehat \zeta_{jj'}^k = \frac{(\widehat \bfepsilon_j^k)^T \bfY_{j'}^k}{n}; \quad
\widehat \bfzeta_{jj'}^{[g]} = (\widehat \zeta_{jj'}^k)_{k \in g}
$$
%
Consider the random event $\cA = \bigcap_{j, j'\neq j, g} \cA_{jj'}^g$ with $\cA_{jj'}^g = \{ 2 \| \widehat \bfzeta_{jj'}^{[g]} \| \leq \lambda_{jj'}^g \}$.

\begin{Proposition}\label{lemma:LemmaE2}
Given that $\lambda_{jj'}^g$ are chosen as
%
$$
\lambda_{jj'}^g \geq \max_{k \in g} \frac{2}{\sqrt{n \omega_{jj}^k}} \left( \sqrt{|g| (1 + 2 c(v_\beta)) } + \frac{\pi}{\sqrt 2} \sqrt {r \log G_0} \right)
$$
%
we shall have $ \BP (\cA) \geq 1 - 2p G_0^{1-q} $ for some $r>1$.
\end{Proposition}

\begin{proof}[Proof of Proposition~\ref{lemma:LemmaE2}]
We follow the proof of Lemma E.2 in 15-656, with $\bfY_j^k, \widehat \bfepsilon_j^k, \widehat \zeta_{jj'}^k, \widehat \bfzeta_{jj'}^{[g]}$ in place of $\bfX_j^k, \bfepsilon_i^k, \zeta_{ij}^k, \bfzeta_{ij}^{[g]}$ respectively. Proceeding in a similar fashion we get
%
$$
\| \widehat \bfzeta_{jj'}^{[g]} \|^2 = \frac{1}{n} \left[ \| \bfZ^{[g]} \|^2 + \sum_{k \in g} \left\{ 2 Z^k (\bfQ_{j'}^k)^T \bfdelta_j^k + | (\bfQ_{j'}^k)^T \bfdelta_j^k |^2 \right\} \right]
$$
%
where $\bfZ^{[g]} = (Z^k)_{k \in g}; Z^k = (\bfQ_{j'}^k)^T \bfepsilon_j^k$ with $\bfepsilon_j^k :=(\bfY^k - \bfX^k \bfB_0^k ) \bfT_{0,j}^k$, $\bfQ_j^k$ is the first eigenvector of $\bfY_j^k (\bfY_j^k)^T/n$, and $\bfdelta_j^k := \bfX^k (\bfB_0^k - \widehat \bfB^k) \bfT_{0,j}^k$.

By cauchy-schwarz inequality, $| (\bfQ_{j'}^k)^T \bfdelta_j^k | \leq \| \bfdelta_j^k \| \leq \| \bfX^k (\bfB_0^k - \widehat \bfB^k) \|_\infty \| \bfT_{0,j} \|_1$. Now since $\Omega_y^k$ is diagonally dominant,
%
$$
\sum_{j \neq j'} |T_{0,jj'}^k| =  \sum_{j \neq j'} |\theta_{0,jj'}^k| = \sum_{j \neq j'} \frac{|\sigma_{y,jj'}^k|}{ \sigma_{y,jj}^k} \leq 1
$$
%
Also $T_{0,jj}^k = 1$, so that $\| \bfT_{0,j}^k \|_1 \leq 2$. Thus $\| \bfdelta_j^k \| \leq 2 \| \bfX^k (\bfB_0^k - \widehat \bfB^k) \|_\infty$. Hence by assumption (T2),
%
$$
\| \widehat \bfzeta_{ij}^{[g]} \| \leq \frac{1}{\sqrt n} ( \| \bfZ^{[g]} \| + 2 |g| c(v_\beta))
$$
%
so that
$$
\BP ( \{ \cA_{ij}^g \}^c ) = \BP \left( \| \widehat \bfzeta_{ij}^{[g]} \| > \frac{\lambda_{ij}^g}{2} \right) \leq \BP \left( \| \bfZ^{[g]} \|  > \frac{\sqrt n \lambda_{ij}^g}{2} - 2 |g| c(v_\beta)  \right)
$$
We now proceed through the proof of Lemma E.2 in 15-656 to end up with the choice of $\lambda_{ij}^g$.
\end{proof}
%

All subsequent derivations in the theorem go through with the new choice of $\lambda_{ij}^g$.

%We now prove the norm consistency of $\widehat \Theta_i - \Theta_{0,i}$.
%
%\begin{Proposition}\label{prop:PropA1}
%Copy from Proposition A.1 in 15-656
%\begin{align}
%\sum_{j \neq i, g \in \cG^{ij}} \| \widehat \bftheta_{ij}^{[g]} - \bftheta_{0,ij}^{[g]} \| & \leq \\
%\cM ( \widehat \Theta_i ) & \leq \\
%\| \widehat \Theta_i - \widehat \Theta_{0,i} \|_F & \leq
%\end{align}
%\end{Proposition}
%
%\begin{proof}[Proof of Proposition~\ref{prop:PropA1}]
%%We first proceed in a similar fashion as the proof of Lemma 3.1 in \cite{LouniciEtal11}. For any $\bftheta_i^k \in \BR^q$, we have
%%%
%%$$
%%\sum_{k=1}^K \frac{1}{n} \| \bfY_i^k - \bfY_{-i}^k \widehat \bftheta_i^k - \bfX^k \widehat \bfB_i^k \|^2 + \sum_{j \neq i, g \in \cG^{ij}} \| \widehat \bftheta_{ij}^{[g]} \| \leq 
%%\sum_{k=1}^K \frac{1}{n} \| \bfY_i^k - \bfY_{-i}^k \bftheta_i^k - \bfX^k \widehat \bfB_i^k \|^2 + \sum_{j \neq i, g \in \cG^{ij}} \| \bftheta_{ij}^{[g]} \|
%%$$
%%%
%%subtracting and adding $\bfY_{-i}^k \bftheta_{0,i}^k$ inside the squared norms on both sides, then simolifying and writing $\widehat \bfepsilon_i^k = \bfY_i^k - \bfY_{-i}^k \bftheta_{0,i}^k - \bfX_i^k \widehat \bfB_i^k$ we get
%%%
%%\begin{align*}
%%\frac{1}{n} \| \bfY_{-i}^k ( \widehat \bftheta_i^k - \bftheta_{0,i}^k ) \|^2 & \leq
%%\| \bfY_{-i}^k ( \bftheta_i^k - \bftheta_{0,i}^k ) \|^2 + 
%%\frac{2}{n} ( \widehat \bfepsilon_i^k )^T \bfY_{-i}^k ( \widehat \bftheta_i^k - \bftheta_i^k )\\
%%& + 2 \sum_{j \neq i, g \in \cG^{ij}} \lambda_{ij}^g ( \| \bftheta_{ij}^{[g]} - \widehat \bftheta_{ij}^{[g]} \|)
%%\end{align*}
%The statement of this proposition is same as that of Proposition A.1 in 15-656, and can proved in a similar fashion. The only difference is a modified choice of $\lambda_{ij}^g$, which we obtain from Proposition~\ref{lemma:LemmaE2}.
%
%\end{proof}

\textit{Part II.} Proof of Thm 2 in 15-656 follows. We only need a new bound for $Var (\bfY_i^k | \bfY_{-i}^k, \bfX^k, \widehat \bfB_i^k)$. For this we have
%
$$ Var (\bfY_i^k | \bfY_{-i}^k, \bfX^k, \widehat \bfB_i^k) = \BE (\widehat \bfepsilon_i^k)^2
= \BE ( \bfepsilon_i^k + \bfdelta_i^k)^2
\leq \left( \frac{1}{d_0} + \frac{c(v_\beta)}{n} \right)^2
$$
%
applying cauchy-schwarz inequality followed by assumption (A2). Now Replace $1/\sqrt{n d_0}$ in choice of $\lambda, \alpha_n$ in Thm 2 statement with $1/\sqrt{n} (\sqrt{1/d_0} + \sqrt{c(v_\beta)/ n})$.

\end{proof}

%\begin{Proposition}
%Given fixed $\widehat \cB$, prediction errors follow bound in T2 with high enough probability.
%\end{Proposition}

\begin{Proposition}\label{prop:ErrorRE}
Consider deterministic $\widehat \cB$ satisfying assumption (T1). Then for sample size $n \succsim \log (pq)$ and $k \in \cI_K$,

\begin{enumerate}
\item We have $\| \bfX^k ( \hat \bfB^k - \bfB_0^k ) \|_\infty \leq c( v_\beta)$, where
%
$$
c(v_\beta) =\sqrt n v_\beta \left[ \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x^k n}} + \max_j \sigma_{x,jj}^k \right]^{1/2}; \quad
c_x^k = \left[ 128 ( 1 + 4 \Lambda_{\max} (\Sigma_x^k)  )^2 \max_j (\sigma_{x,jj}^k)^2 \right]^{-1}
$$
%
with probability $ \geq 1 - 1/p^{\tau_1-2}, \tau_1 > 2$.
%
\item $\widehat \bfE^k$ satisfies the RE condition: $ \widehat \bfE^k \sim RE (\psi^*, \phi^*)$, where 
%
$$
\psi^* = \frac{ \Lambda_{\min} (\Sigma_x^k)}{2}; \quad \phi^* = \frac{ \psi^* \log p}{n} + 2 v_\beta c_2 [ \Lambda_{\max} (\Sigma_x^k) \Lambda_{\max} (\Sigma_y^k) ]^{1/2} \sqrt{\frac{ \log(pq)}{n}}
$$
%
with probability $\geq 1 - 6c_1 \exp [-(c_2^2-1) \log(pq)], c_1 > 0, c_2 > 1$.
\end{enumerate} 
\end{Proposition}

\begin{proof}[Proof of Proposition~\ref{prop:ErrorRE}]

For any sub-gaussian zero-mean design matrix $\bfX \in \BM(n,p)$ with parameters $(\Sigma_x, \sigma_x^2)$, and any $\hat \bfB, \bfB_0 \in \BM(p,q)$ such that $\| \hat \bfB - \bfB_0 \|_F \leq v_\beta$, we follow the proof of Proposition 3 in \cite{LinEtal16} to obtain that the following holds
%
$$
\left\| (\widehat \bfB - \bfB_0)^T \left( \frac{\bfX^T \bfX}{n} \right) (\widehat \bfB - \bfB_0) \right\|_\infty \leq
v_\beta^2 \left[ \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x n}} + \max_j \sigma_{x,jj}^k \right]
$$
%
with probability $\geq 1 - 1/p^{\tau_1-2}$ for some $\tau_1>2$, where
%
$$
c_x = \left[ 128 ( 1 + 4 \sigma_x^2)^2 \max_j (\sigma_{x,jj}^k)^2 \right]^{-1}
$$
%
Here we substitute $\bfX, \hat \bfB, \bfB_0$ with $\bfX^k, \hat \bfB^k, \hat \bfB_0^k$ respectively. Since rows of $\bfX^k$ come independently from $\cN( {\bf 0}, \Sigma_x^k)$, $\sigma_x^2$ in our case is the spectral norm of $\Sigma_x^k$ \citep{LohWainwright12}, which is $\Lambda_{\max} (\Sigma_x^k)$. Finally
%
$$
\| \bfX^k ( \hat \bfB^k - \bfB_0^k ) \|_\infty \leq 
\sqrt{\left\| (\widehat \bfB^k - \bfB_0^k)^T (\bfX^k)^T \bfX^k (\widehat \bfB^k - \bfB_0^k) \right\|_\infty}
$$
%
The proof of part 1 is immediate now.

For part 2, we start with an auxiliary lemma:
%
\begin{Lemma}\label{lemma:ErrorRElemma1}
For a sub-gaussian design matrix $\bfX \in \BM(n,p)$ with columns having mean ${\bf 0}_p$ and covariance matrix $\Sigma_x$, the sample covariance matrix $\widehat \Sigma_x = \bfX^T \bfX/n$ satisfies the RE condition
%
$$
\widehat \Sigma_x \sim RE \left( \frac{\Lambda_{\min} ( \Sigma_x) }{2}, \frac{\Lambda_{\min} ( \Sigma_x) \log p }{2 n} \right)
$$
\end{Lemma}
%
This is the same as Lemma 2 in Appendix B of \cite{LinEtal16} and its proof can be found there. Now denote $\widehat \bfE^k = \bfY^k - \bfX^k \widehat \bfB^k$. For $\bfv \in \BR^q$, we have
%
\begin{align}\label{eqn:ErrorREeqn1}
\bfv^T \widehat \bfS^k \bfv &= \frac{1}{n} \| \widehat \bfE^k \bfv \|^2 \notag\\
&= \frac{1}{n} \| (\bfE^k + \bfX^k ( \bfB_0^k - \widehat \bfB^k ))\bfv \|^2 \notag\\
&= \bfv^T \bfS^k \bfv + \frac{1}{n} \| \bfX^k (\bfB_0^k - \widehat \bfB^k) \bfv \|^2 + 2 \bfv^T (\bfB_0^k - \widehat \bfB^k)^T \left( \frac {(\bfX^k)^T \bfE^k}{n} \right) \bfv
\end{align}
%
For the first summand, $ \bfv^T \bfS^k \bfv \geq \psi_y \| \bfv \|^2 - \phi_y \| \bfv \|_1^2$ with $\psi_y = \Lambda_{\min} (\Sigma_y^k)/2, \phi_y = \psi_y \log p/n$ by applying Lemma \ref{lemma:ErrorRElemma1} on $\bfS^k$. The second summand is greater than or equal to 0. For the third summand,
%
$$
2 \bfv^T (\widehat \bfB^k - \bfB_0^k)^T \left( \frac {(\bfX^k)^T \bfE^k}{n} \right) \bfv \geq
-2 v_\beta \left\| \frac {(\bfX^k)^T \bfE^k}{n} \right\|_\infty \| \bfv \|_1^2
$$
%
by assumption (T1). Now we use another lemma:
\begin{Lemma}\label{lemma:ErrorRElemma2}
For zero-mean independent sub-gaussian matrices $\bfX \in \BM(n,p), \bfE \in \BM(n,q)$ with parameters $(\Sigma_x, \sigma_x^2)$ and $(\Sigma_e, \sigma_e^2)$ respectively, given that $n \succsim \log(pq)$ the following holds with probability $\geq 1 - 6c_1 \exp [-(c_2^2-1) \log(pq)]$ for some $c_1 >0, c_2 > 1$:
%
$$
\frac{1}{n} \| \bfX^T \bfE \|_\infty \leq c_2 [ \Lambda_{\max} (\Sigma_x) \Lambda_{\max} (\Sigma_e) ]^{1/2} \sqrt{\frac{ \log(pq)}{n}}
$$
%
\end{Lemma}
%
This is a part of Lemma 3 of Appendix B in \cite{LinEDtal16}, and has been proved therein. Here we take $\bfX \equiv \bfX^k, \bfE \equiv \bfE^k$, and subsequently collecting all summands in (\ref{eqn:ErrorREeqn1}) get
%
$$
\bfv^T \widehat{ \bfS}^k \bfv \geq \psi_y \| \bfv \|^2 - \left( \phi_y + 2 v_\beta c_2 [ \Lambda_{\max} (\Sigma_x^k) \Lambda_{\max} (\Sigma_y^k) ]^{1/2} \sqrt{\frac{ \log(pq)}{n}} \right) \| \bfv \|_1^2
$$
with probability $\geq 1 - 6c_1 \exp [-(c_2^2-1) \log(pq)]$. This concludes the proof.
\end{proof}


%-----------------------------
\hrulefill
%-----------------------------


Now concentrate on the $k$-population estimation problem. We want to obtain
%
$$
\widehat \bfbeta = \argmin_{ \bfbeta \in \BR^{pqK}} \{ -2 \bfbeta \widehat \bfgamma + \bfbeta^T \bfGamma \bfbeta + \| \bfbeta \|_{2,g} \}
$$
with
$$
\bfbeta = \begin{bmatrix}
\ve (\bfB^1)\\
\vdots\\
\ve (\bfB^K)\\
\end{bmatrix}; \quad
\bfGamma = \begin{bmatrix}
I_q \otimes (\bfX^1) TX^1 / n) & &\\
& \ddots &\\
& & I_q \otimes (\bfX^K)^T X^K / n)
\end{bmatrix} 
$$
\begin{Theorem}

\end{Theorem}

\bibliographystyle{apalike}
%\bibliographystyle{imsart-nameyear}
\bibliography{snpbib}
\end{document}