\documentclass[12pt, letterpaper]{article}
%\documentclass[aoas,preprint]{imsart}
\pdfoutput=1

\usepackage{mycommands1,amssymb,amsmath,amsthm,color,pagesize,outlines,cite,subfigure}
\usepackage[small]{caption}
\usepackage{hyperref} % for linking references
\hypersetup{colorlinks = true, citecolor = blue, urlcolor = blue}

\usepackage{stackrel}

\usepackage[round]{natbib}

% for algorithm
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}

% DON'T change margins - should be 1 inch all around.
\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.9in}
\addtolength{\textheight}{0.9in}
\addtolength{\topmargin}{-.4in}

%% measurements for 1 inch margin
%\addtolength{\oddsidemargin}{-.875in}
%\addtolength{\evensidemargin}{-.875in}
%\addtolength{\textwidth}{1.75in}
%\addtolength{\topmargin}{-.875in}
%\addtolength{\textheight}{1.75in}

%\pagestyle{myheadings}
%\markboth{}{\underline{{\bf Notes: (do not circulate)} \hspace{4.5cm} {\sc  Ansu Chatterjee} \hspace{0.25cm}}}

\DeclareMathOperator*{\ve}{vec}
\DeclareMathOperator*{\diag}{diag }
\DeclareMathOperator*{\Tr}{Tr}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\Th}{^{\text{th}}}

\makeatletter
\newcommand{\opnorm}{\@ifstar\@opnorms\@opnorm}
\newcommand{\@opnorms}[1]{%
  \left|\mkern-1.5mu\left|\mkern-1.5mu\left|
   #1
  \right|\mkern-1.5mu\right|\mkern-1.5mu\right|
}
\newcommand{\@opnorm}[2][]{%
  \mathopen{#1|\mkern-1.5mu#1|\mkern-1.5mu#1|}
  #2
  \mathclose{#1|\mkern-1.5mu#1|\mkern-1.5mu#1|}
}
\makeatother

%% Appendix theorem counter
\usepackage{chngcntr}
\usepackage{apptools}
\AtAppendix{\counterwithin{Theorem}{section}}
\numberwithin{equation}{section}

\begin{document}

\newtheorem{Theorem}{Theorem}[section]
\newtheorem{Lemma}[Theorem]{Lemma}
\newtheorem{Corollary}[Theorem]{Corollary}
\newtheorem{Proposition}[Theorem]{Proposition}
\newtheorem{Conjecture}[Theorem]{Conjecture}
\theoremstyle{definition} \newtheorem{Definition}[Theorem]{Definition}

\title{Joint Estimation and Inference for Multiple Multi-layered Gaussian Graphical Models}
\date{}
\author{Subhabrata Majumdar}
\maketitle

\noindent\textbf{Abstract}: 
The rapid development of high-throughput technologies has enabled generation of data from biological processes that span multiple layers, like genomic, proteomic or metabolomic data; and pertain to multiple sources, like disease subtypes or experimental conditions. In this work we propose a general statistical framework based on graphical models for horizontal (i.e. across conditions or subtypes) and vertical (i.e. across different layers containing data on molecular compartments) integration of information in such datasets. We start with decomposing the multi-layer problem into a series of two-layer problems. For each two-layer problem, we model the outcomes at a node in the lower layer as dependent on those of other nodes in that layer, as well as all nodes in the upper layer. Following the biconvexity of our objective function, this estimation problem decomposes into two parts, where we use neighborhood selection and subsequent refitting of the precision matrix to quantify the dependency of two nodes in a single layer, and use group-penalized least square estimation to quantify the directional dependency of two nodes in different layers. Finally, to test for differences in these directional dependencies across multiple sources, we devise a hypothesis testing procedure that utilizes already computed neighborhood selection coefficients for nodes in the upper layer. We establish theoretical results for the validity of this testing procedure and the consistency of our estimates, and also evaluate their performance through simulations and a real data application.

\vspace{.5cm}
\noindent\textbf{Keywords}: Data integration; Gaussian Graphical Models; Neighborhood selection; Group lasso

\newpage

\section{Notations}
We shall denote scalars by small letters, vectors by bold small letters and matrices by bold capital letters. For any matrix $\bfA$, $(\bfA)_{ij}$ denote its element in the $(i,j)^\text{th}$ position. For $a,b \in \BN$, we denote the set of all $a \times b$ real matrices by $\BM(a,b)$. For any positive integer $c$, define $\cI_c = \{ 1, \ldots, c\}$.

\section{Model}
Consider the two -layered setup:

\begin{eqnarray}
\BX^k = (X^k_1, \ldots, X^k_p)^T \sim \cN (0, \Sigma^k_x)\\
\BY^k = \BX^k \bfB^k + \BE^k; \quad \BE^k = (E^k_1, \ldots, E^k_p)^T \sim \cN (0, \Sigma^k_y)\\
\bfB^k \in \BM(p,q); \quad \Omega^k_x = (\Sigma^k_x)^{-1}; \quad \Omega^k_y = (\Sigma^k_y)^{-1}
\end{eqnarray}

Want to estimate $\{ (\Omega^k_x, \Omega^k_y, \bfB^k); k \in \cI_K$ from data $\cZ^k = \{ (\bfY^k, \bfX^k); \bfY^k \in \BM(n,q), \bfX^k \in \BM(n,p), k \in \cI_K\}$. in presence of known grouping structures $\cG_x, \cG_y, \cH$ respectively. 

%\paragraph{Notation:} Denote 3-dimensional array objects as elements of $\BT(a,b,c)$, the set of all $a \times b \times c$ tensors.
%Define $\cS^x = (\Omega^k_x), \cS^y = (\Omega^k_y), \cB = (\bfB^k)$

Estimation of $\{ \Omega_x^k \}$ done using JSEM. For the other part, we use the following two-step procedure:

\begin{enumerate}
\item Run neighborhood selection on $y$-network incorporating effects of $x$-data and an additional blockwise group penalty:
%
\begin{align}
& \min_{\cB, \Theta} \left\{ \sum_{j=1}^q  \frac{1}{n_k} \left[ \sum_{k=1}^K \| \bfY^k_j - (\bfY_{-j}^k - \bfX^k \bfB_{-j}^k) \bftheta_j^k - \bfX^k \bfB_j^k \|^2 + \sum_{j \neq i} \sum_{g \in \cG_y^{ij}} \lambda_{ij}^g \| \bftheta_{ij}^{[g]} \| \right] + \sum_{b \in \cG_x \times \cG_y \times \cH} \eta^b \| \bfB^{[b]} \| \right\}\\
&= \min \left\{ f ( \cY, \cX, \cB, \Theta) + P (\Theta) + Q (\cB) \right\} 
\end{align}
%
where $\Theta = \{ \Theta_i \}, \cB = \{ \bfB^k \}, \cY = \{ \bfY^k \}, \cX = \{ \bfX^k \}, \cE = \{ \bfE^k \}$.

This estimates $\cB$ { \colrbf (possibly refit and/or within-group threshold) }.

\item Step I part 2 and step II of JSEM (see 15-656 pg 6) follows to estimate $\{ \Omega_y^k \}$.
\end{enumerate}

The objective function is bi-convex, so we are going to do the following in step 1-

\begin{itemize}
\item Start with initial estimates of $\cB$ and $\Theta$, say $\cB^{(0)}, \Theta^{(0)}$.
\item Iterate:
%
\begin{align}
\Theta^{(t+1)} &= \argmin \left\{ f ( \cY, \cX, \cB^{(t)}, \Theta^{(t)}) + P (\Theta^{(t)}) \right\}\\
\cB^{(t+1)} &= \argmin \left\{ f ( \cY, \cX, \cB^{(t)}, \Theta^{(t+1)}) + Q (\cB^{(t)}) \right\}
\end{align}
\item Continue till convergence.
\end{itemize}
%

\section{Conditions}
Conditions A1 from JSEM paper holds for $\cX$ and $\cE$. Also A2, A3 from JSEM paper.

\section{Results}
To prove the results in this section, we shall use a reparametrization of the neighborhood coefficients at the lower level. Specifically, notice that for $j \in \cI_q, k \in \cI_K$, the corresponding summand in $f(\cY, \cX, \cB, \Theta)$ can be rearranged as
%
\begin{align*}
\| \bfY^k_j - \bfX^k \bfB_j^k - (\bfY_{-j}^k - \bfX^k \bfB_{-j}^k) \bftheta_j^k \|^2 &=
\| \bfY^k_j - \bfY_{-j}^k \bftheta_j^k - (\bfX^k \bfB_j^k -\bfX^k \bfB_{-j}^k \bftheta_j^k) \|^2 \\
&= \| ( \bfY - \bfX \bfB ) \bfT_j^k \|^2
\end{align*}
%
where
%
$$
T_{jj'}^k = \begin{cases}
1 \text{ if } j = j'\\
- \theta_{jj'}^k \text{ if } j \neq j'
\end{cases}
$$
%
Thus, with $\bfT^k := (\bfT_j^k)_{j \in \cI_q}$, we have
$$
f( \cY, \cX, \cB, \Theta) = \frac{1}{n} \sum_{j=1}^p \sum_{k=1}^K \| ( \bfY^k - \bfX^k \bfB^k ) \bfT_j^k \|^2
= \frac{1}{n} \sum_{k=1}^K \| \bfY^k - \bfX^k \bfB^k ) \bfT^k \|_F^2
= \sum_{k=1}^K \Tr (\bfS^k (\bfT^k)^2 )
$$
%
where $\bfS^k = (1/n) (\bfY^k - \bfX^k \bfB^k) (\bfY^k - \bfX^k \bfB^k)^T$ is the sample covariance matrix.

Now suppose $\bfbeta = \ve (\bfB)$, and any subscript or superscript on $\bfB$ will be passed on to $\bfbeta$. Denote by $\widehat \bfbeta$ and $\widehat \Theta$ the generic estimators given by
%
\begin{align}
\widehat \bfbeta &= \argmin_{\bfbeta \in \BR^{pq}} \left\{-2 \bfbeta^T \widehat \bfgamma + \bfbeta^T \widehat \bfGamma \bfbeta + \lambda_n \sum_{g \in \cG} \| \bfbeta^{[g]}  \| \right\} \label{eqn:EstEqn1}\\
\widehat \Theta_j &= \argmin_{\Theta_j \in \BM(q-1, K)} \left\{ \frac{1}{n} \sum_{k=1}^K \| \bfY^k_j - \bfX^k \widehat \bfB_j^k - (\bfY_{-j}^k - \bfX^k \widehat \bfB^k_{-j} ) \bftheta_j^k \|^2 + \gamma_n \sum_{j \neq j'} \sum_{g \in \cG_y^{jj'}} \| \bftheta_{jj'}^{[g]} \| \right\} \label{eqn:EstEqn2}
\end{align}
%
where
%
$$
\widehat \bfGamma = \begin{bmatrix}
(\widehat \bfT^1)^2 \otimes \frac{(\bfX^1)^T \bfX^1}{n} & &\\
& \ddots &\\
& & (\widehat \bfT^K)^2 \otimes \frac{(\bfX^K)^T \bfX^K}{n}
\end{bmatrix}; \quad
\widehat \bfgamma = \begin{bmatrix}
(\widehat \bfT^1)^2 \otimes \frac{(\bfX^1)^T}{n}\\
\vdots\\
(\widehat \bfT^K)^2 \otimes \frac{(\bfX^K)^T}{n}
\end{bmatrix}
\begin{bmatrix}
\ve (\bfY^1)\\
\vdots\\
\ve (\bfY^K)
\end{bmatrix}
$$
with $\widehat \bfT^k$ defined the same way using $\widehat \bftheta_j^k$ as we defined $\bfT^k$ using $\bftheta_j^k$.
\begin{Theorem}\label{thm:ThetaThm}
Assume fixed $\cX, \cE$ and deterministic $\widehat \cB = \{ \widehat \bfB^k \}$. Also for $k = 1, \ldots, K$,

\noindent{\bf(T1)} $\| \widehat \bfB^k - \bfB^k_0 \|_1 \leq v_\beta$, where $v_\beta = { \colrbf \eta_\beta \sqrt{\frac{\log (pq)}{n}}}$ with $\eta_\beta$ being a quantity depending on $\cB$ only;

%\noindent{\bf(T2)} $\| \bfX^k (\widehat \bfB^k - \bfB^k_0 ) \|_\infty \leq c(v_\beta)$, where $c(v_\beta)$ is $O(1)$ and depends on $v_\beta$.

\noindent{\bf(T2)} Denote \textbf{$\widehat \bfE^k = \bfY^k - \bfX^k \widehat \bfB^k, k \in \cI_K$}. Then for all $j \in \cI_q$,
%
$$
\frac{1}{n} \left\| (\widehat \bfE_{-j}^k)^T \widehat \bfE^k \bfT_{0,j}^k \right\|_\infty \leq
%\BQ_{\max} = \max_{k \in \cI_k} 
\BQ \left(v_\beta, \Sigma_x^k, \Sigma_y^k \right)
$$
%
where $\BQ \left(v_\beta, \Sigma_x^k, \Sigma_y^k \right)$ is a $O(1/\sqrt n)$ deterministic function depending on the population parameters $\cB, \Sigma_x^k$ and $\Sigma_y^k$.

\noindent{\bf(T3)} Denote $\widehat \bfS^k = (\widehat \bfE^k)^T \widehat \bfE^k/n$. Then $\widehat \bfS^k \sim RE(\psi^k, \phi^k)$ with $Kq \phi \leq \psi/2$ where $ \psi = \min_k \psi^k, \phi = \max_k \phi^k $;

\noindent{\bf(T4)} Assumption (A2) holds for $\Sigma_y^k$.

Then, given the choice of tuning parameter
%
$$
\gamma_n \geq 4 \sqrt q \BQ_{\max}; \quad \BQ_{\max} := \max_{k \in \cI_K} \BQ \left(v_\beta, \Sigma_x^k, \Sigma_y^k \right)
$$
%
the following holds
%
\begin{align}
\frac{1}{K} \sum_{k=1}^K \| \widehat \Omega_y^k - \Omega_y^k \|_F^2 \leq
{\colrbf O \left( \frac{48 c_0 \sqrt{ q |g_{\max}| S} \BQ_{\max}} {\psi}
\right)}
\end{align}
%
where $|g_{\max}|$ is the maximum group size.

%Further if A1 holds with $s = s_0$, and A3 is satisfied then
%
%(II) Direction consistency.
\end{Theorem}

\begin{proof}[Proof of Theorem~\ref{thm:ThetaThm}]
The proof has three parts, where we prove the consistency of the neighborhood regression coefficients, selection of edge sets, and finally the refitting step, respectively. This is the same structure as the proof of Theorem 1 in \cite{MaMichailidis15}, where they prove consistency of the JSEM estimates. The derivation of the first part is different from that in the JSEM proof, which we shall show in detail. The second and third parts follow similar lines, incorporating the updated quantities from part 1. For these we provide a rough sketch and leave the details to the reader.

%Define $\bfT_{0,j}^k$ the same way as $\bfT_j^k$. For any $g \in \cG^{jj'}, k \in g$, and $j \neq j'$, let
%%
%$$
%\widehat \bfepsilon_j^k = (\bfY^k - \bfX^k \widehat \bfB^k ) \bfT_{0,j}^k; \quad
%\widehat \zeta_{jj'}^k = \frac{(\widehat \bfepsilon_j^k)^T \bfY_{j'}^k}{n}; \quad
%\widehat \bfzeta_{jj'}^{[g]} = (\widehat \zeta_{jj'}^k)_{k \in g}
%$$
%%
%Consider the random event $\cA = \bigcap_{j, j'\neq j, g} \cA_{jj'}^g$ with $\cA_{jj'}^g = \{ 2 \| \widehat \bfzeta_{jj'}^{[g]} \| \leq \lambda_{jj'}^g \}$.
%
%\begin{Lemma}\label{lemma:LemmaE2}
%Given that $\lambda_{jj'}^g$ are chosen as
%%
%$$
%\lambda_{jj'}^g \geq \max_{k \in g} \frac{2}{\sqrt{n \omega_{jj}^k}} \left( \sqrt{|g| (1 + 2 c(v_\beta)) } + \frac{\pi}{\sqrt 2} \sqrt {r \log G_0} \right)
%$$
%%
%we shall have $ \BP (\cA) \geq 1 - 2p G_0^{1-q} $ for some $r>1$.
%\end{Lemma}
%
%\begin{proof}[Proof of Lemma~\ref{lemma:LemmaE2}]
%We follow the proof of Lemma E.2 in 15-656, with $\bfY_j^k, \widehat \bfepsilon_j^k, \widehat \zeta_{jj'}^k, \widehat \bfzeta_{jj'}^{[g]}$ in place of $\bfX_j^k, \bfepsilon_i^k, \zeta_{ij}^k, \bfzeta_{ij}^{[g]}$ respectively. Proceeding in a similar fashion we get
%%
%$$
%\| \widehat \bfzeta_{jj'}^{[g]} \|^2 = \frac{1}{n} \left[ \| \bfZ^{[g]} \|^2 + \sum_{k \in g} \left\{ 2 Z^k (\bfQ_{j'}^k)^T \bfdelta_j^k + | (\bfQ_{j'}^k)^T \bfdelta_j^k |^2 \right\} \right]
%$$
%%
%where $\bfZ^{[g]} = (Z^k)_{k \in g}; Z^k = (\bfQ_{j'}^k)^T \bfepsilon_j^k$ with $\bfepsilon_j^k :=(\bfY^k - \bfX^k \bfB_0^k ) \bfT_{0,j}^k$, $\bfQ_j^k$ is the first eigenvector of $\bfY_j^k (\bfY_j^k)^T/n$, and $\bfdelta_j^k := \bfX^k (\bfB_0^k - \widehat \bfB^k) \bfT_{0,j}^k$.
%
%By cauchy-schwarz inequality, $| (\bfQ_{j'}^k)^T \bfdelta_j^k | \leq \| \bfdelta_j^k \| \leq \| \bfX^k (\bfB_0^k - \widehat \bfB^k) \|_\infty \| \bfT_{0,j} \|_1$. Now since $\Sigma_y^k$ is diagonally dominant,
%%
%$$
%\sum_{j \neq j'} |T_{0,jj'}^k| =  \sum_{j \neq j'} |\theta_{0,jj'}^k| = \sum_{j \neq j'} \frac{|\sigma_{y,jj'}^k|}{ \sigma_{y,jj}^k} \leq 1
%$$
%%
%Also $T_{0,jj}^k = 1$, so that $\| \bfT_{0,j}^k \|_1 \leq 2$. Thus $\| \bfdelta_j^k \| \leq 2 \| \bfX^k (\bfB_0^k - \widehat \bfB^k) \|_\infty$. Hence by assumption (T2),
%%
%$$
%\| \widehat \bfzeta_{jj'}^{[g]} \| \leq \frac{1}{\sqrt n} ( \| \bfZ^{[g]} \| + 2 |g| c(v_\beta))
%$$
%%
%so that
%$$
%\BP ( \{ \cA_{jj'}^g \}^c ) = \BP \left( \| \widehat \bfzeta_{jj'}^{[g]} \| > \frac{\lambda_{jj'}^g}{2} \right) \leq \BP \left( \| \bfZ^{[g]} \|  > \frac{\sqrt n \lambda_{jj'}^g}{2} - 2 |g| c(v_\beta)  \right)
%$$
%We now proceed through the proof of Lemma E.2 in 15-656 to end up with the choice of $\lambda_{ij}^g$.
%\end{proof}
%

%All subsequent derivations in the theorem go through with the new choice of $\lambda_{ij}^g$.

{\it Step 1: consistency of neighborhood regression.} The following proposition establishes error bounds for the estimated $y$-neighborhood coefficients.

\begin{Proposition}\label{prop:Thm1ProofProp2}
Consider the estimation problem in (\ref{eqn:EstEqn2}) and choose $\gamma_n \geq 4 \sqrt q \BQ_{\max}$. Given the conditions (T2) and (T3) hold, for any solution of (\ref{eqn:EstEqn2}) we shall have
%
\begin{align}
\| \widehat \Theta_j - \Theta_{0,j} \|_F & \leq 12 \sqrt{|g_{\max}| s_j} \gamma_n / \psi \label{eqn:Thm1ProofProp2Bd1}\\
\sum_{j \neq j', g \in \cG_y^{jj'}} \| \hat \bftheta_{jj'}^{[g]} - \bftheta_{0,jj'}^{[g]} \| & \leq 48 | g_{\max}| s_j \gamma_n / \psi \label{eqn:Thm1ProofProp2Bd2}
\end{align}
%
Also denote the non-zero support of $\widehat \Theta_j$ by $\widehat \cS_j$, i.e. $\widehat \cS_j = \{ (j',g): \hat \bftheta_{jj'}^{[g]} \neq {\bf 0} \}$. Then
%
\begin{align}
| \widehat \cS_j| \leq 128 | g_{\max}| s_j / \psi \label{eqn:Thm1ProofProp2Bd3}
\end{align}
\end{Proposition}

\begin{proof}[Proof of Proposition~\ref{prop:Thm1ProofProp2}]
In its reparametrized version, (\ref{eqn:EstEqn2}) becomes
%
\begin{align}
\widehat \bfT_j = \argmin_{\bfT_j} \left\{ \frac{1}{n} \sum_{k=1}^K \| (\bfY^k - \bfX^k \widehat \bfB^k) \bfT_j^k \|^2 + \gamma_n \sum_{j \neq j', g \in \cG_y^{jj'}} \| \bfT_{jj'}^{[g]} \| \right\}
\end{align}
%
with $\bfT_{jj'}^{[g]} := (T_{jj'}^k)_{k \in g}$. Now for any $\bfT_j \in \BM(q, K)$ we have
%
$$
\frac{1}{n} \sum_{k=1}^K \| (\bfY^k - \bfX^k \widehat \bfB^k) \widehat \bfT_j^k \|^2 + \gamma_n \sum_{j \neq j', g \in \cG_y^{jj'}} \| \widehat \bfT_{jj'}^{[g]} \| \leq
\frac{1}{n} \sum_{k=1}^K \| (\bfY^k - \bfX^k \widehat \bfB^k) \bfT_j^k \|^2 + \gamma_n \sum_{j \neq j', g \in \cG_y^{jj'}} \| \bfT_{jj'}^{[g]} \|
$$
%
For $\bfT_j = \bfT_{0,j}$ this reduces to
%
\begin{align}\label{eqn::Thm1ProofProp2Eqn1}
\sum_{k=1}^K (\bfd_j^k)^T \widehat \bfS^k \bfd_j^k & \leq - 2 \sum_{k=1}^K (\bfd_j^k)^T \widehat \bfS^k \bfT_{0,j}^k + \gamma_n \sum_{j \neq j', g \in \cG_y^{jj'}} \left( \| \bfT_{jj'}^{[g]} \| -  \| \bfT_{jj'}^{[g]} + \bfd_{jj'}^{[g]}\| \right)
\end{align}

%
with $\widehat \bfT_j^k := \bfT_{0,j}^k + \bfd_{j}^k$ etc. For the $k^\text{th}$ summand in the first term on the right hand side, since $d_{jj}^k = 0$, $\widehat \bfE^k \bfd_j^k = \widehat \bfE_{-j}^k \bfd_{-j}^k$. Thus by cauchy-schwarz inequality
%
$$
\left| (\bfd_j^k)^T \widehat \bfS^k \bfT_{0,j}^k \right| \leq
\| (\bfd_j^k \| \left\| \frac{ (\widehat \bfE^k)^T \widehat \bfE^k }{n} \bfT_{0,j}^k \right\| \leq
\sqrt{q} \| \bfd_j^k \| \left\| \frac{1}{n} (\widehat \bfE_{-j}^k)^T \widehat \bfE^k \bfT_{0,j}^k \right\|_\infty
$$
%
which is $\leq \| \bfd_j^k\| \sqrt q \BQ(v_\beta, \Sigma_x^k, \Sigma_y^k) \leq \| \bfd_j^k\| \gamma_n/4 $ by assumption (T2) and choice of $\gamma_n$. For the second term, 
suppose $\cS_{0,j}$ is the support of $\Theta_{0,j}$, i.e. $\cS_{0,j} = \{ (j',g): \bftheta_{jj'}^{[g]} \neq 0 \}$. Then
%
\begin{align*}
\sum_{j \neq j', g \in \cG_y^{jj'}} \left( \| \bfT_{jj'}^{[g]} \| -  \| \bfT_{jj'}^{[g]} + \bfd_{jj'}^{[g]}\| \right) & \leq
\sum_{(j',g) \in \cS_{0,j}} \left( \| \bfT_{jj'}^{[g]} \| -  \| \bfT_{jj'}^{[g]} + \bfd_{jj'}^{[g]}\| \right) -
\sum_{(j',g) \notin \cS_{0,j}} \| \bfd_{jj'}^{[g]} \|\\
& \leq \sum_{(j',g) \in \cS_{0,j}} \| \bfd_{jj'}^{[g]} \| - \sum_{(j',g) \notin \cS_{0,j}} \| \bfd_{jj'}^{[g]} \|
\end{align*}
%$ \| \bfT_{jj'}^{[g]} \| -  \| \bfT_{jj'}^{[g]} + \bfd_{jj'}^{[g]}\| \leq \| \bfd_{jj'}^{[g]}\|$ by triangle inequality, and then $ \| \bfd_{jj'}^{[g]}\| \leq \sum_{k \in g} | d_{jj'}^k | $, 
so that (\ref{eqn::Thm1ProofProp2Eqn1}) reduces to
%
\begin{align}\label{eqn:Thm1ProofProp2Eqn3}
\sum_{k=1}^K (\bfd_j^k)^T \widehat \bfS^k \bfd_j^k & \leq 
\frac{\gamma_n}{2} \left[ \sum_{(j',g) \in \cS_{0,j}} \| \bfd_{jj'}^{[g]} \| + \sum_{(j',g) \notin \cS_{0,j}} \| \bfd_{jj'}^{[g]} \| \right] +
\gamma_n \left[ \sum_{(j',g) \in \cS_{0,j}} \| \bfd_{jj'}^{[g]} \| - \sum_{(j',g) \notin \cS_{0,j}} \| \bfd_{jj'}^{[g]} \| \right] \notag\\
& = \frac{3 \gamma_n}{2} \sum_{(j',g) \in \cS_{0,j}} \| \bfd_{jj'}^{[g]} \| - \frac{\gamma_n}{2} \sum_{(j',g) \notin \cS_{0,j}} \| \bfd_{jj'}^{[g]} \| \notag\\
& \leq \frac{3 \gamma_n}{2} \sum_{j \neq j', g \in \cG_y^{jj'}} \| \bfd_{jj'}^{[g]} \|
\end{align}
%
Since the left hand side is $\geq 0$, this also implies
%
$$
\sum_{(j',g) \notin \cS_{0,j}} \| \bfd_{jj'}^{[g]} \| \leq 3 \sum_{(j',g) \in \cS_{0,j}} \| \bfd_{jj'}^{[g]} \| \quad \Rightarrow
\sum_{j \neq j', g \in \cG_y^{jj'}} \| \bfd_{jj'}^{[g]} \| \leq
4 \sum_{(j',g) \in \cS_{0,j}} \| \bfd_{jj'}^{[g]} \| \leq 4 \sqrt{| g_{\max}| s_j} \| \bfD_j \|_F
$$
% 
with $\bfD_j = (\bfd_j^k)_{k \in \cI_K}$. Now the RE condition on $\widehat \bfS^k$ means that
%
$$
\sum_{k=1}^K (\bfd_j^k)^T \widehat \bfS^k \bfd_j^k \geq 
\sum_{k=1}^K \left( \psi_k \| \bfd_j^k \|^2 - \phi_k \| \bfd_j^k \|_1^2 \right) \geq
\psi \| \bfD_j \|_F^2 - \phi \| \bfD_j \|_1^2 \geq 
(\psi - Kq \phi ) \| \bfD_j \|_F^2 \geq \frac{\psi}{2}  \| \bfD_j \|_F^2
$$
%
by assumption (T3). Thus we finally have
%
\begin{align}\label{eqn:Thm1ProofProp2Eqn2}
\frac{\psi}{3} \| \bfD_j \|_F^2 & \leq
\gamma_n \sum_{j \neq j', g \in \cG_y^{jj'}} \| \bfd_{jj'}^{[g]} \| \leq
4 \gamma_n \sqrt{| g_{\max}| s_j} \| \bfD_j\|_F
\end{align}
%
Since
%
$$
(\bfD_j)_{j',k} = \widehat T_{jj'}^k - T_{0,jj'}^k = \begin{cases}
0 \text{ if } j = j'\\
-(\widehat \theta_{jj'}^k - \theta_{0,jj'}^k ) \text{ if } j \neq j'
\end{cases}
$$
%
The bounds in (\ref{eqn:Thm1ProofProp2Bd1}) and (\ref{eqn:Thm1ProofProp2Bd2}) are obtained by replacing the corresponding elements in (\ref{eqn:Thm1ProofProp2Eqn2}).

For the bound on $| \widehat \cS_j|$, notice that if $\hat \bftheta_{jj'}^{[g]} \neq 0$ for some $(j',g)$,
%
\begin{align*}
\frac{1}{n} \sum_{k \in g} \left| ((\widehat \bfE_{-j}^k)^T \widehat \bfE^k ( \widehat \bfT_j^k - \bfT_{0,j}^k ))^{j'} \right| & \geq
\frac{1}{n} \sum_{k \in g} \left| ((\widehat \bfE_{-j}^k)^T \widehat \bfE^k \widehat \bfT_j^k )^{j'} \right| - \frac{1}{n} \sum_{k \in g} \left| ((\widehat \bfE_{-j}^k)^T \widehat \bfE^k \bfT_{0,j}^k )^{j'} \right|\\
& \geq |g| \gamma_n - \sum_{k \in g} \BQ ( v_\beta, \Sigma_x^k, \Sigma_y^k )
\end{align*}
%
using the KKT condition for (\ref{eqn:EstEqn2}) and assumption (T2). The choice of $\gamma_n$ now ensures that the right hand side is $\geq 3|g| \gamma_n / 4$. Hence
%
\begin{align*}
| \hat \cS_j| & \leq \sum_{(j',g) \in \widehat \cS_j} \frac{16}{9 n^2 |g|^2 \gamma_n^2 } \sum_{k \in g} \left| ((\widehat \bfE_{-j}^k)^T \widehat \bfE^k ( \widehat \bfT_j^k - \bfT_{0,j}^k ))^{j'} \right|^2\\
& \leq \frac{16}{9 \gamma_n^2} \sum_{k=1}^K \frac{1}{n} \left\| (\widehat \bfE_{-j}^k)^T \widehat \bfE^k ( \widehat \bfT_j^k - \bfT_{0,j}^k ) \right\|^2 \\
& = \frac{16}{9 \gamma_n^2} \sum_{k=1}^K (\bfd_j^k)^T \widehat \bfS^k \bfd_j^k \\
& \leq \frac{8}{3 \gamma_n} \sum_{j \neq j', g \in \cG_y^{jj'}} \| \bfd_{jj'}^{[g]} \| \leq \frac{1}{\psi} 128 | g_{\max}| s_j 
\end{align*}
%
using (\ref{eqn:Thm1ProofProp2Eqn3}) and (\ref{eqn:Thm1ProofProp2Eqn2}).

{\it Step 2: Edge set selection.} We denote the selected edge set for the $k^\text{th}$ Y-network by $\hat E^k$. Denote its population version by $E_0^k$. Further, let
%
$$
\tilde \Omega_y^k = \diag (\Omega_y^k) + \Omega_{y, E_0^k \cap \hat E^k}^k
$$
%
With similar derivations to the proof of Corollary A.1 in \cite{MaMichailidis15}, The following two upper bounds can be established:
%
\begin{align}
| \hat E^k | \leq \frac{ 128 | g_{\max} | S }{\psi}\\
\frac{1}{K} \sum_{k=1}^K \| \tilde \Omega_y^k - \Omega_y^k \|_F \leq
\frac{12 c_0 \sqrt{| g_{\max}| S} \gamma_n} {\sqrt K \psi}
\end{align}
%
following which, taking $\gamma_n = 4 \sqrt q \BQ_{\max}$ and 
%
\begin{align}
\Lambda_{\min} ( \tilde \Omega_y^k) \geq d_0 - 12 \sqrt{| g_{\max}| S} \gamma_n / \psi > 0; \quad
\Lambda_{\max} ( \tilde \Omega_y^k) & \leq c_0 + 12 \sqrt{| g_{\max}| S} \gamma_n / \psi < \infty
\end{align}
%
with
%
$$
t_1 = {\colrbf tbd}
$$

{\it Step 3: Refitting.}

\end{proof}

%We now prove the norm consistency of $\widehat \Theta_i - \Theta_{0,i}$.
%
%\begin{Proposition}\label{prop:PropA1}
%Copy from Proposition A.1 in 15-656
%\begin{align}
%\sum_{j \neq i, g \in \cG^{ij}} \| \widehat \bftheta_{ij}^{[g]} - \bftheta_{0,ij}^{[g]} \| & \leq \\
%\cM ( \widehat \Theta_i ) & \leq \\
%\| \widehat \Theta_i - \widehat \Theta_{0,i} \|_F & \leq
%\end{align}
%\end{Proposition}
%
%\begin{proof}[Proof of Proposition~\ref{prop:PropA1}]
%%We first proceed in a similar fashion as the proof of Lemma 3.1 in \cite{LouniciEtal11}. For any $\bftheta_i^k \in \BR^q$, we have
%%%
%%$$
%%\sum_{k=1}^K \frac{1}{n} \| \bfY_i^k - \bfY_{-i}^k \widehat \bftheta_i^k - \bfX^k \widehat \bfB_i^k \|^2 + \sum_{j \neq i, g \in \cG^{ij}} \| \widehat \bftheta_{ij}^{[g]} \| \leq 
%%\sum_{k=1}^K \frac{1}{n} \| \bfY_i^k - \bfY_{-i}^k \bftheta_i^k - \bfX^k \widehat \bfB_i^k \|^2 + \sum_{j \neq i, g \in \cG^{ij}} \| \bftheta_{ij}^{[g]} \|
%%$$
%%%
%%subtracting and adding $\bfY_{-i}^k \bftheta_{0,i}^k$ inside the squared norms on both sides, then simolifying and writing $\widehat \bfepsilon_i^k = \bfY_i^k - \bfY_{-i}^k \bftheta_{0,i}^k - \bfX_i^k \widehat \bfB_i^k$ we get
%%%
%%\begin{align*}
%%\frac{1}{n} \| \bfY_{-i}^k ( \widehat \bftheta_i^k - \bftheta_{0,i}^k ) \|^2 & \leq
%%\| \bfY_{-i}^k ( \bftheta_i^k - \bftheta_{0,i}^k ) \|^2 + 
%%\frac{2}{n} ( \widehat \bfepsilon_i^k )^T \bfY_{-i}^k ( \widehat \bftheta_i^k - \bftheta_i^k )\\
%%& + 2 \sum_{j \neq i, g \in \cG^{ij}} \lambda_{ij}^g ( \| \bftheta_{ij}^{[g]} - \widehat \bftheta_{ij}^{[g]} \|)
%%\end{align*}
%The statement of this proposition is same as that of Proposition A.1 in 15-656, and can proved in a similar fashion. The only difference is a modified choice of $\lambda_{ij}^g$, which we obtain from Proposition~\ref{lemma:LemmaE2}.
%
%\end{proof}

\textit{Part II.} Proof of Thm 2 in 15-656 follows. We only need a new bound for $Var (\bfY_i^k | \bfY_{-i}^k, \bfX^k, \widehat \bfB_i^k)$. For this we have
%
$$ Var (\bfY_i^k | \bfY_{-i}^k, \bfX^k, \widehat \bfB_i^k) = \BE (\widehat \bfepsilon_i^k)^2
= \BE ( \bfepsilon_i^k + \bfdelta_i^k)^2
\leq \left( \frac{1}{d_0} + \frac{c(v_\beta)}{n} \right)^2
$$
%
applying cauchy-schwarz inequality followed by assumption (A2). Now Replace $1/\sqrt{n d_0}$ in choice of $\lambda, \alpha_n$ in Thm 2 statement with $1/\sqrt{n} (\sqrt{1/d_0} + \sqrt{c(v_\beta)/ n})$.

\end{proof}

%\begin{Proposition}
%Given fixed $\widehat \cB$, prediction errors follow bound in T2 with high enough probability.
%\end{Proposition}

\begin{Proposition}\label{prop:ErrorRE}
Consider deterministic $\widehat \cB$ satisfying assumption (T1). Then for sample size $n \succsim \log (pq)$ and $k \in \cI_K$,

\begin{enumerate}
%\item We have $\| \bfX^k ( \hat \bfB^k - \bfB_0^k ) \|_\infty \leq c( v_\beta)$, where
%%
%$$
%c(v_\beta) =\sqrt n v_\beta \left[ \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x^k n}} + \max_j \sigma_{x,jj}^k \right]^{1/2}; \quad
%c_x^k = \left[ 128 ( 1 + 4 \Lambda_{\max} (\Sigma_x^k)  )^2 \max_j (\sigma_{x,jj}^k)^2 \right]^{-1}
%$$
%%
%with probability $ \geq 1 - 1/p^{\tau_1-2}, \tau_1 > 2$.
%
\item $\widehat \bfS^k$ satisfies the RE condition: $ \widehat \bfS^k \sim RE (\psi^k, \phi^k)$, where 
%
$$
\psi^k = \frac{ \Lambda_{\min} (\Sigma_x^k)}{2}; \quad \phi^k = \frac{ \psi^k \log p}{n} + 2 v_\beta c_2 [ \Lambda_{\max} (\Sigma_x^k) \Lambda_{\max} (\Sigma_y^k) ]^{1/2} \sqrt{\frac{ \log(pq)}{n}}
$$
%
with probability $\geq 1 - 6c_1 \exp [-(c_2^2-1) \log(pq)] - 2 \exp (- c_3 n), c_1, c_3 > 0, c_2 > 1$.
%
\item The following deviation bound is satisfied for any $j \in \cI_q$
%
%$$
%\left\| \widehat \bfS^k \bfT_{0,j}^k \right\|_\infty \leq 4 v_\beta c_2 [ \Lambda_{\max} (\Sigma_x^k) \Lambda_{\max} (\Sigma_y^k) ]^{1/2} \sqrt{\frac{ \log(pq)}{n}} + 2 \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x^k n}} + 2 \max_j \sigma_{x,jj}^k
%$$
%
$$
\left\|\frac{1}{n} (\widehat \bfE_{-j}^k)^T \widehat \bfE^k \bfT_{0,j}^k \right\|_\infty \leq \BQ \left(v_\beta, \Sigma_x^k, \Sigma_y^k \right)
$$
%

with probability $\geq 1 - 1/p^{\tau_1-2} - 6c_1 \exp [-(c_2^2-1) \log(pq)] - 6c_4 \exp [-(c_5^2-1) \log(pq)], c_4 > 0, c_5 > 1$, where
%
\begin{align*}
\BQ \left(v_\beta, \Sigma_x^k, \Sigma_y^k \right) &=
2 v_\beta^2 \left[ \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x^k n}} + \max_i \sigma_{x,ii}^k \right] +\\
& 4 v_\beta c_2 [ \Lambda_{\max} (\Sigma_x^k) \Lambda_{\max} (\Sigma_y^k) ]^{1/2} \sqrt{\frac{ \log(pq)}{n}} +\\
& c_5 \left[ \Lambda_{\max} ( \Sigma_{y,-j}^k) ( \sigma_{y,jj}^k +
(\bftheta_{0,j}^k)^T \Sigma_{y,-j}^k \bftheta_{0,j}^k - 2 (\bfsigma_{y,j,-j}^k)^T \bftheta_{0,j}^k ) \right]^{1/2} \sqrt{\frac{\log(q-1)}{n}}
\end{align*}
%
\end{enumerate} 
\end{Proposition}

\begin{proof}[Proof of Proposition~\ref{prop:ErrorRE}]

%For any sub-gaussian zero-mean design matrix $\bfX \in \BM(n,p)$ with parameters $(\Sigma_x, \sigma_x^2)$, and any $\hat \bfB, \bfB_0 \in \BM(p,q)$ such that $\| \hat \bfB - \bfB_0 \|_F \leq v_\beta$, we follow the proof of Proposition 3 in \cite{LinEtal16} to obtain that for sample size
%%
%\begin{align}\label{eqn:ErrorREeqn1}
%n & \geq 512 ( 1 + 4 \sigma_x^2)^4 \max_j (\Sigma_{x,jj})^4 \log (4p^{\tau_1})
%\end{align}
%the following holds
%%
%\begin{align}\label{eqn:ErrorREeqn2}
%\left\| (\widehat \bfB - \bfB_0)^T \left( \frac{\bfX^T \bfX}{n} \right) (\widehat \bfB - \bfB_0) \right\|_\infty & \leq
%v_\beta^2 \left[ \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x n}} + \max_j \Sigma_{x,jj} \right]
%\end{align}
%%
%with probability $\geq 1 - 1/p^{\tau_1-2}$ for some $\tau_1>2$, where
%%
%$$
%c_x = \left[ 128 ( 1 + 4 \sigma_x^2)^2 \max_j (\Sigma_{x,jj})^2 \right]^{-1}
%$$
%%
%Here we substitute $\bfX, \hat \bfB, \bfB_0$ with $\bfX^k, \hat \bfB^k, \hat \bfB_0^k$ respectively. Since rows of $\bfX^k$ come independently from $\cN( {\bf 0}, \Sigma_x^k)$, $\sigma_x^2$ in our case is the spectral norm of $\Sigma_x^k$ \citep{LohWainwright12}, which is $\Lambda_{\max} (\Sigma_x^k)$. Finally
%%
%$$
%\| \bfX^k ( \hat \bfB^k - \bfB_0^k ) \|_\infty \leq 
%\sqrt{\left\| (\widehat \bfB^k - \bfB_0^k)^T (\bfX^k)^T \bfX^k (\widehat \bfB^k - \bfB_0^k) \right\|_\infty}
%$$
%%
%The expression of part 1 is immediate now, and (\ref{eqn:ErrorREeqn1}) ensures that part 1 holds when the leading term of the sample size requirement is $n \succsim \log (pq)$.

We drop the superscript $k$ since there is no scope of ambiguity. For part 1, we start with an auxiliary lemma:
%
\begin{Lemma}\label{lemma:ErrorRElemma1}
For a sub-gaussian design matrix $\bfX \in \BM(n,p)$ with columns having mean ${\bf 0}_p$ and covariance matrix $\Sigma_x$, the sample covariance matrix $\widehat \Sigma_x = \bfX^T \bfX/n$ satisfies the RE condition
%
$$
\widehat \Sigma_x \sim RE \left( \frac{\Lambda_{\min} ( \Sigma_x) }{2}, \frac{\Lambda_{\min} ( \Sigma_x) \log p }{2 n} \right)
$$
%
with probability $\geq 1 - 2 \exp(-c_3 n)$ for some $c_3 > 0$.
\end{Lemma}
%
This is same as Lemma 2 in Appendix B of \cite{LinEtal16} and its proof can be found there. Now denote $\widehat \bfE = \bfY - \bfX \widehat \bfB$. For $\bfv \in \BR^q$, we have
%
\begin{align}\label{eqn:ErrorREeqn3}
\bfv^T \widehat \bfS \bfv &= \frac{1}{n} \| \widehat \bfE \bfv \|^2 \notag\\
&= \frac{1}{n} \| (\bfE + \bfX ( \bfB_0 - \widehat \bfB ))\bfv \|^2 \notag\\
&= \bfv^T \bfS \bfv + \frac{1}{n} \| \bfX (\bfB_0 - \widehat \bfB) \bfv \|^2 + 2 \bfv^T (\bfB_0 - \widehat \bfB)^T \left( \frac {(\bfX)^T \bfE}{n} \right) \bfv
\end{align}
%
For the first summand, $ \bfv^T \bfS^k \bfv \geq \psi_y \| \bfv \|^2 - \phi_y \| \bfv \|_1^2$ with $\psi_y = \Lambda_{\min} (\Sigma_y)/2, \phi_y = \psi_y \log p/n$ by applying Lemma \ref{lemma:ErrorRElemma1} on $\bfS$. The second summand is greater than or equal to 0. For the third summand,
%
$$
2 \bfv^T (\bfB_0 - \widehat \bfB)^T \left( \frac {(\bfX)^T \bfE}{n} \right) \bfv \geq
-2 v_\beta \left\| \frac {(\bfX)^T \bfE}{n} \right\|_\infty \| \bfv \|_1^2
$$
%
by assumption (T1). Now we use another lemma:
%
\begin{Lemma}\label{lemma:ErrorRElemma2}
For zero-mean independent sub-gaussian matrices $\bfX \in \BM(n,p), \bfE \in \BM(n,q)$ with parameters $(\Sigma_x, \sigma_x^2)$ and $(\Sigma_e, \sigma_e^2)$ respectively, given that $n \succsim \log(pq)$ the following holds with probability $\geq 1 - 6c_1 \exp [-(c_2^2-1) \log(pq)]$ for some $c_1 >0, c_2 > 1$:
%
$$
\frac{1}{n} \| \bfX^T \bfE \|_\infty \leq c_2 [ \Lambda_{\max} (\Sigma_x) \Lambda_{\max} (\Sigma_e) ]^{1/2} \sqrt{\frac{ \log(pq)}{n}}
$$
%
\end{Lemma}
%
This is a part of Lemma 3 of Appendix B in \cite{LinEtal16}, and is proved therein. Subsequently we collect all summands in (\ref{eqn:ErrorREeqn3}) and get
%
$$
\bfv^T \widehat{ \bfS} \bfv \geq \psi_y \| \bfv \|^2 - \left( \phi_y + 2 v_\beta c_2 [ \Lambda_{\max} (\Sigma_x) \Lambda_{\max} (\Sigma_y) ]^{1/2} \sqrt{\frac{ \log(pq)}{n}} \right) \| \bfv \|_1^2
$$
with probability $\geq 1 - 2\exp(- c_3 n) - 6c_1 \exp [-(c_2^2-1) \log(pq)]$. This concludes the proof of part 1.

To prove part 2, we decompose the quantity in question:
%
\begin{align}\label{eqn:ErrorRElemma2maineqn}
\left\| \frac{1}{n} \widehat \bfE_{-j}^T \widehat \bfE \bfT_{0,j} \right\|_\infty &=
\left\| \frac{1}{n} \left[ \bfE_{-j} + \bfX (\bfB_{0,j} - \widehat \bfB_j) \right]^T \left[ \bfE + \bfX (\bfB_0 - \widehat \bfB) \right] \bfT_{0,j} \right\|_\infty \notag\\
& \leq \left\| \frac{1}{n} \bfE_{-j}^T \bfE \bfT_{0,j} \right\|_\infty +
\left\| \frac{1}{n} \bfE_{-j}^T \bfX (\bfB_0 - \widehat \bfB) \bfT_{0,j} \right\|_\infty \notag\\
& + \left\| \frac{1}{n} (\bfB_{0,j} - \widehat \bfB_j)^T \bfX^T \bfX (\bfB_0 - \widehat \bfB) \bfT_{0,j} \right\|_\infty +
\left\| \frac{1}{n} (\bfB_{0,j} - \widehat \bfB_j)^T \bfX^T \bfE \bfT_{0,j} \right\|_\infty \notag\\
&= \| \bfW_1 \|_\infty + \| \bfW_2 \|_\infty + \| \bfW_3 \|_\infty + \| \bfW_4 \|_\infty
\end{align}
%
Now
%
$$
\bfW_1 = \frac{1}{n} \bfE_{-j}^T ( \bfE_j - \bfE_{-j} \bftheta_{0,j})
$$
%
For node $j$ in the $y$-network, $\BE_{-j}$ and $E_j - \BE_{-j} \bftheta_{0,j}$ are the neighborhood regression coefficients and residuals, respectively. Thus they are orthogonal, so we can apply Lemma \ref{lemma:ErrorRElemma2} on $\bfE_{-j}$ and $\bfE_j - \bfE_{-j} \bftheta_{0,j}$ to obtain that for $n \succsim \log (q-1)$,
%
\begin{align}\label{eqn:ErrorRElemma2eqn5}
\| \bfW_1 \|_\infty & \leq c_5 \left[ \Lambda_{\max} ( \Sigma_{y,-j}) ( \sigma_{y,jj} + 
\bftheta_{0,j}^T \Sigma_{y,-j} \bftheta_{0,j} - 2 \bfsigma_{y,j,-j}^T \bftheta_{0,j} ) \right]^{1/2} \sqrt{\frac{\log(q-1)}{n}}
\end{align}
%
holds with probability $\geq 1 - 6c_4 \exp [-(c_5^2-1) \log(pq)]$ for some $c_4 > 0, c_5 > 1$.
%

The same bounds hold for $\bfW_2$ and $\bfW_4$:
%
\begin{align*}
\| \bfW_2 \|_\infty & \leq \left\| \frac{1}{n} \bfE_{-j}^T \bfX (\bfB_0 - \widehat \bfB) \right\|_\infty \| \bfT_{0,j} \|_1 \leq
\left\| \frac{1}{n} \bfE^T \bfX \right\|_\infty \| \bfB_0 - \widehat \bfB \|_1 \| \bfT_{0,j} \|_1\\
\| \bfW_4 \|_\infty & \leq \left\| \frac{1}{n} (\bfB_{0,j} - \widehat \bfB_j)^T \bfX^T \bfE \right\|_\infty \| \bfT_{0,j} \|_1 \leq
\left\| \frac{1}{n} \bfE^T \bfX \right\|_\infty \| \bfB_0 - \widehat \bfB \|_1 \| \bfT_{0,j} \|_1\\
\end{align*}
%
Now since $\Omega_y$ is diagonally dominant, $|\omega_{y,jj}| \geq \sum_{j \neq j'} |\omega_{y,jj'}|$ for any $j \in \cI_q$. Hence
%
$$
\| \bfT_{0,j} \|_1 = \sum_{j'=1}^q | T_{jj'} | = 1 + \sum_{j \neq j'} | \theta_{jj'} | = 1 + \frac{1}{\omega_{y,jj}} \sum_{j \neq j'} | \omega_{y,jj'} | \leq 2
$$
%
so that for $n \succsim \log (pq)$,
%
\begin{align}\label{eqn:ErrorRElemma2eqn6}
\| \bfW_2 \|_\infty + \| \bfW_4 \|_\infty  & \leq
4 v_\beta c_2 [ \Lambda_{\max} (\Sigma_x) \Lambda_{\max} (\Sigma_y) ]^{1/2} \sqrt{\frac{ \log(pq)}{n}}
\end{align}
%
with probability $\geq 1 - 6c_1 \exp [-(c_2^2-1) \log(pq)]$ by applying Lemma~\ref{lemma:ErrorRElemma2} and assumption (T1).

Finally for $\bfW_3$, we apply Lemma 8 of \cite{RavikumarEtal11} on the (sub-gaussian) design matrix $\bfX$ to obtain that for sample size
$$
n \geq 512 ( 1 + 4 \Lambda_{\max} (\Sigma_x^k))^4 \max_i (\sigma_{x,ii}^k )^4 \log (4p^{\tau_1})
$$
%
we get that with probability $ \geq 1 - 1/p^{\tau_1-2}, \tau_1 > 2$,
%
$$
\left\| \frac{\bfX^T \bfX}{n} \right\|_\infty \leq \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x n}} + \max_i \sigma_{x,ii}; \quad
c_x = \left[ 128 ( 1 + 4 \Lambda_{\max} (\Sigma_x)  )^2 \max_i (\sigma_{x,ii})^2 \right]^{-1}
$$
%
Thus with the same probability,
%
\begin{align}\label{eqn:ErrorRElemma2eqn4}
\| \bfW_4 \|_\infty \leq \left\| \frac{\bfX^T \bfX}{n} \right\|_\infty \| \widehat \bfB - \bfB_0 \|_1^2 \| \bfT_{0,j} \|_1 
\leq 2 v_\beta^2 \left[ \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x n}} + \max_i \sigma_{x,ii} \right]
\end{align}
%
We now bound the right hand side of (\ref{eqn:ErrorRElemma2maineqn}) using (\ref{eqn:ErrorRElemma2eqn5}), (\ref{eqn:ErrorRElemma2eqn6}) and (\ref{eqn:ErrorRElemma2eqn4}) to complete the proof, with the leading term of the sample size requirement being $n \succsim \log(pq)$.

%%
%\begin{align}
%\| \widehat \bfS^k \bfT_{0,j}^k \|_\infty & \leq
%\| \widehat \bfS^k \|_\infty \| \bfT_{0,j}^k  \|_1 \notag\\
%& \leq \left[ \| \widehat \bfS^k - \bfS^k \|_\infty + \| \bfS^k \|_\infty \right] \| \bfT_{0,j}^k  \|_1 \label{eqn:ErrorRElemma2eqn1}
%\end{align}
%%
%Since $\Sigma_y^k$ is diagonally dominant, (see proof of Proposition~\ref{lemma:LemmaE2}) we have
%%
%\begin{align}\label{eqn:ErrorRElemma2eqn2}
% \| \bfT_{0,j}^k \|_1 \leq 2
%\end{align}
%%
%Moving on to $\| \widehat \bfS^k - \bfS^k \|_\infty$:
%%
%\begin{align}
%\| \widehat \bfS^k - \bfS^k \|_\infty & \leq \left\| \frac{2}{n} (\bfE^k)^T \bfX^k (\bfB^k_0 - \widehat \bfB^k) \right\|_\infty + 
%\left\| (\widehat \bfB^k - \bfB_0^k)^T \left( \frac{(\bfX^k)^T (\bfX^k)}{n} \right) (\widehat \bfB^k) - \bfB_0^k)) \right\|_\infty \notag\\
%& \leq 2 \left\| \frac{(\bfX^k)^T \bfE^k}{n} \right\|_\infty \left\| \bfB^k_0 - \widehat \bfB^k) \right\|_1 + 
%\left\| (\widehat \bfB^k - \bfB_0^k)^T \left( \frac{(\bfX^k)^T (\bfX^k)}{n} \right) (\widehat \bfB^k) - \bfB_0^k)) \right\|_\infty \label{eqn:ErrorRElemma2eqn3}
%\end{align}
%%
%By applying Lemma~\ref{lemma:ErrorRElemma2} on $\bfX^k, \bfE^k$, and assumption (T1) we have for $n \succsim \log (pq)$ and with probability $\geq 1 - 6c_1 \exp [-(c_2^2-1) \log(pq)]$,
%%
%$$
%2 \left\| \frac{(\bfX^k)^T \bfE^k}{n} \right\|_\infty \left\| \bfB^k_0 - \widehat \bfB^k) \right\|_1 \leq
%2 v_\beta c_2 [ \Lambda_{\max} (\Sigma_x^k) \Lambda_{\max} (\Sigma_y^k) ]^{1/2} \sqrt{\frac{ \log(pq)}{n}}
%$$
%%
%The second term is bounded by substituting $\bfX, \hat \bfB, \bfB_0$ with $\bfX^k, \hat \bfB^k, \hat \bfB_0^k$ respectively in (\ref{eqn:ErrorREeqn2}).
%
%For a bound on the $\ell_\infty$-norm of $\bfS^k = (\bfX^k)^T \bfX^k/n$, we apply Lemma 8 of \cite{RavikumarEtal11} on the (sub-gaussian) design matrix $\bfX^k$ to obtain that for sample size
%$$
%n \geq 512 ( 1 + 4 \Lambda_{\max} (\Sigma_x^k))^4 \max_j (\sigma_{x,jj}^k )^4 \log (4p^{\tau_1})
%$$
%%
%we get that with probability $ \geq 1 - 1/p^{\tau_1-2}$,
%%
%\begin{align}\label{eqn:ErrorRElemma2eqn4}
%\| \bfS^k \|_\infty & \leq \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x^k n}} + \max_j \sigma_{x,jj}^k; \quad
%c_x^k = \left[ 128 ( 1 + 4 \Lambda_{\max} (\Sigma_x^k)  )^2 \max_j (\sigma_{x,jj}^k)^2 \right]^{-1}
%\end{align}
%%
%We now bound the right hand side of (\ref{eqn:ErrorRElemma2eqn1}) using (\ref{eqn:ErrorRElemma2eqn2}), (\ref{eqn:ErrorRElemma2eqn3}) and (\ref{eqn:ErrorRElemma2eqn4}), which proves part 3.
\end{proof}

%-----------------------------
\hrulefill
%-----------------------------

Now concentrate on the $k$-population estimation problem. We want to obtain
%
$$
\widehat \bfbeta = \argmin_{\bfbeta \in \BR^{pq}} \left\{-2 \bfbeta^T \widehat \bfgamma + \bfbeta^T \widehat \bfGamma \bfbeta + \lambda_n \sum_{g \in \cG} \| \bfbeta^{[g]}  \| \right\}
$$
%with
%$$
%\bfbeta = \begin{bmatrix}
%\ve (\bfB^1)\\
%\vdots\\
%\ve (\bfB^K)\\
%\end{bmatrix}; \quad
%\bfGamma = \begin{bmatrix}
%I_q \otimes (\bfX^1) TX^1 / n) & &\\
%& \ddots &\\
%& & I_q \otimes (\bfX^K)^T X^K / n)
%\end{bmatrix} 
%$$
\begin{Theorem}\label{thm:BetaThm}
Assume fixed $\cX, \cE$, and deterministic $\widehat \Theta = \{ \widehat \Theta_j \}$. Also for $j \in \cI_q$,

\noindent{\bf(B1)} $\| \widehat \Theta_j - \Theta_{0,j} \|_F \leq v_\Theta \sqrt{\frac{\log q}{n}}$ for some $v_\Theta$ dependent on $\Theta$.

\noindent{\bf(B2)} Denote $\widehat \bfGamma^k = (\widehat \bfT^k)^2 \otimes (\bfX^k)^T \bfX^k/n, \widehat \bfgamma^k = (\widehat \bfT^k)^2 \otimes (\bfX^k)^T \bfY^k/n$. Then the deviation bound holds:
%
$$
\left\| \widehat \bfgamma^k - \widehat \bfGamma^k \bfbeta_0 \right\|_\infty \leq \BR( v_\Theta, \Sigma_x^k, \Sigma_y^k)
$$
%
{\colrbf tbd}

\noindent{\bf(B3)} $\widehat \bfGamma \sim RE(\psi_*, \phi_*)$ with $Kpq \phi_* \leq \psi_*/2$.

Then, given the choice of tuning parameter
%
$$
\lambda \geq 4 \sqrt{pq} \BR_{\max}; \quad \BR_{\max} := \max_{k \in \cI_K} \BR \left(v_\Theta, \Sigma_x^k, \Sigma_y^k \right)
$$
%
the following holds
%
\begin{align}
\| \widehat \bfbeta - \bfbeta_0 \| & \leq 12 \sqrt s_\beta \lambda_n / \psi^* \label{eqn:BetaThmEqn1}\\
\sum_{g \in \cG} \| \bfbeta^{[g]} - \bfbeta_0^{[g]} \| & \leq 48 s_\beta \lambda_n / \psi^* \label{eqn:BetaThmEqn2}\\
(\widehat \bfbeta - \bfbeta_0 )^T \widehat \bfGamma (\widehat \bfbeta - \bfbeta_0 ) & \leq
72 s_\beta \lambda_n^2 / \psi^* \label{eqn:BetaThmEqn3}
\end{align}
%
%Also denote the non-zero support of $\widehat \bfbeta$ by $\widehat \cS_\beta$, i.e. $\widehat \cS_\beta = \{ g: \hat \bfbeta^{[g]} \neq {\bf 0} \}$. Then
%%
%\begin{align}\label{eqn:BetaThmEqn4}
%| \widehat \cS_\beta| \leq 128 s_\beta / \psi^*
%\end{align}
\end{Theorem}

\begin{proof}[Proof of Theorem~\ref{thm:BetaThm}]
The proof follows that of Theorem~\ref{thm:ThetaThm}, with a different group norm structure. We only point our the differences.

Putting $\bfbeta = \bfbeta_0$ in (\ref{eqn:EstEqn1}) we get
%
$$
-2 \widehat \bfbeta^T \widehat \bfgamma + \bfbeta^T \widehat \bfGamma \widehat \bfbeta + \lambda_n \sum_{g \in \cG} \| \widehat \bfbeta^{[g]}  \| \leq
-2 \bfbeta_0^T \widehat \bfgamma + \bfbeta_0^T \widehat \bfGamma \bfbeta_0 + \lambda_n \sum_{g \in \cG} \| \bfbeta_0^{[g]}  \|
$$
%
Denote $\bfb = \widehat \bfbeta - \bfbeta_0$. Then we have
%
$$
\bfb^T \widehat \bfGamma \bfb \leq 2 \bfb^T ( \widehat \bfgamma - \widehat \bfGamma \bfbeta_0 ) + \lambda_n
\sum_{g \in \cG} ( \| \bfbeta_0^{[g]} \| - \| \bfbeta_0^{[g]} + \bfb^{[g]} \|)
$$
%
Proceeding similarly as the proof of Theorem~\ref{thm:ThetaThm}, with a different deviation bound and choice of $\lambda_n$ based on that, we get expressions equivalent to (\ref{eqn:Thm1ProofProp2Eqn3}) and (\ref{eqn:Thm1ProofProp2Eqn2}) respectively:
%
\begin{align}
\bfb^T \widehat \bfGamma \bfb & \leq \frac{3}{2} \sum_{g \in \cG} \| \bfb^{[g]} \| \\
\frac{\psi^*}{3} \| \bfb \|^2 & \leq \lambda_n \sum_{g \in \cG} \| \bfb^{[g]} \| \leq 4 \lambda_n \sqrt{s_\beta} \| \bfb \|
\end{align}
%
The bounds in (\ref{eqn:BetaThmEqn1}), (\ref{eqn:BetaThmEqn2}), (\ref{eqn:BetaThmEqn3}) %and (\ref{eqn:BetaThmEqn4}) 
follow.

\end{proof}

\begin{Proposition}\label{prop:ThmBetaRE}
Consider deterministic $\widehat \Theta$ satisfying assumption (B1). Then for sample size $n \succsim \log (pq)$,

\begin{enumerate}
%\item We have $\| \bfX^k ( \hat \bfB^k - \bfB_0^k ) \|_\infty \leq c( v_\beta)$, where
%%
%$$
%c(v_\beta) =\sqrt n v_\beta \left[ \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x^k n}} + \max_j \sigma_{x,jj}^k \right]^{1/2}; \quad
%c_x^k = \left[ 128 ( 1 + 4 \Lambda_{\max} (\Sigma_x^k)  )^2 \max_j (\sigma_{x,jj}^k)^2 \right]^{-1}
%$$
%%
%with probability $ \geq 1 - 1/p^{\tau_1-2}, \tau_1 > 2$.
%
\item %For sample size $n \succsim \max (s_\beta \log p, d^2 \log q)$,
$\widehat \bfGamma$ satisfies the RE condition: $ \widehat \bfGamma \sim RE (\psi_*, \phi_*)$, where 
%
$$
\psi_* = \min_k \psi^k \left( \min_i \psi_t^i - d v_\Theta \right), 
\phi_* = \max_k \phi^k \left( \min_i \phi_t^i + d v_\Theta \right)
$$
%
with probability $\geq 1 - 2 \exp(c_3 n), c_3>0$.
%
\item The following deviation bound is satisfied:
%
%$$
%\left\| \widehat \bfS^k \bfT_{0,j}^k \right\|_\infty \leq 4 v_\beta c_2 [ \Lambda_{\max} (\Sigma_x^k) \Lambda_{\max} (\Sigma_y^k) ]^{1/2} \sqrt{\frac{ \log(pq)}{n}} + 2 \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x^k n}} + 2 \max_j \sigma_{x,jj}^k
%$$
%
$$
\left\| \widehat \bfgamma - \widehat \bfGamma \bfbeta_0 \right\|_\infty \leq \BR \left(v_\beta, \Sigma_x^k, \Sigma_y^k \right) \sqrt{ \frac{ \log(pq)}{n}}
$$
%
with probability $1 - 12 c_1 \exp[ (c_2^2-1) \log (pq)], c_1>0, c_2>1$, where
$$
\BR \left(v_\beta, \Sigma_x^k, \Sigma_y^k \right) = c_2 \sqrt{\Lambda_{\max} (\Sigma_x^k)} \left( d v_\Theta \Lambda_{\min} (\Sigma_y^k) +
\frac{1}{\Lambda_{\min} (\Sigma_y^k) } \right)
$$
\end{enumerate} 
\end{Proposition}

\begin{proof}[Proof of Proposition~\ref{prop:ThmBetaRE}]
For part 1 it is enough to prove that with $ \widehat \Sigma_x^k := (\bfX^k)^T \bfX^k/n$,
%
\begin{align}\label{eqn:ThmBetaREProofEqn1}
\widehat \bfT_k^2 \otimes \widehat \Sigma_x^k & \sim RE (\psi_*^k, \phi_*^k)
\end{align}
%
with high enough probability. because then we can take $\psi_* = \min_k \psi_*^k, \phi_* = \max_k \phi_*^k$. The proof of (\ref{eqn:ThmBetaREProofEqn1}) follows similar lines of the proof of Proposition 1 in \cite{LinEtal16}, only replacing $\Theta_\epsilon, \widehat \Theta_\epsilon, \bfX$ therein with $(\bfT^k)^2, (\widehat \bfT^k)^2, \bfX^k$, respectively. We omit the details.%, with an extra step to prove that $(\bfT^k)^2$ is diagonally dominant. For this, denote the elements of $(\bfT^k)^2$ by $\tau_{jj'}, j; j' \in \cI_q$. Then
%%
%\begin{align*}
%\tau_{jj} - \sum_{j \neq j'} | \tau_{jj'}| & = \sum_{l=1}^q \left[
%\frac{\omega_{jl}^2}{\omega_{jj}^2 } - \sum_{j \neq j'} \frac{| \omega_{jl} \omega_{j'l} |}{\omega_{jj} \omega_{j'j'} } \right]
%\end{align*}
%%
%For each summand,
%%
%\begin{align*}
%\frac{\omega_{jl}^2}{\omega_{jj}^2 } - \sum_{j \neq j'} \frac{| \omega_{jl} \omega_{j'l} |}{\omega_{jj} \omega_{j'j'}} & = \frac{| \omega_{jl} |}{\omega_{jj} } \left[ \frac{| \omega_{jl}| }{\omega_{jj} } - \sum_{j' \neq j} \frac{| \omega_{j'l} |}{\omega_{j'j'}} \right] \\
%\end{align*}
%

\end{proof}

%-----------------------------
\hrulefill
%-----------------------------

\section{Two-sample testing}
Suppose there are two disease subtypes: $k = 1,2$, and we are interested in testing whether the downstream effect of a predictor in X-data is same across both subtypes, i.e. if $\bfb_i^1 = \bfb_i^2$ for some $i \in \cI_p$. For this we consider the modified optimization problem:
%
\begin{align}
& \min_{\cB, \Theta} \frac{1}{n} \left\{ \sum_{j=1}^q \sum_{k=1}^2 \| \bfY_j^k - \bfY_{-j}^k \bftheta_j^k - \bfX^k \bfB_{j}^k \|^2 + \sum_{j \neq j'} \lambda_{jj'} \| \bftheta_{jj'}^* \| + \sum_{i=1}^p \eta_i \| \bfB_{i*}^* \| \right\} \notag\\
&= \min \left\{ f ( \cY, \cX, \cB, \Theta) + P (\Theta) + Q (\cB) \right\} 
\end{align}
%
with $n_1 = n_2 = n$ for simplicity; and $\bfB^k = (\bfb_1^k, \ldots, \bfb_q^k), (\bfB_{i*}^*) \in \BR^{ q \times K}$.

In this setup, define the desparsified estimate of $\bfb_i^k$ as
%
\begin{align}\label{eqn:DebiasedBeta}
\widehat \bfc_i^k = \widehat \bfb_i^k + \frac{1}{n t_i^k} \left( \bfX_i^k - \bfX_{-i}^k \widehat \bfzeta_i^k \right)^T
(\bfY^k - \bfX^k \widehat \bfB^k )
\end{align}
%
for $k = 1,2$, where $t_i^k = ( \bfX_i^k - \bfX_{-i}^k \widehat \bfzeta_i^k )^T \bfX_{-i}^k/n$. Then we have the asymptotic joint distribution of a scaled version of the debiased coefficients for the $i\Th$ predictor effect.

\begin{Theorem}\label{Thm:ThmTesting}
Define $\widehat s_i^k = \sqrt{\| \bfX_i^k - \bfX_{-i}^k \widehat \bfzeta_i^k \|^2/n}$, and $m_i^k = \sqrt n t_i^k / \widehat s_i^k$. Assume the following:

\noindent {\bf (C1)} For the X-neighborhood estimators we have $\| \widehat \bfzeta^k - \bfzeta_0^k \|_1 \leq v_\zeta$.

\noindent {\bf (C2)} The precision matrix estimators satisfy
%
$$
\left\| \widehat (\Omega_y^k)^{1/2} - (\Omega_y^k)^{1/2} \right\|_\infty \leq v_\Omega
$$
%
Then for the debiased estimators in \eqref{eqn:DebiasedBeta} we have
%
\begin{align}\label{eqn:ThmTestingEqn}
\begin{bmatrix}
\widehat \Omega_y^1 &\\
& \widehat \Omega_y^2
\end{bmatrix}^{1/2}
%
\begin{bmatrix}
m_i^1 (\widehat \bfc_i^1 - \bfb_i^1) &\\
&  m_i^2 (\widehat \bfc_i^2 - \bfb_i^2)
\end{bmatrix}
\sim \cN_{2q} ({\bf 0}, \bfI) + \bfS_n
\end{align}
%
where
%
$$
\| \bfS_n \|_\infty \leq {\colrbf tbd}
$$
%
with probability $\geq {\colrbf tbd}$.
\end{Theorem}
%

\begin{proof}[Proof of Theorem~\ref{Thm:ThmTesting}]
Let us define the following:
%
\begin{align*}
\bfM_i &= \diag(m^1_i, m^2_i)\\
\widehat \bfC_i &= \diag(\widehat \bfc^1_i, \widehat \bfc^2_i)\\
\widehat \bfD_i &= \diag(\widehat \bfb^1_i, \widehat \bfb^2_i)\\
\bfD_i &= \diag(\bfb^1_{0,i}, \bfb^2_{0,i})\\
\bfR_i^k &= \bfX_i^k - \bfX_{-i}^k \widehat \bfzeta_i^k; k = 1,2
\end{align*}
%
Then from \eqref{eqn:DebiasedBeta} we have
%
\begin{align}\label{eqn:ThmTestingProofeq1}
\bfM_i ( \widehat \bfC_i - \widehat \bfD_i )^T &= \frac{1}{\sqrt n}
\begin{bmatrix}
\frac{1}{\widehat s^1_i} (\bfR^1_i)^T \widehat \bfE^1 \\
\frac{1}{\widehat s^2_i} (\bfR^2_i)^T \widehat \bfE^2
\end{bmatrix}
\end{align}
%
We now decompose $\widehat \bfE^k:$
%
\begin{align*}
\widehat \bfE^k &= \bfY^k - \bfX^k \widehat \bfB^k \\
&= \bfE^k + \bfX^k (\bfB_0^k - \widehat \bfB^k)\\
&= \bfE^k + \bfX_i^k (\bfb_{0,i}^k - \widehat \bfb_i^k) + \bfX_{-i}^k (\bfB_{0,-i}^k - \widehat \bfB_{-i}^k)
\end{align*}
%
Putting them back in \eqref{eqn:ThmTestingProofeq1} and using $t^k = (\bfR^k)^T \bfX^k/n$,
%
\begin{align}\label{eqn:ThmTestingProofeq2}
\bfM_i ( \widehat \bfC_i - \widehat \bfD_i)^T &= \frac{1}{\sqrt n}
\begin{bmatrix}
\frac{1}{\widehat s^1_i} (\bfR^1_i)^T \bfE^1 \\
\frac{1}{\widehat s^2_i} (\bfR^2_i)^T \bfE^2
\end{bmatrix} +
\bfM_i (\bfD_i - \widehat \bfD_i )^T +
\frac{1}{\sqrt n}
\begin{bmatrix}
\frac{1}{\widehat s^1_i} (\bfR^1_i)^T \bfX_{-i}^1 (\bfB_{0,-i}^1 - \widehat \bfB_{-i}^1) \\
\frac{1}{\widehat s^2_i} (\bfR^2_i)^T \bfX_{-i}^2 (\bfB_{0,-i}^2 - \widehat \bfB_{-i}^2)
\end{bmatrix}\\
\Rightarrow
\bfM_i ( \widehat \bfC_i - \bfD_i)^T &= \frac{1}{\sqrt n}
\begin{bmatrix}
\frac{1}{\widehat s^1} (\bfR^1_i)^T \bfE^1 \\
\frac{1}{\widehat s^2} (\bfR^2_i)^T \bfE^2
\end{bmatrix} +
\frac{1}{\sqrt n}
\begin{bmatrix}
\frac{1}{\widehat s^1_i} (\bfR^1_i)^T \bfX_{-i}^1 (\bfB_{0,-i}^1 - \widehat \bfB_{-i}^1) \\
\frac{1}{\widehat s^2_i} (\bfR^2_i)^T \bfX_{-i}^2 (\bfB_{0,-i}^2 - \widehat \bfB_{-i}^2)
\end{bmatrix}
\end{align}

At this point, to prove \eqref{eqn:ThmTestingEqn}, it is enough to show the following:
%
\begin{align}
\frac{1}{\sqrt n \widehat s_i}  \widehat \Omega_y^{1/2} \bfE^T \bfR_i \sim
\cN_q ({\bf 0}, \bfI) + \bfS_{1n}; \quad 
\| \bfS_{1n} \|_\infty \leq {\colrbf tbd} \label{eqn:ThmTestingProofeq3}\\
\left\| \frac{1}{\sqrt n \widehat s_i} \bfR_i^T \bfX_{-i} (\bfB_{0,-i} - \widehat \bfB_{-i})
\widehat \Omega_y^{1/2} \right\|_\infty \leq {\colrbf tbd} \label{eqn:ThmTestingProofeq4}
\end{align}
%
dropping $k$ in the superscripts. To show \eqref{eqn:ThmTestingProofeq3} we have
%
\begin{align}\label{eqn:ThmTestingProofeq3}
\frac{1}{\sqrt n \widehat s_i}  \widehat \Omega_y^{1/2} \bfE^T \bfR_i =
\frac{1}{\sqrt n \widehat s_i}  (\widehat \Omega_y^{1/2} - \Omega_y^{1/2}) \bfE^T \bfR_i +
\frac{1}{\sqrt n \widehat s_i}  \Omega_y^{-1/2} \bfE^T \bfR_i
\end{align}
%
The second summand is distributed as $\cN_q ({\bf 0}, \bfI)$. For the first summand,
%
\begin{align}\label{eqn:ThmTestingProofeq31}
\frac{1}{\sqrt n}  \left\| (\widehat \Omega_y^{1/2} - \Omega_y^{1/2}) \bfE^T \bfR_i \right\|_\infty & \leq
\frac{1}{\sqrt n}  \left\| \widehat \Omega_y^{1/2} - \Omega_y^{1/2} \right\|_\infty  \left\| \bfE^T \bfR_i \right\|_\infty  \notag\\
& \leq \sqrt n v_\Omega \frac{1}{n} \left[ \| \bfE^T (\bfX_i -  \bfX_{-i} \bfzeta_i ) \|_\infty + \| \bfE^T \bfX_{-i} (\widehat \bfzeta_i - \bfzeta_{0,i} ) \|_\infty \right] \notag\\
& \leq \sqrt n v_\Omega \left[ \frac{1}{n} \| \bfE^T (\bfX_i -  \bfX_{-i} \bfzeta_i ) \|_\infty + 
\frac{v_\zeta^2}{n} \| \bfE^T \bfX_{-i} \|_\infty \right]
\end{align}
%
Applying Lemma~\ref{lemma:ErrorRElemma2} twice we have for $n \succsim \log(pq)$,
%
\begin{align}
\frac{1}{n} \| \bfE^T (\bfX_i -  \bfX_{-i} \bfzeta_i ) \|_\infty & \leq 
c_7 [ \sigma_{x,i,-i} \Lambda_{\max} (\Sigma_e) ]^{1/2} \sqrt{\frac{ \log q}{n}}\label{eqn:ThmTestingProofeq32}\\
\frac{1}{n} \| \bfE^T \bfX_{-i} \|_\infty  & \leq 
c_9 [ \Lambda_{\max} (\Sigma_{x,-i}) \Lambda_{\max} (\Sigma_e) ]^{1/2} \sqrt{\frac{ \log((p-1)q)}{n}}\label{eqn:ThmTestingProofeq33}
\end{align}
%
with probability $\geq 1 - 6c_6 \exp [-(c_7^2-1) \log q] - 6c_8 \exp [-(c_9^2-1) \log((p-1)q)]$ for some $c_6, c_8 >0, c_7, c_9 > 1$.

On the other hand
%
\begin{align}\label{eqn:ThmTestingProofeq34}
s_i := \frac{1}{n} \left\| \bfX_i - \bfX_{-i} \widehat \bfzeta_i \right\|^2 & \leq 
\widehat s_i + \frac{1}{n} \left\| \bfX_{-i} (\widehat \bfzeta_i - \bfzeta_{0,i} ) \right\|^2 \notag\\
& \leq s_i + v_\zeta^2 \left[ \sqrt{ \frac{ \log 4 + \tau_{1 i} \log (p-1)}{c_{x i} n}} + \max_{i' \neq i} \sigma_{x,i'i'} \right]
\end{align}
%
with probability $ \geq 1 - 1/p^{\tau_{1 i}-2}, \tau_{1 i} > 2$, and
%
\begin{align}\label{eqn:ThmTestingProofeq5}
n \geq 512 ( 1 + 4 \Lambda_{\max} (\Sigma_{x,-i}^k))^4 \max_{i' \neq i} (\sigma_{x,i'i'}^k )^4 \log (4(p-1)^{\tau_{1 i}})
\end{align}
%
where
$$
c_x = \left[ 128 ( 1 + 4 \Lambda_{\max} (\Sigma_{x,-i})  )^2 \max_{i' \neq i} (\sigma_{x,i'i'}^k )^2 \right]^{-1}
$$
%
by applying Lemma 8 of \cite{RavikumarEtal11} on $\bfX_{-i}^T \bfX_{-i}/n$.

Denote the second term in the right hand side of \eqref{eqn:ThmTestingProofeq34} by $V_i$. Then, for $n$ satisfying \eqref{eqn:ThmTestingProofeq5} and $s_i > V_i$, we get the bound:
%
\begin{align}\label{eqn:ThmTestingProofeq6}
\frac{1}{\widehat s_i} \leq \frac{1}{s_i - V_i}
\end{align}
%
Combining \eqref{eqn:ThmTestingProofeq31}, \eqref{eqn:ThmTestingProofeq32}, \eqref{eqn:ThmTestingProofeq33} and \eqref{eqn:ThmTestingProofeq6} gives the upper bound for the first summand of \eqref{eqn:ThmTestingProofeq3} that holds with probability larger than or equal to
%
$$
1 - 6c_6 e^{-(c_7^2-1) \log q} - 6c_8 e^{-(c_9^2-1) \log((p-1)q)} - \frac{1}{p^{\tau_{1 i}-2}}
$$
%
for some $c_6, c_8 >0, c_7, c_9 > 1, \tau_{1 i} > 2$.

To prove \eqref{eqn:ThmTestingProofeq4} we have
%
$$
\frac{1}{n} \| \bfR_i^T \bfX_{-i} \|_\infty \leq \frac{1}{n} \| (\bfX_i - \bfX_{-i} \bfzeta_{0,i})^T \bfX_{-i} \|_\infty +
\frac{1}{n} \| \bfX_{-i}^T \bfX_{-i} ( \widehat \bfzeta_i - \bfzeta_{0,i}) \|_\infty 
$$

\end{proof}
\bibliographystyle{apalike}
%\bibliographystyle{imsart-nameyear}
\bibliography{GGMbib}
\end{document}