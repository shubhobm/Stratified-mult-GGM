\documentclass[fleqn,11pt]{article}
%\documentclass[aoas,preprint]{imsart}

\usepackage{mycommands1,amssymb,amsmath,amsthm,color,pagesize,outlines,cite,subfigure}
\usepackage[small]{caption}
\usepackage[pdftex]{epsfig}
\usepackage{hyperref} % for linking references
\hypersetup{colorlinks = true, citecolor = blue, urlcolor = blue}

\usepackage{stackrel}

\usepackage[round]{natbib}

% for algorithm
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}

%\addtolength{\evensidemargin}{-.5in}
%\addtolength{\oddsidemargin}{-.5in}
%\addtolength{\textwidth}{0.9in}
%\addtolength{\textheight}{0.9in}
%\addtolength{\topmargin}{-.4in}

%% measurements for 1 inch margin
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}
\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\usepackage{setspace}
\doublespacing

%\pagestyle{myheadings}
%\markboth{}{\underline{{\bf Notes: (do not circulate)} \hspace{4.5cm} {\sc  Ansu Chatterjee} \hspace{0.25cm}}}

\DeclareMathOperator*{\ve}{vec}
\DeclareMathOperator*{\diag}{diag }
\DeclareMathOperator*{\Tr}{Tr}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

%% Appendix theorem counter
\usepackage{chngcntr}
\usepackage{apptools}
\AtAppendix{\counterwithin{Theorem}{section}}
\numberwithin{equation}{section}

\begin{document}

\newtheorem{Theorem}{Theorem}[section]
\newtheorem{Lemma}[Theorem]{Lemma}
\newtheorem{Corollary}[Theorem]{Corollary}
\newtheorem{Proposition}[Theorem]{Proposition}
\newtheorem{Conjecture}[Theorem]{Conjecture}
\theoremstyle{definition} \newtheorem{Definition}[Theorem]{Definition}

\title{Two-sample testing in data integration}
\date{}
\author{Subhabrata Majumdar}
\maketitle

\noindent\textbf{Abstract}: 
\vspace{.5cm}

\noindent\textbf{Keywords}: 

\newpage

\section{Model}
We have data $\cZ = \{ \cZ^1, \ldots, \cZ^K \}; \cZ^k = (\bfY^k, \bfX^k)$ where $\bfY^k \in \BR^{n \times q}, \bfX^k \in \BR^{n \times p}$ for $1 \leq k \leq K$.

\begin{eqnarray}
\bfX^k = (\bfX^k_1, \ldots, \bfX^k_p)^T \sim \cN (0, \Sigma^k_x)\\
\bfY^k = \bfX^k \bfB^k + \bfE^k; \quad \bfE^k = (\bfE^k_1, \ldots, \bfE^k_p)^T \sim \cN (0, \Sigma^k_y)\\
\Omega^k_x = (\Sigma^k_x)^{-1}; \quad \Omega^k_y = (\Sigma^k_y)^{-1}
\end{eqnarray}

Want to estimate $\{ (\Omega^k_x, \Omega^k_y, \bfB^k); 1 \leq k \leq K \}$ in presence of known grouping structures $\cG_x, \cG_y, \cH$ respectively. 

\paragraph{Notation:} Denote 3-dimensional array objects as elements of $\BT(a,b,c)$, the set of all $a \times b \times c$ tensors.
Define $\cS^x = (\Omega^k_x), \cS^y = (\Omega^k_y), \cB = (\bfB^k)$


Estimation of $\{ \Omega_x^k \}$ done using JSEM. For the other part, we use the following two-step procedure:

\begin{enumerate}
\item Run neighborhood selection on $y$-network incorporating effects of $x$-data and an additional blockwise group penalty:
%
\begin{align}
& \min_{\cB, \Theta} \left\{ \sum_{i=1}^p  \frac{1}{n_k} \left[ \sum_{k=1}^K \| \bfY^k_i - \bfY_{-i}^k \bftheta_i^k - \bfX^k \bfB_i^k \|^2 + 2 \sum_{j \neq i} \sum_{g \in \cG_y^{ij}} \lambda_{ij}^g \| \bftheta_{ij}^{[g]} \| \right] + 2 \sum_{b \in \cG_x \times \cG_y \times \cH} \eta^b \| \bfB^{[b]} \| \right\}\\
&= \min \left\{ f ( \cY, \cX, \cB, \Theta) + P (\Theta) + Q (\cB) \right\} 
\end{align}
%
where $\Theta = \{ \Theta_i \}, \cB = \{ \bfB^k \}, \cY = \{ \bfY^k \}, \cX = \{ \bfX^k \}, \cE = \{ \bfE^k \}$.

This estimates $\cB$ { \colrbf (possibly refit and/or within-group threshold) }.

\item Step I part 2 and step II of JSEM (see 15-656 pg 6) follows to estimate $\{ \Omega_y^k \}$.
\end{enumerate}

The objective function is bi-convex, so we are going to do the following in step 1-

\begin{itemize}
\item Start with initial estimates of $\cB$ and $\Theta$, say $\cB^{(0)}, \Theta^{(0)}$.
\item Iterate:
%
\begin{align}
\Theta^{(t+1)} &= \argmin \left\{ f ( \cY, \cX, \cB^{(t)}, \Theta^{(t)}) + P (\Theta^{(t)}) \right\}\\
\cB^{(t+1)} &= \argmin \left\{ f ( \cY, \cX, \cB^{(t)}, \Theta^{(t+1)}) + Q (\cB^{(t)}) \right\}
\end{align}
\item Continue till convergence.
\end{itemize}
%

\section{Two-sample testing}
Suppose there are two disease subtypes: $k = 1,2$, and we are interested in testing whether the downstream effect of a predictor is X-data is same across both subtypes, i.e. if $\bfb_i^1 = \bfb_i^2$ for some $i \in \{ 1, \ldots, p \}$. For this we consider the modified optimization problem:
%
\begin{align}
& \min_{\cB, \Theta} \frac{1}{n} \left\{ \sum_{j=1}^q \sum_{k=1}^2 \| \bfY_j^k - \bfY_{-j}^k \bftheta_j^k - \bfX^k \bfb_{j}^k \|^2 + \sum_{j \neq j'} \lambda_{jj'} \| \bftheta_{jj'}^* \| + \sum_{i=1}^p \eta_i \| \bfB_{i*}^* \| \right\}\\
&= \min \left\{ f ( \cY, \cX, \cB, \Theta) + P (\Theta) + Q (\cB) \right\} 
\end{align}
%
with $n_1 = n_2 = n$ for simplicity; and $\bfB^k = (\bfb_1^k, \ldots, \bfb_q^k), (\bfB_{i*}^*) \in \BR^{ q \times K}$

\section{Conditions}
Conditions A1, A2, A3 from JSEM paper.

\section{Results}
Define
%
\begin{align}
\hat \Theta^i &= \argmin_{\Theta_i} \left\{ \frac{1}{n_k} \sum_{k=1}^K \| \bfY^k_i - \bfY_{-i}^k \bftheta_i^k - \bfX^k \hat \bfB_i^k \|^2 + 2 \sum_{j \neq i} \sum_{g \in \cG_y^{ij}} \lambda_{ij}^g \| \bftheta_{ij}^{[g]} \| \right\}
\end{align}

\begin{Theorem}\label{thm:ThetaThm}
Assume fixed $\cX, \cE$ and deterministic $\hat \cB = \{ \bfB^k \}$. Also

\noindent{\bf(T1)} $\| \hat \bfB_i^k - \bfB_i^k \| \leq v_\beta$;

\noindent{\bf(T2)} $\| \bfX^k (\hat \bfB_i^k - \bfB_i^k ) \| \leq c(v_\beta)$ for some non-negative function $c(.)$;

Group uniform IC.

Then

\noindent (I) Estimation consistency

\noindent (II) Direction consistency 
\end{Theorem}

\begin{proof}[Proof of Theorem~\ref{thm:ThetaThm}]


\textit{Part I.} Follows proof of thm 1 in 15-656. The proof has 3 parts: consistency of neighborhood regression, selection of edge sets, and finally the refitting step.

For any $g \in \cG^{ij}, k \in g$, and $j \neq i$, let
%
$$
\hat \bfepsilon_i^k = \bfY_i^k - \bfY_{-i}^k \bftheta_{0,i}^k - \bfX^k \hat \bfB_i^k; \quad
\hat \zeta_{ij}^k = \frac{(\hat \bfepsilon_i^k)^T \bfY_j^k}{n}; \quad
\hat \bfzeta_{ij}^{[g]} = (\hat \zeta_{ij}^k)_{k \in g}
$$
%
Consider the random event $\cA = \bigcap_{i, j\neq i, g} \cA_{ij}^g$ with $\cA_{ij}^g = \{ 2 \| \hat \bfzeta_{ij}^{[g]} \| \leq \lambda_{ij}^g \}$.

\begin{Proposition}\label{lemma:LemmaE2}
Given that $\lambda_{ij}^g$ are chosen as
%
$$
\lambda_{ij}^g \geq \max_{k \in g} \frac{2}{\sqrt{n \omega_{ii}^k}} \left( \sqrt{|g|} + \frac{\pi}{\sqrt 2} \sqrt {q \log G_0}  + \sqrt {c (v_\beta)} \right)
$$
%
we shall have $ \BP (\cA) \geq 1 - 2p G_0^{1-q} $ for some $q>1$.
\end{Proposition}

\begin{proof}[Proof of Proposition~\ref{lemma:LemmaE2}]
We follow the proof of Lemma E.2 in 15-656, with $\bfY_j^k, \hat \bfepsilon_i^k, \hat \zeta_{ij}^k, \hat \bfzeta_{ij}^{[g]}$ in place of $\bfX_j^k, \bfepsilon_i^k, \zeta_{ij}^k, \bfzeta_{ij}^{[g]}$ respectively. Proceeding in a similar fashion we get
%
$$
\| \hat \bfzeta_{ij}^{[g]} \|^2 = \frac{1}{n} ( \| \bfZ^{[g]} \|^2 + 2 \sum_{k \in g} Z^k (\bfQ_j^k)^T \bfdelta_i^k + \| (\bfQ_j^k)^T \bfdelta_i^k  \|^2)
$$
%
where $\bfZ^{[g]} = (Z^k)_{k \in g}; Z^k = (\bfQ_j^k)^T \bfepsilon_i^k$ with $\bfepsilon_i^k := \bfY_i^k - \bfY_{-i}^k \bftheta_{0,i}^k - \bfX^k \bfB_{0,i}^k$, $\bfQ_j^k$ is the first eigenvector of $\bfY_j^k (\bfY_j^k)^T/n$, and $\bfdelta_i^k := \bfX^k (\bfB_{0,i}^k - \hat \bfB_i^k)$. Applying Cauchy-schwarz inequality to right side and by assumption (T2),
%
$$
\| \hat \bfzeta_{ij}^{[g]} \| \leq \frac{1}{\sqrt n} ( \| \bfZ^{[g]} + \sqrt{ c(v_\beta)})
$$
%
thus
$$
\BP ( \{ \cA_{ij}^g \}^c ) = \BP \left( \| \hat \bfzeta_{ij}^{[g]} \| > \frac{\lambda_{ij}^g}{2} \right) \leq \BP \left( \| \bfZ^{[g]} \|  > \frac{\sqrt n \lambda_{ij}^g}{2} - \sqrt{c(v_\beta)}  \right)
$$
We now proceed through the proof of Lemma E.2 in 15-656 to end up with the choice of $\lambda_{ij}^g$.
\end{proof}
%

All subsequent derivations in the theorem go through with the new choice of $\lambda_{ij}^g$.

%We now prove the norm consistency of $\hat \Theta_i - \Theta_{0,i}$.
%
%\begin{Proposition}\label{prop:PropA1}
%Copy from Proposition A.1 in 15-656
%\begin{align}
%\sum_{j \neq i, g \in \cG^{ij}} \| \hat \bftheta_{ij}^{[g]} - \bftheta_{0,ij}^{[g]} \| & \leq \\
%\cM ( \hat \Theta_i ) & \leq \\
%\| \hat \Theta_i - \hat \Theta_{0,i} \|_F & \leq
%\end{align}
%\end{Proposition}
%
%\begin{proof}[Proof of Proposition~\ref{prop:PropA1}]
%%We first proceed in a similar fashion as the proof of Lemma 3.1 in \cite{LouniciEtal11}. For any $\bftheta_i^k \in \BR^q$, we have
%%%
%%$$
%%\sum_{k=1}^K \frac{1}{n} \| \bfY_i^k - \bfY_{-i}^k \hat \bftheta_i^k - \bfX^k \hat \bfB_i^k \|^2 + \sum_{j \neq i, g \in \cG^{ij}} \| \hat \bftheta_{ij}^{[g]} \| \leq 
%%\sum_{k=1}^K \frac{1}{n} \| \bfY_i^k - \bfY_{-i}^k \bftheta_i^k - \bfX^k \hat \bfB_i^k \|^2 + \sum_{j \neq i, g \in \cG^{ij}} \| \bftheta_{ij}^{[g]} \|
%%$$
%%%
%%subtracting and adding $\bfY_{-i}^k \bftheta_{0,i}^k$ inside the squared norms on both sides, then simolifying and writing $\hat \bfepsilon_i^k = \bfY_i^k - \bfY_{-i}^k \bftheta_{0,i}^k - \bfX_i^k \hat \bfB_i^k$ we get
%%%
%%\begin{align*}
%%\frac{1}{n} \| \bfY_{-i}^k ( \hat \bftheta_i^k - \bftheta_{0,i}^k ) \|^2 & \leq
%%\| \bfY_{-i}^k ( \bftheta_i^k - \bftheta_{0,i}^k ) \|^2 + 
%%\frac{2}{n} ( \hat \bfepsilon_i^k )^T \bfY_{-i}^k ( \hat \bftheta_i^k - \bftheta_i^k )\\
%%& + 2 \sum_{j \neq i, g \in \cG^{ij}} \lambda_{ij}^g ( \| \bftheta_{ij}^{[g]} - \hat \bftheta_{ij}^{[g]} \|)
%%\end{align*}
%The statement of this proposition is same as that of Proposition A.1 in 15-656, and can proved in a similar fashion. The only difference is a modified choice of $\lambda_{ij}^g$, which we obtain from Proposition~\ref{lemma:LemmaE2}.
%
%\end{proof}

\textit{Part II.} Proof of Thm 2 in 15-656 follows. We only need a new bound for $Var (\bfY_i^k | \bfY_{-i}^k, \bfX^k, \hat \bfB_i^k)$. For this we have
%
$$ Var (\bfY_i^k | \bfY_{-i}^k, \bfX^k, \hat \bfB_i^k) = \BE (\hat \bfepsilon_i^k)^2
= \BE ( \bfepsilon_i^k + \bfdelta_i^k)^2
\leq \left( \frac{1}{d_0} + \frac{c(v_\beta)}{n} \right)^2
$$
%
applying cauchy-schwarz inequality followed by assumption (A2). Now Replace $1/\sqrt{n d_0}$ in choice of $\lambda, \alpha_n$ in Thm 2 statement with $1/\sqrt{n} (\sqrt{1/d_0} + \sqrt{c(v_\beta)/ n})$.

\end{proof}

\begin{Proposition}
Given fixed $\hat \cB$, prediction errors follow bound in T2 with high enough probability.
\end{Proposition}


\hrulefill

Now concentrate on the $k$-population estimation problem. We want to obtain
%
$$
\hat \bfbeta = \argmin_{ \bfbeta \in \BR^{pqK}} \{ -2 \bfbeta \hat \bfgamma + \bfbeta^T \bfGamma \bfbeta + \| \bfbeta \|_{2,g} \}
$$
with
$$
\bfbeta = \begin{bmatrix}
\ve (\bfB^1)\\
\vdots\\
\ve (\bfB^K)\\
\end{bmatrix}; \quad
\bfGamma = \begin{bmatrix}
I_q \otimes (\bfX^1) TX^1 / n) & &\\
& \ddots &\\
& & I_q \otimes (\bfX^K)^T X^K / n)
\end{bmatrix} 
$$
\begin{Theorem}

\end{Theorem}

\bibliographystyle{apalike}
%\bibliographystyle{imsart-nameyear}
\bibliography{snpbib}
\end{document}