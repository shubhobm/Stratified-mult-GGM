\documentclass[fleqn,11pt]{article}
%\documentclass[aoas,preprint]{imsart}

\usepackage{mycommands1,amssymb,amsmath,amsthm,color,pagesize,outlines,cite,subfigure}
\usepackage[small]{caption}
\usepackage[pdftex]{epsfig}
\usepackage{hyperref} % for linking references
\hypersetup{colorlinks = true, citecolor = blue, urlcolor = blue}

\usepackage{stackrel}

\usepackage[round]{natbib}

% for algorithm
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}

%\addtolength{\evensidemargin}{-.5in}
%\addtolength{\oddsidemargin}{-.5in}
%\addtolength{\textwidth}{0.9in}
%\addtolength{\textheight}{0.9in}
%\addtolength{\topmargin}{-.4in}

%% measurements for 1 inch margin
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}
\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

%\usepackage{setspace}
%\doublespacing

%\pagestyle{myheadings}
%\markboth{}{\underline{{\bf Notes: (do not circulate)} \hspace{4.5cm} {\sc  Ansu Chatterjee} \hspace{0.25cm}}}

\DeclareMathOperator*{\ve}{vec}
\DeclareMathOperator*{\diag}{diag }
\DeclareMathOperator*{\Tr}{Tr}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

%% Appendix theorem counter
\usepackage{chngcntr}
\usepackage{apptools}
\AtAppendix{\counterwithin{Theorem}{section}}
\numberwithin{equation}{section}

\begin{document}

\newtheorem{Theorem}{Theorem}[section]
\newtheorem{Lemma}[Theorem]{Lemma}
\newtheorem{Corollary}[Theorem]{Corollary}
\newtheorem{Proposition}[Theorem]{Proposition}
\newtheorem{Conjecture}[Theorem]{Conjecture}
\theoremstyle{definition} \newtheorem{Definition}[Theorem]{Definition}

\title{Simultaneous Selection of Multiple Important Single Nucleotide Polymorphisms in Familial Genome Wide Association Studies data}
\date{}
\author{Subhabrata Majumdar}
\maketitle

\noindent\textbf{Abstract}: Genome Wide Association Studies (GWAS) on samples obtained from families instead of unrelated individuals are an established way of assessing gene-environment interactions and effects of Single Nucleotide Polymorphisms (SNP) on the development of behavioral disorders. Due to multiple reasons like weak signals of causal SNPs or multiple correlated SNPs, traditional methods of SNP detection based on single-marker analysis suffer from loss of power in such studies. Although statistical model selection methods are able to remedy this, owing to the dependent structure of the familial data they are computationally demanding. Here we propose a fast variable selection algorithm to address this problem. Working on a linear mixed model with three variance components and fixed effects for multiple SNPs, we obtain a quantity called the $e$-value for each SNP by only training the model with all covariates, and select SNPs having $e$-values below a threshold. To compute the $e$-values, we utilize a fast and scalable bootstrap procedure that relies on Monte-Carlo sampling to obtain bootstrapped copies of estimated fixed effect vectors. Numerical studies reveal our method to be more effective in detecting causal SNPs than either single-marker analysis on mixed models or model selection methods that ignore the familial dependency structure. We also use the $e$-values to perform gene-level analysis in a familial GWAS dataset and detect several SNPs that have potential effect on alcohol consumption in individuals.
\vspace{.5cm}

\noindent\textbf{Keywords}: Model selection, bootstrap, data depth, family data, twin studies, ACE model, alcoholism

\newpage

\section{Model}
We have data $\cZ = \{ \cZ^1, \ldots, \cZ^K \}; \cZ^k = (\bfY^k, \bfX^k)$ where $\bfY^k \in \BR^{n \times q}, \bfX^k \in \BR^{n \times p}$ for $1 \leq k \leq K$.

\begin{eqnarray}
\bfX^k = (\bfX^k_1, \ldots, \bfX^k_p)^T \sim \cN (0, \Sigma^k_x)\\
\bfY^k = \bfX^k \bfB^k + \bfE^k; \quad \bfE^k = (\bfE^k_1, \ldots, \bfE^k_p)^T \sim \cN (0, \Sigma^k_y)\\
\Omega^k_x = (\Sigma^k_x)^{-1}; \quad \Omega^k_y = (\Sigma^k_y)^{-1}
\end{eqnarray}

Want to estimate $\{ (\Omega^k_x, \Omega^k_y, \bfB^k); 1 \leq k \leq K \}$ in presence of known grouping structures $\cG_x, \cG_y, \cH$ respectively. Estimation of $\{ \Omega_x^k \}$ done using JSEM. For the other part, we use the following two-step procedure:

\begin{enumerate}
\item Run neighborhood selection on $y$-network incorporating effects of $x$-data:
%
\begin{align}
& \min \left\{ \sum_{i=1}^p  \frac{1}{n_k} \left[ \sum_{k=1}^K \| \bfY^k_i - \bfY_{-i}^k \bftheta_i^k - \bfX^k \bfB_i^k \|^2 + 2 \sum_{j \neq i} \sum_{g \in \cG_y^{ij}} \lambda_{ij}^g \| \bftheta_{ij}^{[g]} \| \right] + 2 \sum_{h \in \cH} \eta^h \| \bfB^{[h]} \| \right\}\\
&= \min \left\{ f ( \cY, \cX, \cB, \Theta) + P (\Theta) + Q (\cB) \right\} 
\end{align}
%
where $\Theta = \{ \Theta_i \}, \cB = \{ \bfB^k \}, \cY = \{ \bfY^k \}, \cX = \{ \bfX^k \}, \cE = \{ \bfE^k \}$.

This estimates $\cB$ { \colrbf (maybe refit and/or within-group threshold?) }.

\item Step I part 2 and step II of JSEM (see 15-656 pg 6) follows to estimate $\{ \Omega_y^k \}$.
\end{enumerate}

The objective function is bi-convex, so we are going to do the following in step 1-

\begin{itemize}
\item Start with initial estimates of $\cB$ and $\Theta$, say $\cB^{(0)}, \Theta^{(0)}$.
\item Iterate:
%
\begin{align}
\Theta^{(k+1)} &= \argmin \left\{ f ( \cY, \cX, \cB^{(k)}, \Theta^{(k)}) + P (\Theta^{(k)}) \right\}\\
\cB^{(k+1)} &= \argmin \left\{ f ( \cY, \cX, \cB^{(k)}, \Theta^{(k+1)}) + Q (\cB^{(k)}) \right\}
\end{align}
\item Continue till convergence.
\end{itemize}
%

\section{Conditions}
Conditions A1, A2, A3 from JSEM paper.

\section{Results}
Define
%
\begin{align}
\hat \bfbeta^k &= \\
\hat \Theta^i &= \argmin_{\Theta_i} \left\{ \frac{1}{n_k} \sum_{k=1}^K \| \bfY^k_i - \bfY_{-i}^k \bftheta_i^k - \bfX^k \hat \bfB_i^k \|^2 + 2 \sum_{j \neq i} \sum_{g \in \cG_y^{ij}} \lambda_{ij}^g \| \bftheta_{ij}^{[g]} \| \right\}
\end{align}

\begin{Theorem}\label{thm:ThetaThm}
Assume fixed $\cX, \cE$ and deterministic $\hat \cB = \{ \bfB^k \}$. Also

\noindent{\bf(T1)} $\| \hat \bfB_i^k - \bfB_i^k \| \leq v_\beta$;

\noindent{\bf(T2)} $\| \bfX^k (\hat \bfB_i^k - \bfB_i^k ) \| \leq c(v_\beta)$ for some non-negative function $c(.)$;

Group uniform IC.

Then

\noindent (I) Estimation consistency

\noindent (II) Direction consistency 
\end{Theorem}

\begin{proof}[Proof of Theorem~\ref{thm:ThetaThm}]


\textit{Part I.} Follows proof of thm 1 in 15-656. The proof has 3 parts: consistency of neighborhood regression, selection of edge sets, and finally the refitting step.

For any $g \in \cG^{ij}, k \in g$, and $j \neq i$, let
%
$$
\hat \bfepsilon_i^k = \bfY_i^k - \bfY_{-i}^k \bftheta_{0,i}^k - \bfX^k \hat \bfB_i^k; \quad
\hat \zeta_{ij}^k = \frac{(\hat \bfepsilon_i^k)^T \bfY_j^k}{n}; \quad
\hat \bfzeta_{ij}^{[g]} = (\hat \zeta_{ij}^k)_{k \in g}
$$
%
Consider the random event $\cA = \bigcap_{i, j\neq i, g} \cA_{ij}^g$ with $\cA_{ij}^g = \{ 2 \| \hat \bfzeta_{ij}^{[g]} \| \leq \lambda_{ij}^g \}$.

\begin{Proposition}\label{lemma:LemmaE2}
Given that $\lambda_{ij}^g$ are chosen as
%
$$
\lambda_{ij}^g \geq \max_{k \in g} \frac{2}{\sqrt{n \omega_{ii}^k}} \left( \sqrt{|g|} + \frac{\pi}{\sqrt 2} \sqrt {q \log G_0}  + \sqrt {c (v_\beta)} \right)
$$
%
we shall have $ \BP (\cA) \geq 1 - 2p G_0^{1-q} $ for some $q>1$.
\end{Proposition}

\begin{proof}[Proof of Proposition~\ref{lemma:LemmaE2}]
We follow the proof of Lemma E.2 in 15-656, with $\bfY_j^k, \hat \bfepsilon_i^k, \hat \zeta_{ij}^k, \hat \bfzeta_{ij}^{[g]}$ in place of $\bfX_j^k, \bfepsilon_i^k, \zeta_{ij}^k, \bfzeta_{ij}^{[g]}$ respectively. Proceeding in a similar fashion we get
%
$$
\| \hat \bfzeta_{ij}^{[g]} \|^2 = \frac{1}{n} ( \| \bfZ^{[g]} \|^2 + 2 \sum_{k \in g} Z^k (\bfQ_j^k)^T \bfdelta_i^k + \| (\bfQ_j^k)^T \bfdelta_i^k  \|^2)
$$
%
where $\bfZ^{[g]} = (Z^k)_{k \in g}; Z^k = (\bfQ_j^k)^T \bfepsilon_i^k$ with $\bfepsilon_i^k := \bfY_i^k - \bfY_{-i}^k \bftheta_{0,i}^k - \bfX^k \bfB_{0,i}^k$, $\bfQ_j^k$ is the first eigenvector of $\bfY_j^k (\bfY_j^k)^T/n$, and $\bfdelta_i^k := \bfX^k (\bfB_{0,i}^k - \hat \bfB_i^k)$. Applying Cauchy-schwarz inequality to right side and by assumption (T2),
%
$$
\| \hat \bfzeta_{ij}^{[g]} \| \leq \frac{1}{\sqrt n} ( \| \bfZ^{[g]} + \sqrt{ c(v_\beta)})
$$
%
thus
$$
\BP ( \{ \cA_{ij}^g \}^c ) = \BP \left( \| \hat \bfzeta_{ij}^{[g]} \| > \frac{\lambda_{ij}^g}{2} \right) \leq \BP \left( \| \bfZ^{[g]} \|  > \frac{\sqrt n \lambda_{ij}^g}{2} - \sqrt{c(v_\beta)}  \right)
$$
We now proceed through the proof of Lemma E.2 in 15-656 to end up with the choice of $\lambda_{ij}^g$.
\end{proof}
%

All subsequent derivations in the theorem go through with the new choice of $\lambda_{ij}^g$.

%We now prove the norm consistency of $\hat \Theta_i - \Theta_{0,i}$.
%
%\begin{Proposition}\label{prop:PropA1}
%Copy from Proposition A.1 in 15-656
%\begin{align}
%\sum_{j \neq i, g \in \cG^{ij}} \| \hat \bftheta_{ij}^{[g]} - \bftheta_{0,ij}^{[g]} \| & \leq \\
%\cM ( \hat \Theta_i ) & \leq \\
%\| \hat \Theta_i - \hat \Theta_{0,i} \|_F & \leq
%\end{align}
%\end{Proposition}
%
%\begin{proof}[Proof of Proposition~\ref{prop:PropA1}]
%%We first proceed in a similar fashion as the proof of Lemma 3.1 in \cite{LouniciEtal11}. For any $\bftheta_i^k \in \BR^q$, we have
%%%
%%$$
%%\sum_{k=1}^K \frac{1}{n} \| \bfY_i^k - \bfY_{-i}^k \hat \bftheta_i^k - \bfX^k \hat \bfB_i^k \|^2 + \sum_{j \neq i, g \in \cG^{ij}} \| \hat \bftheta_{ij}^{[g]} \| \leq 
%%\sum_{k=1}^K \frac{1}{n} \| \bfY_i^k - \bfY_{-i}^k \bftheta_i^k - \bfX^k \hat \bfB_i^k \|^2 + \sum_{j \neq i, g \in \cG^{ij}} \| \bftheta_{ij}^{[g]} \|
%%$$
%%%
%%subtracting and adding $\bfY_{-i}^k \bftheta_{0,i}^k$ inside the squared norms on both sides, then simolifying and writing $\hat \bfepsilon_i^k = \bfY_i^k - \bfY_{-i}^k \bftheta_{0,i}^k - \bfX_i^k \hat \bfB_i^k$ we get
%%%
%%\begin{align*}
%%\frac{1}{n} \| \bfY_{-i}^k ( \hat \bftheta_i^k - \bftheta_{0,i}^k ) \|^2 & \leq
%%\| \bfY_{-i}^k ( \bftheta_i^k - \bftheta_{0,i}^k ) \|^2 + 
%%\frac{2}{n} ( \hat \bfepsilon_i^k )^T \bfY_{-i}^k ( \hat \bftheta_i^k - \bftheta_i^k )\\
%%& + 2 \sum_{j \neq i, g \in \cG^{ij}} \lambda_{ij}^g ( \| \bftheta_{ij}^{[g]} - \hat \bftheta_{ij}^{[g]} \|)
%%\end{align*}
%The statement of this proposition is same as that of Proposition A.1 in 15-656, and can proved in a similar fashion. The only difference is a modified choice of $\lambda_{ij}^g$, which we obtain from Proposition~\ref{lemma:LemmaE2}.
%
%\end{proof}

\textit{Part II.} Proof of Thm 2 in 15-656 follows. We only need a new bound for $Var (\bfY_i^k | \bfY_{-i}^k, \bfX^k, \hat \bfB_i^k)$. For this we have
%
$$ Var (\bfY_i^k | \bfY_{-i}^k, \bfX^k, \hat \bfB_i^k) = \BE (\hat \bfepsilon_i^k)^2
= \BE ( \bfepsilon_i^k + \bfdelta_i^k)^2
\leq \left( \frac{1}{d_0} + \frac{c(v_\beta)}{n} \right)^2
$$
%
applying cauchy-schwarz inequality followed by assumption (A2). Now Replace $1/\sqrt{n d_0}$ in choice of $\lambda, \alpha_n$ in Thm 2 statement with $1/\sqrt{n} (\sqrt{1/d_0} + \sqrt{c(v_\beta)/ n})$.

\end{proof}

\begin{Proposition}
Given fixed $\hat \cB$, prediction errors follow bound in T2 with high enough probability.
\end{Proposition}

\bibliographystyle{apalike}
%\bibliographystyle{imsart-nameyear}
\bibliography{snpbib}
\end{document}