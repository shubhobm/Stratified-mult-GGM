\section{Introduction}
The human body is a complex system, comprising of several regulatory networks that are connected within and between themselves. These networks also contain a natural hierarchy within them: for exmaple genes produce RNAs through transcription, which etc etc

$->$ recent advances made a lot of data available. mention tcga, ``It has largely been accepted that a comprehensive understanding of a biological system can come only from a joint analysis of all omics layers"

give structure in figure. lit. review.

Gaussian Graphical Models (GGM) have been extensively used to model biological networks in the last few years. While the initial work on GGMs focused on estimating undirected edges within a single network through obtaining sparse estimates of the inverse covariance matrix, or the precision matrix from the data (e.g. see the references in \cite{BuhlmannvandeGeer11}), the attention has now shifted to estimating parameters from more complex structures: especially multiple related graphical models and hierarchical multilayer models with both directed and undirected edges. For the first problem, \cite{GuoEtal11} and \cite{XieLiuValdar16} assumed perturbations over a common underlying structure for modelling multiple precision matrices, while \cite{DanaherEtal14} proposed using fused lasso or group lasso penalties in a joint group lasso model for the same purpose. To incorporate prior information on the group structures across several graphs, \cite{MaMichailidis15} proposed the Joint Structural Estimation Method (JSEM), which uses group-penalized neighborhood regression and subsequent refitting for the estimation of precision matrices.

For the second problem, a two-layered structure can be modeled by interpreting directed edges between the two layers as elements of a multitask regression coefficient matrix, while undirected edges inside either layer correspond to the precision matrix of predictors in that layer. While several methods exist in the literature for joint estimation of both parameters \citep{RothmanEtal10, LeeLiu12, CaiEtal12}, only recently \cite{LinEtal16} made the observation that a multilayer model can, in fact, be decomposed into a series of two-layer problems. Subsequently, they proposed an estimation algorithm and derived theoretical properties of the resulting estimators.

\begin{figure}
\centering
\includegraphics[]{multi2layer}
\caption{Multiple multilayer graphical models. The matrices $(\bfX^k, \bfY^k, \bfZ^k), k = 1,2,3$ indicate data for each layer and category $k$. Within-layer connections (black lines) are undirected, while between-layer connections (red lines) go from an upper layer to the successive lower layer. For each type of edges (i.e. within $\cX, \cY, \cZ$ and $\cX \rightarrow \cY, \cY \rightarrow \cZ$), there are common edges across some or all $k$. }
\label{fig:multi2layer}
\end{figure}

\paragraph{}
All the above approaches model either the horizontal or the vertical complexity in the full hierarchical structure of Figure~\ref{fig:multi2layer}. This means multiple related groups of of heterogeneous datasets has to be modeled by analyzing all data in individual layers (i.e. models for $\{ \bfX^k \}$, $\{ \bfY^k \}$, $\{ \bfZ^k \}$), and then separately analyzing individual hierarchies of datasets (i.e. separate models for $(\bfX^k, \bfY^k, \bfZ^k), k = 1,2,3$). Although a recent paper \citep{ZhangOuyangZhao17} provides a model for the full structure in Figure~\ref{fig:multi2layer} using penalized log-likelihoods, they do not provide theoretical guarantees for the estimates, and limit the numerical examples to small feature dimensions ($<70$) only. Thus, a truly rigorous and scalable formulation of a model for data integration is yet to be proposed.

While there has been some progress for parameter estimation in multilayer models, little is known about the asymptotic properties of resulting estimates. Current research on asymptotic distributions and testing procedures for estimates from high-dimensional problems has been restricted to single-response regression using lasso \citep{ZhangZhang14,JavanmardMontanari14,JavanmardMontanari18,vanDeGeerEtal14} or group lasso \citep{MitraZhang16} penalties, and partial correlation matrices of single \citep{CaiLiu16} or multiple \citep{Liu17} GGMs. From a systemic perspective, testing and identifying downstream interactions that differ across experimental conditions or disease subtypes can offer important insight on the underlying biological process \citep{MaoEtal17,LiEtal15}. In our framework, this can be done by developing a hypothesis testing procedure for entries in thee within-layer regression matrices.

\paragraph{}
The contributions of this paper are two-fold. Firstly, we propose an integrative framework to conduct simultaneous inference for all parameters in multiple multilayer graphical models, essentially formalizing the structure in Figure~\ref{fig:multi2layer}. We decompose the multi-layer problem into a series of two layer problems, incorporate prior information on structural dependencies through imposing group structures on the model parameters, propose an estimation algorithm for them and derive theoretical properties of the estimators. Secondly, we obtain debiased versions of within-layer regression coefficients in this two-layer model, and derive their asymptotic distributions using estimates of model parameters that satisfy generic convergence guarantees. Consequently, we formulate a global test, as well as a simultaneous testing procedure that controls for False Discovery Rate (FDR) to detect important directed edges between layers.

The framework for knowledge discovery from heterogeneous data sources we propose is highly flexible. The group sparsity assumptions in our estimation technique can be replaced by other structural restrictions, for example low-rank or low-rank-plus-sparse, as an when deemed appropriate by the prior dependency assumptions across parameters. As long as the resulting estimates converge to corresponding true parameters at certain rates, they can be plugged into the testing methodology.
\paragraph{Organization of paper.}

\paragraph{Notations.}
We denote scalars by small letters, vectors by bold small letters and matrices by bold capital letters. For any matrix $\bfA$, $(\bfA)_{ij}$ denote its element in the $(i,j)^\text{th}$ position. For $a,b \in \BN$, we denote the set of all $a \times b$ real matrices by $\BM(a,b)$. For any positive integer $c$, define $\cI_c = \{ 1, \ldots, c\}$. For vectors $\bfv$ and matrices $\bfM$, $\| \bfv \|$, $\|\bfv \|_1$ or $\|\bfM \|_1$ and $\|\bfv \|_\infty$ or $\|\bfM \|_\infty$ denote euclidean, $\ell_1$ and $\ell_\infty$ norms, respectively. The notation $\supp(\bfA)$ indicates the non-zero edge set in a matrix (or vector) $\bfA$, i.e. $\supp(\bfA) = \{(i,j): (\bfA)_{ij} \neq 0\}$. For positive real numbers $A, B$ we write $A \succsim B$ if there exists $c>0$ independent of model parameters such that $A \geq cB$. 