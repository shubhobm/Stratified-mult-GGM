\section{The Joint Multiple Multilevel Estimation Framework}
\subsection{Formulation}
Suppose there are $K$ independent datasets, each pertaining to an $M$-layered Gaussian Graphical Model (GGM). The $k^{\Th}$ model has the following structure:

\vspace{1em}
\begin{tabular}{ll}
{\it Layer 1}- &
%
$\BD_1^k = (D_{1 1}^k, \ldots, D^k_{1 p_1}) \sim
\cN (0, \Sigma_1^k); \quad k \in \cI_K,$\\
{\it Layer $m$} $(1< m \leq M)$-  &
%
$ \BD_m^k = \BD_{m-1}^k \bfB_m^k + \BE_m^k$, with $\bfB_m^k \in \BM(p_{m-1}, p_m) $\\
& and $\BE_m^k = (E_{m 1}^k, \ldots, E^k_{m p_m}) \sim
\cN (0, \Sigma_m^k); \quad k \in \cI_K $.\\
\end{tabular}
\vspace{1em}

We assume known structured sparsity patterns, denoted by $\cG_m$ and $\cH_m$, for the parameters of interest in the above model, i.e. the precision matrices $\Omega_m^k := (\Sigma_m^k)^{-1}$ and the regression coefficient matrices $\bfB_m^k$, respectively. These patterns provide information on horizontal dependencies across $k$ for the corresponding parameters, and our goal here is to leverage them to estimate the full hierarchical structure of the network- specifically to obtain the undirected edges inside nodes of a single layer, and the directed edges between two successive layers through jointly estimating $\{ \Omega_m^k \}$ and $\{ \bfB_m^k \}$.

Consider a two-layer model, which is a special case of the above model with $M=2$:
%
\begin{eqnarray}
\BX^k = (X^k_1, \ldots, X^k_p)^T \sim \cN (0, \Sigma^k_x);\\
\BY^k = \BX^k \bfB^k + \BE^k; \quad \BE^k = (E^k_1, \ldots, E^k_p)^T \sim \cN (0, \Sigma^k_y);\\
\bfB^k \in \BM(p,q), \quad \Omega^k_x = (\Sigma^k_x)^{-1}; \quad \Omega^k_y = (\Sigma^k_y)^{-1};
\end{eqnarray}
%
where we want to estimate $\{ (\Omega^k_x, \Omega^k_y, \bfB^k); k \in \cI_K$ from data $\cZ^k = \{ (\bfY^k, \bfX^k); \bfY^k \in \BM(n,q), \bfX^k \in \BM(n,p), k \in \cI_K\}$ in presence of known grouping structures $\cG_x, \cG_y, \cH$ respectively and assuming $n_k = n$ for all $k \in \cI_K$ for simplicity. We focus most of the theoretical discussion in the rest of the paper on jointly estimating $\Omega_y:= \{ \Omega_y^k \}$ and $\cB := \{ \bfB^k \}$. This is because for $M>2$, within-layer undirected edges of any $m{\Th}$ layer $(m>1)$ and between-layer directed edges from the $(m-1)^{\Th}$ layer to the $m^{\Th}$ layer can be estimated from the corresponding data matrices in a similar fashion. On the other hand, parameters in the first layer are analogous to $\Omega_x := \{ \Omega_x^k \}$ that are dependent only on $\{ \bfX^k\}$, so any method for joint estimation of multiple graphical models can be used to estimate them (e.g. \cite{GuoEtal11, MaMichailidis15}). This provides all building blocks for estimating the full hierarchical structure of our $M$-layered multiple GGMs.

%Setting $M=1$ reduces the above model to joint estimation of GGMs with structured sparsity \citep{MaMichailidis15}, while setting $K=1$ reduces the model to a multi-layer GGM, which can be estimated by breaking it down to successive two-layer models and then minimizing a penalized conditional log-likelihood function \citep{LinEtal16}.

\subsection{Algorithm}
We assume an element-wise group sparsity pattern over $k$ for the precision matrices $\Omega_x^k$:
%
\[
\cG_x = \{ \cG_x^{ii'}: i \neq i'; i, i' \in \cI_p \},
\]
%
where each $\cG_x^{ii'}$ is a partition of $\cI_K$. Subsequently we use the Joint Structural Estimation Method (JSEM) \citep{MaMichailidis15} to estimate $\Omega_x$, which first uses the group structure given by $\cG_x$ in penalized nodewise regressions \citep{MeisenBuhlmann06} to obtain neighborhood coefficients of each variable $X_i, i \in \cI_p$, then fits a graphical lasso model over the combined support sets to obtain sparse estimates of the precision matrices:
%
\begin{align}\label{eqn:jsem-model}
\widehat \zeta_i &= \argmin_{\zeta_i} \left\{
\frac{1}{n} \sum_{k=1}^K \| \bfX_i^k - \bfX_{-i}^k \bfzeta_i^k \|^2 +
2 \sum_{i' \leq i} \sum_{g \in \cG_x^{ii'}} \eta_n \| \bfzeta_{ii'}^{[g]} \| \right\} \notag\\
\widehat E_x^k &= \{(i,i'): 1 \leq i < i' \leq p, \hat \zeta_{ii'}^k \neq 0 \text{ OR } \hat \zeta_{i'i}^k \neq 0 \}\notag\\
\widehat \Omega_x^k &= \argmin_{\Omega_x^k \in \BS_+ (\hat E_x^k)}
\left\{ \Tr (\widehat \bfS_x^k \Omega_x^k ) - \log \det (\Omega_x^k) \right\}
\end{align}
%
where $\widehat \bfS_x^k := (\bfX^k)^T \bfX^k/n_k$.

We obtain sparse estimates of $\Omega_y$ and $\cB$ by solving the following group-penalized least square minimization problem:
%
\begin{align}
\{ \widehat \cB, \widehat \Theta \} &= 
\argmin_{\cB, \Theta} \left\{ \frac{1}{n} \sum_{j=1}^q \sum_{k=1}^K \| \bfY^k_j - (\bfY_{-j}^k - \bfX^k \bfB_{-j}^k) \bftheta_j^k - \bfX^k \bfB_j^k \|^2 \right. \notag\\
& \left. + 2 \sum_{j \neq j'} \sum_{g \in \cG_y^{jj'}} \gamma_n \| \bftheta_{jj'}^{[g]} \| + \sum_{h \in \cH} \lambda_n \| \bfB^{[h]} \| \right\} \label{eqn:jmmle-objfun}\\
\widehat E_y^k &= \{(j,j'): 1 \leq j < j' \leq q, \hat \theta_{jj'}^k \neq 0 \text{ OR } \hat \theta_{j'j}^k \neq 0 \}\notag\\
\widehat \Omega_y^k &= \argmin_{\Omega_y^k \in \BS_+ (\hat E_y^k)}
\left\{ \Tr (\widehat \bfS_y^k \Omega_y^k ) - \log \det (\Omega_y^k) \right\} \label{eqn:omega-y-calc}
\end{align}
%&= \min \left\{ f ( \cY, \cX, \cB, \Theta) + P (\Theta) + Q (\cB) \right\} 
%
The outcome of a node in the lower layer is thus modeled using all other nodes in that layer {\it and} nodes in the immediate upper layer, with their effects quantified using $\widehat \bftheta_j^k$ and $\widehat \bfB_j^k$, respectively.

\subsubsection{Alternating algorithm}
The objective function in \eqref{eqn:jmmle-objfun} is bi-convex, i.e. convex in $\cB$ for fixed $\Theta$, and vice-versa, but not jointly convex in $\{ \cB, \Theta \}$. Consequently, we use an alternating iterative algorithm to solve for $\{ \cB, \Theta \}$ that minimizes \eqref{eqn:jmmle-objfun} by iteratively cycling between $\cB$ and $\Theta$, i.e. holding one set of parameters fixed and solving for the other, then alternating until convergence.

Choice of initial values plays a crucial role in the performance of this alternating algorithm. Following the analysis of \cite{LinEtal16}, who proposed an estimation framework for the special case of our multi-level structure for $K=1$ based on a similar algorithm, we choose the initial values $\{ \bfB^{k (0)} \}$ by first selecting a support set for the $j^{\Th}$ column, say $\tilde \cS_j^k$, as the support of the debiased lasso estimate of \cite{JavanmardMontanari14}, then fitting a Lasso regression model with small penalty value $\lambda_n^0$:
%
\begin{align}\label{eqn:init-B}
\widehat \bfB_j^{k (0)} = \argmin_{\supp(\bfB_j^k) \subseteq \tilde \cS_j^k} \|\bfY_j^k - \bfX^k \bfB_j^k \|^2 + \lambda_n^0 \| \bfB_j^k \|_1
\end{align}
%
We obtain initial estimates of $ \Theta_j, j \in \cI_q$ by performing group-penalized nodewise regression on the residuals $\widehat \bfE^{k (0)} := \bfY^k - \bfX^k \bfB_j^{k (0)}$:
%
\begin{align}\label{eqn:init-Theta}
\widehat \Theta_j^{(0)} = \argmin_{\Theta_j} \frac{1}{n} \sum_{k=1}^K \|
\widehat \bfE_j^{k (0)} - \widehat \bfE_{-j}^{k (0)} \bftheta_j^k \|^2
+ 2 \gamma_n \sum_{j \neq j'} \sum_{g \in \cG_y^{jj'}} \| \bftheta_{jj'}^{[g]} \|
\end{align}

The steps of our full estimation procedure, which we call the Joint Multiple Multilevel Estimation (JMMLE) method, can thus be summarized in Algorithm \ref{algo:jmmle-algo}.

\begin{Algorithm}
(The JMMLE Algorithm)
\label{algo:jmmle-algo}

\noindent 1. Initialize $\widehat \cB$ using \eqref{eqn:init-B}.

\noindent 2. Initialize $\widehat \Theta$ using \eqref{eqn:init-Theta}.

\noindent 3. Update $\widehat \cB$ as:
%
\begin{align}\label{eqn:update-B}
\widehat \cB^{(t+1)} &= \argmin_{\cB \in \BM(p,q)} \left\{ \frac{1}{n} \sum_{j=1}^q \sum_{k=1}^K \| \bfY^k_j - (\bfY_{-j}^k - \bfX^k \bfB_{-j}^k) \widehat \bftheta_j^{k (t)} - \bfX^k \bfB_j^{k } \|^2
+ \lambda_n \sum_{h \in \cH} \| \bfB^{[h]} \| \right\}
\end{align}

\noindent 4. Obtain $\widehat \bfE^{k (t+1)} := \bfY^k - \bfX^k \bfB_j^{k (t)}, k \in \cI_K$. Update $\widehat \Theta$ as:
%
\begin{align}\label{eqn:update-Theta}
\widehat \Theta_j^{(t+1)} = \argmin_{\Theta_j \in \BM(q-1, K)}
\left\{ \frac{1}{n} \sum_{k=1}^K
\| \widehat \bfE_j^{k (t+1)} - \widehat \bfE_{-j}^{k (t+1)} \bftheta_j^k \|^2
+ 2 \gamma_n \sum_{j \neq j'} \sum_{g \in \cG_y^{jj'}} \| \bftheta_{jj'}^{[g]} \| \right\}
\end{align}

\noindent 5. Continue till convergence.

\noindent 6. Calculate $\widehat \Omega_y^k, k \in \cI_K$ using \eqref{eqn:omega-y-calc}.
\end{Algorithm}

\subsubsection{Tuning parameter selection}
The nodewise regression step in the JSEM model \eqref{eqn:jsem-model} uses Bayesian Information Criterion (BIC) for tuning parameter selection. The step for updating $\{ \Theta \}$, i.e. \eqref{eqn:update-Theta}, in our JMMLE algorithm is analogous to this procedure, hence we use BIC to select the penalty parameter $\gamma_n$. In our setting the BIC for a given $\gamma$ and fixed $\cB$ is given by:
%
\begin{align*}
\text{BIC} (\gamma; \cB) &=
\Tr \left( \bfS_y^k \widehat \Omega_{y,\gamma}^k \right) - \log \det \left( \widehat \Omega_{y,\gamma}^k \right) +
\frac{\log n}{n} \sum_{k=1}^K | \widehat E_{y,\gamma}^k |
\end{align*}
%
where $\gamma$ in subscript indicates the corresponding quantity is calculated taking $\gamma$ as the tuning parameter, and $\bfS_y^k := (\bfY^k - \bfX^k \bfB^k)^T (\bfY^k - \bfX^k \bfB^k)/n$. Every time $\widehat \Theta$ is updated in the JMMLE algorithm, we choose the optimal $\gamma$ as the one with the smallest BIC over a fixed set of values $\cC_n$. Thus for a fixed $\lambda$, our final choice of $\gamma$ will be 
$
\gamma^* (\lambda) = \argmin_{\gamma \in \cC_n} \text{BIC} (\gamma; \widehat \cB_\lambda)
$.

We use the High-dimensional BIC (HBIC) to select the other tuning parameter, $\lambda$:
%
\begin{align*}
\text{HBIC} (\lambda; \Theta) &=
\frac{1}{n} \sum_{j=1}^q \sum_{k=1}^K \| \bfY^k_j - (\bfY_{-j}^k - \bfX^k \widehat \bfB_{-j,\lambda}^k ) \bftheta_j^{k } - \bfX^k \widehat \bfB_{j,\lambda}^k \|^2 +\\
& \log (\log n) \frac{\log (pq)}{n} \sum_{k=1}^K
\left( \| \bfB^k \|_0 + | \widehat E_{y, \gamma^* (\lambda)}^k| \right)
\end{align*}
%
We choose an optimal $\lambda$ as the minimizer of HBIC by training multiple JMMLE models using Algorithm \ref{algo:jmmle-algo} over a finite set of values $\lambda \in \cD_n$: 
$
\lambda^* = \argmin_{\lambda \in \cD_n} \text{HBIC} (\lambda, \widehat \Theta_{\gamma^*(\lambda)})
$.

%\begin{enumerate}
%\item Run neighborhood selection on $y$-network incorporating effects of $x$-data and an additional blockwise group penalty:
%%
%
%%
%where $\Theta = \{ \Theta_i \}, \cB = \{ \bfB^k \}, \cY = \{ \bfY^k \}, \cX = \{ \bfX^k \}, \cE = \{ \bfE^k \}$.
%
%This estimates $\cB$ { \colrbf (possibly refit and/or within-group threshold) }.
%
%\item Step I part 2 and step II of JSEM (see 15-656 pg 6) follows to estimate $\{ \Omega_y^k \}$.
%\end{enumerate}

%The objective function is bi-convex, so we are going to do the following in step 1-
%
%\begin{itemize}
%\item Start with initial estimates of $\cB$ and $\Theta$, say $\cB^{(0)}, \Theta^{(0)}$.
%\item Iterate:
%%
%\begin{align}
%\Theta^{(t+1)} &= \argmin \left\{ f ( \cY, \cX, \cB^{(t)}, \Theta^{(t)}) + P (\Theta^{(t)}) \right\}\\
%\cB^{(t+1)} &= \argmin \left\{ f ( \cY, \cX, \cB^{(t)}, \Theta^{(t+1)}) + Q (\cB^{(t)}) \right\}
%\end{align}
%\item Continue till convergence.
%\end{itemize}
%%

\subsection{Consistency of JMMLE estimators}

{\bf summary}
\begin{itemize}
\item Say what to do
\item Define extra notations
\item Define conditions
\item State results
\end{itemize}

\begin{Theorem}\label{corollary:thm-Omega}
Consider any deterministic $\widehat \cB$ that satisfy the following bound
%
$$
\| \widehat \bfB^k - \bfB_0^k \|_1 \leq v_\beta = \eta_\beta \sqrt{ \frac{ \log(pq)}{n}}
$$
%
Then, for sample size $n \succsim \log (pq)$ and choice of tuning parameter $\gamma_n = 4 \sqrt{| g_{\max}|} \BQ_0$, there exist constants $ c_1, c_3, c_4 > 0, c_2, c_5 > 1$ such that the following holds
%
\begin{align}\label{eqn:OmegaBounds}
\frac{1}{K} \sum_{k=1}^K \| \widehat \Omega_y^k - \Omega_y^k \|_F \leq
O \left( \BQ_0 \sqrt{\frac{| g_{\max}| S}{K}} \right)
\end{align}
%
with probability $\geq 1 - 1/p^{\tau_1-2} - 6c_1 \exp [-(c_2^2-1) \log(pq)] - 2 \exp (- c_3 n) - 6c_4 \exp [-(c_5^2-1) \log(pq)]$.
\end{Theorem}

\begin{Theorem}\label{thm:thm-B}
Assume deterministic $\widehat \Theta$, so that for $j \in \cI_q$,
%
\[
\| \widehat \Theta_j - \Theta_{0,j} \|_F \leq v_\Theta  = \eta_\Theta \sqrt{\frac{\log q}{n}}
\]
%
for some $\eta_\Theta > 0$ dependent on $\Theta$ only. {\colrbf more conditions?} Then, given the choice of tuning parameter
%
$$
\lambda_n \geq 4 \sqrt{| h_{\max} |} \BR_0 \sqrt{ \frac{ \log(pq)}{n}}; \quad 
\BR_0 := \max_{k \in \cI_K} \BR \left(v_\Theta, \Sigma_x^k, \Sigma_y^k \right)
$$
%
the following holds
%
\begin{align}
\| \widehat \bfbeta - \bfbeta_0 \|_1 & \leq 48 \sqrt{ | h_{\max} |} s_\beta \lambda/ \psi^* \label{eqn:BetaThmEqn1}\\
\| \widehat \bfbeta - \bfbeta_0 \| & \leq 12 \sqrt s_\beta \lambda/ \psi^* \label{eqn:BetaThmEqn2}\\
\sum_{h \in \cH} \| \bfbeta^{[h]} - \bfbeta_0^{[h]} \| & \leq 48 s_\beta \lambda/ \psi^* \label{eqn:BetaThmEqn3}\\
(\widehat \bfbeta - \bfbeta_0 )^T \widehat \bfGamma (\widehat \bfbeta - \bfbeta_0 ) & \leq
72 s_\beta \lambda^2 / \psi^* \label{eqn:BetaThmEqn4}
\end{align}
%
with probability $\geq {\colrbf tbd}$.
\end{Theorem}

Conditions A1 from JSEM paper holds for $\cX$ and $\cE$. Also A2, A3 from JSEM paper.

To prove the results in this section, we use a reparametrization of the neighborhood coefficients at the lower level. Specifically, notice that for $j \in \cI_q, k \in \cI_K$, the corresponding summand in $f(\cY, \cX, \cB, \Theta)$ can be rearranged as
%
\begin{align*}
\| \bfY^k_j - \bfX^k \bfB_j^k - (\bfY_{-j}^k - \bfX^k \bfB_{-j}^k) \bftheta_j^k \|^2 &=
\| \bfY^k_j - \bfY_{-j}^k \bftheta_j^k - (\bfX^k \bfB_j^k -\bfX^k \bfB_{-j}^k \bftheta_j^k) \|^2 \\
&= \| ( \bfY - \bfX \bfB ) \bfT_j^k \|^2
\end{align*}
%
where
%
$$
T_{jj'}^k = \begin{cases}
1 \text{ if } j = j'\\
- \theta_{jj'}^k \text{ if } j \neq j'
\end{cases}
$$
%
Thus, with $\bfT^k := (\bfT_j^k)_{j \in \cI_q}$, we have
$$
f( \cY, \cX, \cB, \Theta) = \frac{1}{n} \sum_{j=1}^p \sum_{k=1}^K \| ( \bfY^k - \bfX^k \bfB^k ) \bfT_j^k \|^2
= \frac{1}{n} \sum_{k=1}^K \| \bfY^k - \bfX^k \bfB^k ) \bfT^k \|_F^2
= \sum_{k=1}^K \Tr (\bfS^k (\bfT^k)^2 )
$$
%
where $\bfS^k = (1/n) (\bfY^k - \bfX^k \bfB^k) (\bfY^k - \bfX^k \bfB^k)^T$ is the sample covariance matrix.

Now suppose $\bfbeta = \ve (\bfB)$, and any subscript or superscript on $\bfB$ is passed on to $\bfbeta$. Denote by $\widehat \bfbeta$ and $\widehat \Theta$ the generic estimators given by
%
\begin{align}
\widehat \bfbeta &= \argmin_{\bfbeta \in \BR^{pq}} \left\{-2 \bfbeta^T \widehat \bfgamma + \bfbeta^T \widehat \bfGamma \bfbeta + \lambda_n \sum_{h \in \cH} \| \bfbeta^{[h]}  \| \right\} \label{eqn:EstEqn1}\\
\widehat \Theta_j &= \argmin_{\Theta_j \in \BM(q-1, K)} \left\{ \frac{1}{n} \sum_{k=1}^K \| \bfY^k_j - \bfX^k \widehat \bfB_j^k - (\bfY_{-j}^k - \bfX^k \widehat \bfB^k_{-j} ) \bftheta_j^k \|^2 + \gamma_n \sum_{j \neq j'} \sum_{g \in \cG_y^{jj'}} \| \bftheta_{jj'}^{[g]} \| \right\} \label{eqn:EstEqn2}
\end{align}
%
where
%
$$
\widehat \bfGamma = \begin{bmatrix}
(\widehat \bfT^1)^2 \otimes \frac{(\bfX^1)^T \bfX^1}{n} & &\\
& \ddots &\\
& & (\widehat \bfT^K)^2 \otimes \frac{(\bfX^K)^T \bfX^K}{n}
\end{bmatrix}; \quad
\widehat \bfgamma = \begin{bmatrix}
(\widehat \bfT^1)^2 \otimes \frac{(\bfX^1)^T}{n}\\
\vdots\\
(\widehat \bfT^K)^2 \otimes \frac{(\bfX^K)^T}{n}
\end{bmatrix}
\begin{bmatrix}
\ve (\bfY^1)\\
\vdots\\
\ve (\bfY^K)
\end{bmatrix}
$$
with $\widehat \bfT^k$ defined the same way using $\widehat \bftheta_j^k$ as we defined $\bfT^k$ using $\bftheta_j^k$.
\begin{Theorem}\label{thm:ThetaThm}
Assume fixed $\cX, \cE$ and deterministic $\widehat \cB = \{ \widehat \bfB^k \}$. Also for $k = 1, \ldots, K$,

\noindent{\bf(T1)} $\| \widehat \bfB^k - \bfB^k_0 \|_1 \leq v_\beta$, where $v_\beta = \eta_\beta \sqrt{\frac{\log (pq)}{n}}$ with $\eta_\beta \geq 0$ depending on $\cB$ only;

%\noindent{\bf(T2)} $\| \bfX^k (\widehat \bfB^k - \bfB^k_0 ) \|_\infty \leq c(v_\beta)$, where $c(v_\beta)$ is $O(1)$ and depends on $v_\beta$.

\noindent{\bf(T2)} Denote \textbf{$\widehat \bfE^k = \bfY^k - \bfX^k \widehat \bfB^k, k \in \cI_K$}. Then for all $j \in \cI_q$,
%
$$
\frac{1}{n} \left\| (\widehat \bfE_{-j}^k)^T \widehat \bfE^k \bfT_{0,j}^k \right\|_\infty \leq
%\BQ_0 = \max_{k \in \cI_k} 
\BQ \left(v_\beta, \Sigma_x^k, \Sigma_y^k \right)
$$
%
where $\BQ \left(v_\beta, \Sigma_x^k, \Sigma_y^k \right)$ is a $O(\sqrt{ \log (pq)/ n)}$ deterministic function of $\cB, \Sigma_x^k$ and $\Sigma_y^k$.

\noindent{\bf(T3)} Denote $\widehat \bfS^k = (\widehat \bfE^k)^T \widehat \bfE^k/n$. Then $\widehat \bfS^k \sim RE(\psi^k, \phi^k)$ with $Kq \phi \leq \psi/2$ where $ \psi = \min_k \psi^k, \phi = \max_k \phi^k $;

\noindent{\bf(T4)} Assumption (A2) holds for $\Sigma_y^k$.

Then, given the choice of tuning parameter
%
$$
\gamma_n = 4 \sqrt{| g_{\max}|} \BQ_0; \quad \BQ_0 := \max_{k \in \cI_K} \BQ \left(v_\beta, \Sigma_x^k, \Sigma_y^k \right)
$$
%
the following holds
%
\begin{align*}
\frac{1}{K} \sum_{k=1}^K \| \widehat \Omega_y^k - \Omega_y^k \|_F \leq
O \left( \BQ_0 \sqrt{\frac{| g_{\max}| S}{K}} \right)
\end{align*}
%
where $|g_{\max}|$ is the maximum group size.

%Further if A1 holds with $s = s_0$, and A3 is satisfied then
%
%(II) Direction consistency.
\end{Theorem}

When $\cX$ and $\cE$ are random, the following propositions ensures that conditions (T2) and (T3) hold with probabilities approaching to 1.

\begin{Proposition}\label{prop:ErrorRE}
Consider deterministic $\widehat \cB$ satisfying assumption (T1). Then for sample size $n \succsim \log (pq)$ and $k \in \cI_K$,

\begin{enumerate}
%\item We have $\| \bfX^k ( \hat \bfB^k - \bfB_0^k ) \|_\infty \leq c( v_\beta)$, where
%%
%$$
%c(v_\beta) =\sqrt n v_\beta \left[ \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x^k n}} + \max_j \sigma_{x,jj}^k \right]^{1/2}; \quad
%c_x^k = \left[ 128 ( 1 + 4 \Lambda_{\max} (\Sigma_x^k)  )^2 \max_j (\sigma_{x,jj}^k)^2 \right]^{-1}
%$$
%%
%with probability $ \geq 1 - 1/p^{\tau_1-2}, \tau_1 > 2$.
%
\item $\widehat \bfS^k$ satisfies the RE condition: $ \widehat \bfS^k \sim RE (\psi^k, \phi^k)$, where 
%
$$
\psi^k = \frac{ \Lambda_{\min} (\Sigma_x^k)}{2}; \quad \phi^k = \frac{ \psi^k \log p}{n} + 2 v_\beta c_2 [ \Lambda_{\max} (\Sigma_x^k) \Lambda_{\max} (\Sigma_y^k) ]^{1/2} \sqrt{\frac{ \log(pq)}{n}}
$$
%
with probability $\geq 1 - 6c_1 \exp [-(c_2^2-1) \log(pq)] - 2 \exp (- c_3 n), c_1, c_3 > 0, c_2 > 1$.
%
\item The following deviation bound is satisfied for any $j \in \cI_q$
%
%$$
%\left\| \widehat \bfS^k \bfT_{0,j}^k \right\|_\infty \leq 4 v_\beta c_2 [ \Lambda_{\max} (\Sigma_x^k) \Lambda_{\max} (\Sigma_y^k) ]^{1/2} \sqrt{\frac{ \log(pq)}{n}} + 2 \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x^k n}} + 2 \max_j \sigma_{x,jj}^k
%$$
%
$$
\left\|\frac{1}{n} (\widehat \bfE_{-j}^k)^T \widehat \bfE^k \bfT_{0,j}^k \right\|_\infty \leq \BQ \left(v_\beta, \Sigma_x^k, \Sigma_y^k \right)
$$
%

with probability $\geq 1 - 1/p^{\tau_1-2} - 6c_1 \exp [-(c_2^2-1) \log(pq)] - 6c_4 \exp [-(c_5^2-1) \log(pq)], c_4 > 0, c_5 > 1$, where
%
\begin{align*}
\BQ \left(v_\beta, \Sigma_x^k, \Sigma_y^k \right) &=
2 v_\beta^2 V_x^k + 4 v_\beta c_2 [ \Lambda_{\max} (\Sigma_x^k) \Lambda_{\max} (\Sigma_y^k) ]^{1/2} \sqrt{\frac{ \log(pq)}{n}} +\\
& c_5 \left[ \Lambda_{\max} ( \Sigma_{y,-j}^k) \sigma_{y,j,-j}^k \right]^{1/2} \sqrt{\frac{\log q}{n}}
\end{align*}
%
with $\sigma_{y,j,-j}^k = \BV( E_j - \BE_{-j} \bftheta_{0,j})$, and 
%
$$
V_x^k = \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x^k n}} + \max_i \sigma_{x,ii}^k; \quad
c_x^k = \left[ 128 ( 1 + 4 \Lambda_{\max} (\Sigma_x)  )^2 \max_i (\sigma_{x,ii})^2 \right]^{-1}
$$
\end{enumerate} 
\end{Proposition}

The error bounds for $\widehat \Omega_y^k, k \in \cI_K$ follow immediately from the above two results.

\begin{Corollary}\label{corollary:OmegaCorollary}
Consider any deterministic $\widehat \cB$ that satisfy the following bound
%
$$
\| \widehat \bfB^k - \bfB_0^k \|_1 \leq v_\beta = \eta_\beta \sqrt{ \frac{ \log(pq)}{n}}
$$
%
Then, for sample size $n \succsim \log (pq)$ and choice of tuning parameter $\gamma_n = 4 \sqrt{| g_{\max}|} \BQ_0$, there exist constants $ c_1, c_3, c_4 > 0, c_2, c_5 > 1$ such that the following holds
%
\begin{align}\label{eqn:OmegaBounds}
\frac{1}{K} \sum_{k=1}^K \| \widehat \Omega_y^k - \Omega_y^k \|_F \leq
O \left( \BQ_0 \sqrt{\frac{| g_{\max}| S}{K}} \right)
\end{align}
%
with probability $\geq 1 - 1/p^{\tau_1-2} - 6c_1 \exp [-(c_2^2-1) \log(pq)] - 2 \exp (- c_3 n) - 6c_4 \exp [-(c_5^2-1) \log(pq)]$.

\end{Corollary}

{\colrbf Discuss tighter bound compared to vanilla JSEM}

After providing the error bounds for solutions to the subproblem \eqref{eqn:EstEqn2}, we concentrate on the subproblem \eqref{eqn:EstEqn1}. Following a similar strategy, we first get error bounds for $\widehat \bfbeta$ assuming everything else fixed.

%with
%$$
%\bfbeta = \begin{bmatrix}
%\ve (\bfB^1)\\
%\vdots\\
%\ve (\bfB^K)\\
%\end{bmatrix}; \quad
%\bfGamma = \begin{bmatrix}
%I_q \otimes (\bfX^1) TX^1 / n) & &\\
%& \ddots &\\
%& & I_q \otimes (\bfX^K)^T X^K / n)
%\end{bmatrix} 
%$$
\begin{Theorem}\label{thm:BetaThm}
Assume fixed $\cX, \cE$, and deterministic $\widehat \Theta = \{ \widehat \Theta_j \}$, so that for $j \in \cI_q$,

\noindent{\bf(B1)} $\| \widehat \Theta_j - \Theta_{0,j} \|_F \leq v_\Theta \sqrt{\frac{\log q}{n}}$ for some $v_\Theta$ dependent on $\Theta$.

\noindent{\bf(B2)} Denote $\widehat \bfGamma^k = (\widehat \bfT^k)^2 \otimes (\bfX^k)^T \bfX^k/n, \widehat \bfgamma^k = (\widehat \bfT^k)^2 \otimes (\bfX^k)^T \bfY^k/n$. Then the deviation bound holds:
%
$$
\left\| \widehat \bfgamma^k - \widehat \bfGamma^k \bfbeta_0 \right\|_\infty \leq \BR( v_\Theta, \Sigma_x^k, \Sigma_y^k) \sqrt{ \frac{ \log(pq)}{n}}
$$
%
where $\BR \left(v_\Theta, \Sigma_x^k, \Sigma_y^k \right)$ is a $O(1)$ deterministic function of $\Theta, \Sigma_x^k$ and $\Sigma_y^k$.

\noindent{\bf(B3)} $\widehat \bfGamma \sim RE(\psi_*, \phi_*)$ with $Kpq \phi_* \leq \psi_*/2$.

Then, given the choice of tuning parameter
%
$$
\lambda_n \geq 4 \sqrt{| h_{\max} |} \BR_0 \sqrt{ \frac{ \log(pq)}{n}}; \quad 
\BR_0 := \max_{k \in \cI_K} \BR \left(v_\Theta, \Sigma_x^k, \Sigma_y^k \right)
$$
%
the following holds
%
\begin{align}
\| \widehat \bfbeta - \bfbeta_0 \|_1 & \leq 48 \sqrt{ | h_{\max} |} s_\beta \lambda_n / \psi^* \label{eqn:BetaThmEqn1}\\
\| \widehat \bfbeta - \bfbeta_0 \| & \leq 12 \sqrt s_\beta \lambda_n / \psi^* \label{eqn:BetaThmEqn2}\\
\sum_{h \in \cH} \| \bfbeta^{[h]} - \bfbeta_0^{[h]} \| & \leq 48 s_\beta \lambda_n / \psi^* \label{eqn:BetaThmEqn3}\\
(\widehat \bfbeta - \bfbeta_0 )^T \widehat \bfGamma (\widehat \bfbeta - \bfbeta_0 ) & \leq
72 s_\beta \lambda_n^2 / \psi^* \label{eqn:BetaThmEqn4}
\end{align}
%
%Also denote the non-zero support of $\widehat \bfbeta$ by $\widehat \cS_\beta$, i.e. $\widehat \cS_\beta = \{ g: \hat \bfbeta^{[g]} \neq {\bf 0} \}$. Then
%%
%\begin{align}\label{eqn:BetaThmEqn4}
%| \widehat \cS_\beta| \leq 128 s_\beta / \psi^*
%\end{align}
\end{Theorem}

Next we verify that conditions (B2) and (B3) hold with high probability given fixed $\widehat \Theta$.
\begin{Proposition}\label{prop:ThmBetaRE}
Consider deterministic $\widehat \Theta$ satisfying assumption (B1). Assume that the matrices $(\widehat \bfT^k)^2, k \in \cI_K$ are diagonally dominant. Then for sample size $n \succsim \log (pq)$,

\begin{enumerate}
%\item We have $\| \bfX^k ( \hat \bfB^k - \bfB_0^k ) \|_\infty \leq c( v_\beta)$, where
%%
%$$
%c(v_\beta) =\sqrt n v_\beta \left[ \sqrt{ \frac{ \log 4 + \tau_1 \log p}{c_x^k n}} + \max_j \sigma_{x,jj}^k \right]^{1/2}; \quad
%c_x^k = \left[ 128 ( 1 + 4 \Lambda_{\max} (\Sigma_x^k)  )^2 \max_j (\sigma_{x,jj}^k)^2 \right]^{-1}
%$$
%%
%with probability $ \geq 1 - 1/p^{\tau_1-2}, \tau_1 > 2$.
%
\item %For sample size $n \succsim \max (s_\beta \log p, d^2 \log q)$,
$\widehat \bfGamma$ satisfies the RE condition: $ \widehat \bfGamma \sim RE (\psi_*, \phi_*)$, where 
%
$$
\psi_* = \min_k \psi^k \left( \min_i \psi_t^i - d v_\Theta \right), 
\phi_* = \max_k \phi^k \left( \min_i \phi_t^i + d v_\Theta \right)
$$
%
with probability $\geq 1 - 2 \exp(c_3 n), c_3>0$.
%
\item The deviation bound in (B2) is satisfied with probability $ \geq 1 - 12 c_1 \exp[ (c_2^2-1) \log (pq)], c_1>0, c_2>1$, where
$$
\BR \left(v_\Theta, \Sigma_x^k, \Sigma_y^k \right) = c_2 \sqrt{\Lambda_{\max} (\Sigma_x^k)} \left( d v_\Theta \Lambda_{\min} (\Sigma_y^k) +
\frac{1}{\Lambda_{\min} (\Sigma_y^k) } \right)
$$
\end{enumerate} 
\end{Proposition}

We now put both the pieces together, and prove that our alternating algorithm results in a solution sequence $\{ \widehat \cB^{(r)}, \widehat \Theta^{(r)} \}, r = 1, 2, \ldots$ that lies uniformly within a non-expanding ball around the true parameter values.
