# calculate penalties
# For each group index in the group array, collect corresponding elements in the main array ...
# and sum their l2 norms
unique.Theta.groups = unique(paste(Theta.group.array))
Theta.norm = sapply(unique.Theta.groups, function(g)
sum(Theta.array[which(Theta.group.array==g, arr.ind=T)]^2))
Theta.norm = sum(sqrt(Theta.norm))
unique.B.groups = unique(paste(B.group.array))
B.norm = sapply(unique.B.groups, function(g)
sum(B.array[which(B.group.array==g, arr.ind=T)]^2))
B.norm = sum(sqrt(B.norm))
squaredError(Y.list, X.list, Theta.array, B.array) + lambda*Theta.norm + gamma*B.norm
}
Normdiff = c()
Objfunc = Obj(Y.list, X.list, Theta_init.array, B_init.array,
Theta.group.array, B.group.array,
lambda=lambda, gamma=.5 * sqrt(log(q)/n))
iter = 0; CONVERGE=FALSE; refit.B=TRUE; update.counter=0;
updateTheta = FALSE; # we don't update Theta until B is stabilized a bit
B_new.array = B_init.array
Theta_new.array = Theta_init.array
jsem.model = NULL
cat('-----\n')
while(!CONVERGE){
iter = iter + 1;
B_old.array = B_new.array
Theta_old.array = Theta_new.array
cat("Iteration ", iter, ":\n")
# Updating B
cat("Updating B array\n")
for(j in 1:q){
# make long vector or errors for j-th column for all k
Et.j.list = list()
for(k in 1:K){
Et.j.list[[k]] = Ehat.list[[k]][,-j] %*% Theta_old.array[-j,j,k]
}
Ehat.theta.j = unlist(Et.j.list)
rm(Et.j.list)
# build model
temp = grpreg(X, Y[,j] + Ehat.theta.j, unlist(as.numeric(B.group.array[,j,])),
family="gaussian", penalty="grLasso", lambda=lambda)
B_new.array[,j,] = matrix(temp$beta[-1], ncol=K, byrow=F)
## refit if necessary
if(refit.B){
# make long vector or errors for j-th column for all k
temp1 = temp$beta[-1]
B.j.support = which(abs(temp1)>1e-6)
if (length(B.j.support)>0){
# build model
temp2 = lm(Y[,j] + Ehat.theta.j~X[,B.j.support]+0)
# save non-zero coefs
temp1[B.j.support] = temp2$coef
B_new.array[,j,] = matrix(temp1, ncol=K, byrow=F)
}
}
# now update j-th column of all K matrices in B_new, and Ehat
for(k in 1:K){
Ehat.list[[k]][,j] = Y.list[[k]][,j] - X.list[[k]] %*% as.matrix(B_new.array[,j,k], ncol=1)
}
}
# # update array of E
# for(k in 1:K){
#   Ehat.list[[k]] = Y.list[[k]] - X.list[[k]] %*% B_new.array[,,k]
# }
# Updating Theta
if (iter >=10 | sqrt(sum(B_new.array - B_old.array)^2)<0.1 | updateTheta == TRUE){
updateTheta = TRUE; # once we start updating Theta, we just start from now on;
cat("Updating Theta array: ")
bic.jsem <- sel.lambda.jsem(do.call(rbind, Ehat.list), do.call(rbind, Ehat.list),
unlist(Y.indices), unlist(Y.indices),
Theta.groups, lambda=gamma)
gamma.min = gamma[which.min(bic.jsem$BIC)]
jsem.model = JSEM(do.call(rbind, Ehat.list), unlist(Y.indices),
Theta.groups, lambda=gamma.min)
Theta_new.array = array(0, c(q,q,K))
for(k in 1:K){
Theta_new.array[,,k] = jsem.model$Theta[[k]]
}
update.counter = update.counter + 1
# bic.mat = rbind(bic.mat, bic.jsem$BIC)
} else{
Theta_new.array = Theta_old.array
}
# check convergence
Objfunc[iter+1] = Obj(Y.list, X.list, Theta_new.array, B_new.array,
Theta.group.array, B.group.array,
lambda=lambda, gamma=gamma.min)
Normdiff[iter] = sqrt(sum(B_new.array - B_old.array)^2)/sqrt(sum(B_new.array^2)) +
sqrt(sum(Theta_new.array - Theta_old.array)^2)/sqrt(sum(Theta_new.array^2))
# if (iter == 1){
#   Norm_diff = Normfunc[1]
# }
# else{
#   Norm_diff = Normfunc[iter] - Normfunc[iter-1]
#   Obj_diff = (Objfunc[iter] - Objfunc[iter-1])/Objfunc[iter-1]
# }
Obj_diff = Objfunc[iter+1]/Objfunc[iter] - 1
# convergence criterion value
nd = abs(Normdiff[iter])
if(iter>1){
nd = c(nd, abs(rev(diff(Normdiff))[1]))
}
if(iter>2){
nd = c(nd, abs(rev(diff(Normdiff,2))[1]))
}
cat("Norm_diff =",round(nd,4),'Obj_diff',round(abs(Obj_diff),5),'\n-----\n')
CONVERGE = (min(nd)<tol)
if (iter == maxit){
cat("Max iterations reached.",'\n')
break;
}
}
while(!CONVERGE){
iter = iter + 1;
B_old.array = B_new.array
Theta_old.array = Theta_new.array
cat("Iteration ", iter, ":\n")
# Updating B
cat("Updating B array\n")
for(j in 1:q){
# make long vector or errors for j-th column for all k
Et.j.list = list()
for(k in 1:K){
Et.j.list[[k]] = Ehat.list[[k]][,-j] %*% Theta_old.array[-j,j,k]
}
Ehat.theta.j = unlist(Et.j.list)
rm(Et.j.list)
# build model
temp = grpreg(X, Y[,j] + Ehat.theta.j, unlist(B.group.array[,j,]),
family="gaussian", penalty="grLasso", lambda=lambda)
B_new.array[,j,] = matrix(temp$beta[-1], ncol=K, byrow=F)
## refit if necessary
if(refit.B){
# make long vector or errors for j-th column for all k
temp1 = temp$beta[-1]
B.j.support = which(abs(temp1)>1e-6)
if (length(B.j.support)>0){
# build model
temp2 = lm(Y[,j] + Ehat.theta.j~X[,B.j.support]+0)
# save non-zero coefs
temp1[B.j.support] = temp2$coef
B_new.array[,j,] = matrix(temp1, ncol=K, byrow=F)
}
}
# now update j-th column of all K matrices in B_new, and Ehat
for(k in 1:K){
Ehat.list[[k]][,j] = Y.list[[k]][,j] - X.list[[k]] %*% as.matrix(B_new.array[,j,k], ncol=1)
}
}
# # update array of E
# for(k in 1:K){
#   Ehat.list[[k]] = Y.list[[k]] - X.list[[k]] %*% B_new.array[,,k]
# }
# Updating Theta
if (iter >=10 | sqrt(sum(B_new.array - B_old.array)^2)<0.1 | updateTheta == TRUE){
updateTheta = TRUE; # once we start updating Theta, we just start from now on;
cat("Updating Theta array: ")
bic.jsem <- sel.lambda.jsem(do.call(rbind, Ehat.list), do.call(rbind, Ehat.list),
unlist(Y.indices), unlist(Y.indices),
Theta.groups, lambda=gamma)
gamma.min = gamma[which.min(bic.jsem$BIC)]
jsem.model = JSEM(do.call(rbind, Ehat.list), unlist(Y.indices),
Theta.groups, lambda=gamma.min)
Theta_new.array = array(0, c(q,q,K))
for(k in 1:K){
Theta_new.array[,,k] = jsem.model$Theta[[k]]
}
update.counter = update.counter + 1
# bic.mat = rbind(bic.mat, bic.jsem$BIC)
} else{
Theta_new.array = Theta_old.array
}
# check convergence
Objfunc[iter+1] = Obj(Y.list, X.list, Theta_new.array, B_new.array,
Theta.group.array, B.group.array,
lambda=lambda, gamma=gamma.min)
Normdiff[iter] = sqrt(sum(B_new.array - B_old.array)^2)/sqrt(sum(B_new.array^2)) +
sqrt(sum(Theta_new.array - Theta_old.array)^2)/sqrt(sum(Theta_new.array^2))
# if (iter == 1){
#   Norm_diff = Normfunc[1]
# }
# else{
#   Norm_diff = Normfunc[iter] - Normfunc[iter-1]
#   Obj_diff = (Objfunc[iter] - Objfunc[iter-1])/Objfunc[iter-1]
# }
Obj_diff = Objfunc[iter+1]/Objfunc[iter] - 1
# convergence criterion value
nd = abs(Normdiff[iter])
if(iter>1){
nd = c(nd, abs(rev(diff(Normdiff))[1]))
}
if(iter>2){
nd = c(nd, abs(rev(diff(Normdiff,2))[1]))
}
cat("Norm_diff =",round(nd,4),'Obj_diff',round(abs(Obj_diff),5),'\n-----\n')
CONVERGE = (min(nd)<tol)
if (iter == maxit){
cat("Max iterations reached.",'\n')
break;
}
}
maxit=20
refit.B=TRUE
Objfunc
Y = do.call(rbind, Y.list)
X = as.matrix(do.call(bdiag, X.list))
# initialize
Normdiff = c()
Objfunc = Obj(Y.list, X.list, Theta_init.array, B_init.array,
Theta.group.array, B.group.array,
lambda=lambda, gamma=.5 * sqrt(log(q)/n))
iter = 0; CONVERGE=FALSE; CONVERGE1=FALSE; refit.B=TRUE;
B_new.array = B_init.array
Theta_new.array = Theta_init.array
jsem.model = NULL
Objfunc
maxit
while(!CONVERGE){
iter = iter + 1;
B_old.array = B_new.array
Theta_old.array = Theta_new.array
cat("Iteration ", iter, ":\n")
# Updating B
cat("Updating B array\n")
for(j in 1:q){
# make long vector or errors for j-th column for all k
Et.j.list = list()
for(k in 1:K){
Et.j.list[[k]] = Ehat.list[[k]][,-j] %*% Theta_old.array[-j,j,k]
}
Ehat.theta.j = unlist(Et.j.list)
rm(Et.j.list)
# build model
temp = grpreg(X, Y[,j] + Ehat.theta.j, unlist(B.group.array[,j,]),
family="gaussian", penalty="grLasso", lambda=lambda)
B_new.array[,j,] = matrix(temp$beta[-1], ncol=K, byrow=F)
## refit if necessary
if(refit.B){
# make long vector or errors for j-th column for all k
temp1 = temp$beta[-1]
B.j.support = which(abs(temp1)>1e-6)
if (length(B.j.support)>0){
# build model
temp2 = lm(Y[,j] + Ehat.theta.j~X[,B.j.support]+0)
# save non-zero coefs
temp1[B.j.support] = temp2$coef
B_new.array[,j,] = matrix(temp1, ncol=K, byrow=F)
}
}
# now update j-th column of all K matrices in B_new, and Ehat
for(k in 1:K){
Ehat.list[[k]][,j] = Y.list[[k]][,j] - X.list[[k]] %*% as.matrix(B_new.array[,j,k], ncol=1)
}
}
# If Beta is stablized, updating Theta then B once then break
if (iter >=10 | sqrt(sum(B_new.array - B_old.array)^2)<0.1){
# Update Theta
cat("Updating Theta array: ")
bic.jsem <- sel.lambda.jsem(do.call(rbind, Ehat.list), do.call(rbind, Ehat.list),
unlist(Y.indices), unlist(Y.indices),
Theta.groups, lambda=gamma)
gamma.min = gamma[which.min(bic.jsem$BIC)]
jsem.model = JSEM(do.call(rbind, Ehat.list), unlist(Y.indices),
Theta.groups, lambda=gamma.min)
Theta_new.array = array(0, c(q,q,K))
for(k in 1:K){
Theta_new.array[,,k] = jsem.model$Theta[[k]]
}
# Update B
B_old.array = B_new.array
cat("Updating B array\n")
for(j in 1:q){
# make long vector or errors for j-th column for all k
Et.j.list = list()
for(k in 1:K){
Et.j.list[[k]] = Ehat.list[[k]][,-j] %*% Theta_new.array[-j,j,k]
}
Ehat.theta.j = unlist(Et.j.list)
rm(Et.j.list)
# build model
temp = grpreg(X, Y[,j] + Ehat.theta.j, unlist(as.numeric(B.group.array[,j,])),
family="gaussian", penalty="grLasso", lambda=lambda)
B_new.array[,j,] = matrix(temp$beta[-1], ncol=K, byrow=F)
## refit if necessary
if(refit.B){
# make long vector or errors for j-th column for all k
temp1 = temp$beta[-1]
B.j.support = which(abs(temp1)>1e-6)
if (length(B.j.support)>0){
# build model
temp2 = lm(Y[,j] + Ehat.theta.j~X[,B.j.support]+0)
# save non-zero coefs
temp1[B.j.support] = temp2$coef
B_new.array[,j,] = matrix(temp1, ncol=K, byrow=F)
}
}
# now update j-th column of all K matrices in B_new, and Ehat
for(k in 1:K){
Ehat.list[[k]][,j] = Y.list[[k]][,j] - X.list[[k]] %*% as.matrix(B_new.array[,j,k], ncol=1)
}
CONVERGE1 = TRUE # Set CONVERGE = 1 to break outer loop
}
} else{
Theta_new.array = Theta_old.array
}
# check convergence
Objfunc[iter+1] = Obj(Y.list, X.list, Theta_new.array, B_new.array,
Theta.group.array, B.group.array,
lambda=lambda, gamma=gamma.min)
Normdiff[iter] = sqrt(sum(B_new.array - B_old.array)^2)/sqrt(sum(B_new.array^2)) +
sqrt(sum(Theta_new.array - Theta_old.array)^2)/sqrt(sum(Theta_new.array^2))
# if (iter == 1){
#   Norm_diff = Normfunc[1]
# }
# else{
#   Norm_diff = Normfunc[iter] - Normfunc[iter-1]
#   Obj_diff = (Objfunc[iter] - Objfunc[iter-1])/Objfunc[iter-1]
# }
Obj_diff = Objfunc[iter+1]/Objfunc[iter] - 1
# convergence criterion value
nd = abs(Normdiff[iter])
if(iter>1){
nd = c(nd, abs(rev(diff(Normdiff))[1]))
}
if(iter>2){
nd = c(nd, abs(rev(diff(Normdiff,2))[1]))
}
cat("Norm_diff =",round(nd,4),'Obj_diff',round(abs(Obj_diff),5),'\n-----\n')
CONVERGE = (CONVERGE1 | (min(nd)<tol))
if (iter == maxit){
cat("Max iterations reached.",'\n')
break;
}
}
CONVERGE
## If refitting of B matrices hasn't been done inside the loop then refit in the end
B_refit.array = B_new.array
## Refit to get Omega
# cat("Getting Omega 1\n")
if(is.null(jsem.model)){
Ahat = list()
for(k in 1:K){
Ahat[[k]] = matrix(0, q, q)
Ahat[[k]][which(abs(Theta_new.array[,,k])>eps, arr.ind=T)] = 1
diag(Ahat[[k]]) = 0
}
} else{
Ahat = jsem.model$Ahat
}
eps=1e-6
## Refit to get Omega
# cat("Getting Omega 1\n")
if(is.null(jsem.model)){
Ahat = list()
for(k in 1:K){
Ahat[[k]] = matrix(0, q, q)
Ahat[[k]][which(abs(Theta_new.array[,,k])>eps, arr.ind=T)] = 1
diag(Ahat[[k]]) = 0
}
} else{
Ahat = jsem.model$Ahat
}
# cat("Getting Omega 2\n")
Info = list()
for (k in 1:K){
Info[[k]] = zeroInd(Ahat[[k]], 1)$zeroArr
}
# cat("Getting Omega 3\n")
Theta_refit = multi.glasso(do.call(rbind, Ehat.list), unlist(Y.indices), gamma.min, Info)
dim(B_refit.array)
B_refit.array[1:5,1:5]
B_refit.array[1:5,1:5,]
B_refit.array[1:10,1:10,]
summary(B_refit.array)
summary(Theta_refit$Omega)
summary(Theta_refit$Theta)
dim(Theta_refit$Theta)
head((Theta_refit$Theta
))
dim(do.call(rbind, Ehat.list))
sapply(Ehat.list,dim)
unlist(Y.indices)
gamma.min
multi.glasso <- function(
trainX,
trainY,
lambda,
zero = NULL,
BIC = FALSE,
eps = 1e-06
){
p = dim(trainX)[2]
K = length(unique(trainY))
n = as.numeric(table(trainY))
#penalty needed for glasso
if (length(lambda)==K) {rho = lambda} else {
rho = rep(lambda, K)}
#Initialize the estimated precision, partial correlation and adjacency matrix
Omega.hat = vector("list", K)
Theta = vector("list", K)
Ahat = vector("list", K)
#Whether there are entries that need to be constrained to zero
if (is.null(zero)){
zero = rep(list(zero), K)
}
# if (max(sapply(zero, length)) == p*(p-1)){
#   stop("One or more matrices are constrained to be zero")
# }
bic.score = rep(0, K)
for (k in 1:K) {
Ahat[[k]] = matrix(0, p, p)
data <- trainX[which(trainY == k), ]
empcov <- cov(data) #empirical cov
if(length(zero[[k]])<p*(p-1)){
while (kappa(empcov) > 1e+2){
empcov = empcov + 0.05 * diag(p)
}
fit <- glasso(empcov, rho = rho[k], zero = zero[[k]], penalize.diagonal=FALSE, maxit = 30)
Omega.hat[[k]] = (fit$wi + t(fit$wi))/2
Theta[[k]] <- diag(diag(Omega.hat[[k]])^(-0.5)) %*% Omega.hat[[k]] %*% diag(diag(Omega.hat[[k]])^(-0.5))
Ahat[[k]][abs(Omega.hat[[k]])>eps] = 1
diag(Ahat[[k]]) = 0
if(BIC){
bic.score[k] = matTr(empcov %*% Omega.hat[[k]]) - log(det(Omega.hat[[k]])) + log(n[k]) * sum(Ahat[[k]])/(2*n[k])
}
} else{ # in case all edges are zero in k-th adjacency matrix
Omega.hat[[k]] = diag(1/(diag(empcov)-0.05))
Theta[[k]] = diag(1, p)
}
}
out = list(Omega = Omega.hat, Theta = Theta, Adj = Ahat, BIC = bic.score, lambda = lambda)
return(out)
}
# cat("Getting Omega 3\n")
Theta_refit = multi.glasso(do.call(rbind, Ehat.list), unlist(Y.indices), gamma.min, Info)
Theta_refit$BIC
Theta_refit$lambda
dim(Theta_refit$Omega[[1]])
dim(Theta_refit$Omega[[2]])
dim(Theta_refit$Theta[[2]])
length(Theta_refit$Theta)
length(Theta_refit$Omega)
m = 3
model = with(data, jmmle.1step(
Y.list=Y.list1, X.list=X.list1,
B.group.array=B.group.array, Theta.groups=Theta.groups,
lambda = lambda.vec[m],
gamma = sqrt(log(q)/n) * seq(0.8, 0.1, -0.1),
init.option=1, tol=1e-3))
source('./Codes/jsem.R')
source('./Codes/Generator.R')
source('./Codes/l1LS_Main.R')
source('./Codes/Objval.R')
source('./Codes/JMLE.R')
# source('./Codes/lasso_inference.r')
# load data
data = readRDS("Data/processed_data.rds")
## tune JMLE model
n = max(sapply(data$X.list1,nrow))
p = ncol(data$X.list1[[1]])
q = ncol(data$Y.list1[[1]])
K = length(data$X.list1)
lambda.vec = sqrt(log(p)/n) * seq(1.8, 0.4, -0.2)
nlambda = length(lambda.vec)
## get all models
m = 3
model = with(data, jmmle.1step(
Y.list=Y.list1, X.list=X.list1,
B.group.array=B.group.array, Theta.groups=Theta.groups,
lambda = lambda.vec[m],
gamma = sqrt(log(q)/n) * seq(0.8, 0.1, -0.1),
init.option=1, tol=1e-3))
source('./Codes/jsem.R')
source('./Codes/Generator.R')
source('./Codes/l1LS_Main.R')
source('./Codes/Objval.R')
source('./Codes/JMLE.R')
model = with(data, jmmle.1step(
Y.list=Y.list1, X.list=X.list1,
B.group.array=B.group.array, Theta.groups=Theta.groups,
lambda = lambda.vec[m],
gamma = sqrt(log(q)/n) * seq(0.8, 0.1, -0.1),
init.option=1, tol=1e-3))
jmle.model = model
SSE.vec = rep(0,K)
hbic.pen.vec = rep(0,K)
for(k in 1:K){
nk = nrow(Y.list[[k]])
Theta.k = jmle.model$Theta_refit$Theta[[k]]
for(j in 1:q)
{
Theta.k[j,j] = 0
}
SSE.vec[k] = sum(diag(crossprod((Y.list[[k]] - X.list[[k]] %*%
jmle.model$B.refit[,,k]) %*% (diag(1,q) - Theta.k))))/nk
hbic.pen.vec[k] = log(log(nk))*log(q*(q-1)/2)/nk * sum(Theta.k != 0)/2 +
log(log(nk))*log(p*q)/nk * sum(jmle.model$B.refit[,,k] != 0)
}
sum(SSE.vec) + sum(hbic.pen.vec)
for(k in 1:K){
nk = nrow(Y.list[[k]])
Theta.k = jmle.model$Theta_refit$Theta[[k]]
for(j in 1:q)
{
Theta.k[j,j] = 0
}
SSE.vec[k] = with(data, sum(diag(crossprod((Y.list[[k]] - X.list[[k]] %*%
jmle.model$B.refit[,,k]) %*% (diag(1,q) - Theta.k))))/nk)
hbic.pen.vec[k] = log(log(nk))*log(q*(q-1)/2)/nk * sum(Theta.k != 0)/2 +
log(log(nk))*log(p*q)/nk * sum(jmle.model$B.refit[,,k] != 0)
}
sum(SSE.vec) + sum(hbic.pen.vec)
