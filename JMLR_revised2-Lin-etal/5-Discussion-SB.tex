\section{Discussion}\label{sec:grouping}

In this paper, we examined multi-layered Gaussian networks, proposed a provably converging algorithm for obtaining 
the estimates of the key model parameters and established their theoretical properties in  high-dimensional settings.
Note that we focused on $\ell_1$ penalties for both the directed and undirected edges, since it was assumed that the multi-layer network was sparse both between layers and within layers. In many scientific applications, external information may require
imposing group penalties, primarily on the directed edge parameters ($B$). For example, in a gene-protein 2-layer network, genes
can be grouped according to their function in pathways and one may be interested in assessing the pathway's impact on proteins.
In that case, a group lasso penalty can be imposed. In general, the proposed framework can easily accommodate 
other types of structured sparsity inducing penalties in accordance to the underlying data generating procedure. 
Naturally, the exact form of the error bounds
established would be different, depending on the exact choice of penalty selected. Nevertheless, as long as the penalty is convex, all arguments regarding bi-convexity and convergence follow, and we can use similar strategies to bound the statistical error of the estimators, obtained via the developed iterative algorithm.

Next, we discuss connections of this work to that in \citet{sohn2012joint,yuan2014partial,mccarter2014sparse}. In these papers,
an alternative parameterization of the 2-layer network is adopted. Specifically, all nodes in layers 1 and 2 are considered
jointly and assumed to be drawn from the following Gaussian distribution:
\begin{equation*}
\begin{pmatrix}
\bs{X} \\ \bs{Y}
\end{pmatrix}\sim \mathcal{N}\left( 0, \begin{pmatrix}
\Omega_{X} & \Omega_{XY} \\ \Omega_{YX} & \Omega_{Y}
\end{pmatrix}^{-1} \right), 
\end{equation*}
and by conditioning $\bs{Y}$ on $\bs{X}$, one obtains:
\begin{equation}\label{eqn:YonX}
\bs{Y}|\bs{X} \sim \mathcal{N}\left(-\Omega_Y^{-1} \Omega'_{XY} \bs{X},  \Omega_Y^{-1}\right).
\end{equation}
Compare (\ref{eqn:YonX}) with our model set-up in Section~\ref{sec:set-up}, the following correspondence holds:
\begin{equation}\label{eqn:correspondence}
B = - \Omega_{XY}\Omega_{Y}^{-1}, \qquad \Omega_Y = \Theta_\epsilon. 
\end{equation}
The latter point is discussed at length in \cite{andersson2001alternative}, together with pros and cons of the two parameterization
schemes. 

Note that the one-to-one correspondence (\ref{eqn:correspondence}) between $(\Omega_{XY},\Omega_Y)$ and $(B,\Theta_\epsilon)$ clearly shows that $B$ and $\Theta_{XY}$ may have very different sparsity patterns depending on the structure of $\Theta_{\epsilon}$. Consequently, in a finite sample setting, the sparsity constrained ML estimates $(\hat{\Omega}_{XY}, \hat{\Omega}_Y)$ and $(\hat{B}, \hat{\Theta}_{\epsilon})$ may deviate considerably from the above correspondence relationship, and it may not be possible to recover a sparse $B$ by estimating $(\Omega_{XY},\Omega_Y)$. 
 
In a low-dimensional data setting, classical asymptotic theory ensures that the regular ML estimates from the two parameterizations (in absence of any sparsity constraints) are similar provided that the problem is well-conditioned and the sample size reasonably large. However, the situation is quite different in high-dimensional settings and in the presence of sparsity penalties. Specifically, given data $X$ and $Y$, instead of parameterizing the model in terms of $(B,\Theta_\epsilon)$, the authors in \cite{sohn2012joint,yuan2014partial,mccarter2014sparse} consider the following optimization problem, parameterized in $(\Omega_{XY},\Omega_Y)$:
\begin{equation}\label{opt:convex}
\min\limits_{\Omega_{XY},\Omega_Y} g(\Omega_{XY},\Omega_Y) \equiv  g_0(\Omega_{XY},\Omega_Y)
+ \mathcal{R}(\Omega_{XY},\Omega_{Y})
\end{equation}
where $g_0(\Omega_{XY},\Omega_Y) = -\log\det\Omega_Y + \frac{1}{n}\text{tr}\left[ (Y + \Omega_{XY}\Omega_{Y}^{-1} X)'\Omega_Y (Y + \Omega_{XY}\Omega_{Y}^{-1}X) \right]$ is jointly convex in $(\Omega_{XY},\Omega_Y)$, and $\mathcal{R}(\Omega_{XY},\Omega_Y)$ is some regularization term. In particular, the element-wise $\ell_1$ norm on $\Omega_Y$, and the element-wise $\ell_1$ or column-wise $\ell_1$ norm (matrix $2,1$ norm) on $\Omega_{XY}$ are the main penalties under consideration in those papers. 

Despite the convex formulation in (\ref{opt:convex}), we would like to point out that in general, the sparsity pattern in $B$ and $\Omega_{XY}$ are not transferable through the regularization term, which underlies a major difference between the formulation in (\ref{opt:convex}) and the one presented in this paper. Given the correspondence in (\ref{eqn:correspondence}), there are two cases where $B$ and $\Omega_{XY}$ share the same sparsity pattern: 1) $\Omega_{Y}$ (or $\Theta_{\epsilon}$, equivalently) is diagonal, or 2) both the $i^{\text{th}}$ row in $B$ and $\Omega_{XY}$ are identically zero, for an arbitrary $i=1,\cdots,p_1$.  However, both settings are fairly restrictive and may not hold in many applications. 

Note that the multivariate regression framework represents a natural modeling tool for a number of problems where the regression coefficients have a specific scientific interpretation. This point is also explicitly made in the work of \citet{andersson2001alternative}.  For high dimensional problems, the $(B,\Theta_\epsilon)$-parametrization, through an addition of  
proper regularization to $B$ (e.g., penalty which enforces element-wise sparsity or group-Lasso type of sparsity, etc) if necessary,
easily preserves the interpretation of both the $B$ and $\Theta_\epsilon$ parameters.
However, with the $(\Omega_{XY},\Omega_Y)$-parametrization, the underlying sparsity in the true data generating procedure, encoded by $B$, will not be easily incorporated, and to add a regularization term on $\Omega_{XY}$ may lose the scientific interpretability, and may also lead to an estimated $B$ whose sparsity pattern is completely mis-specified, obtained from (\ref{eqn:correspondence}) with $\widehat{\Omega}_{XY},\widehat{\Omega}_Y$ plugged in.

Another difference we would like to point out is that once we add penalty terms to the objective function in the low dimensional setting, or switch to the high dimensional setting (as considered in \citet{sohn2012joint} and \citet{yuan2014partial}), the correspondence between the optimizer(s) of (\ref{eqn:opt}) and the optimizer(s) of (\ref{opt:convex}) become difficult to connect analytically. 
